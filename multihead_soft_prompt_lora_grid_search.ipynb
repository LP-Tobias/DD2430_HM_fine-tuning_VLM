{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T23:16:02.749950Z",
     "iopub.status.busy": "2024-10-08T23:16:02.749600Z",
     "iopub.status.idle": "2024-10-08T23:16:23.680407Z",
     "shell.execute_reply": "2024-10-08T23:16:23.679521Z",
     "shell.execute_reply.started": "2024-10-08T23:16:02.749912Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install peft\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T23:16:23.682277Z",
     "iopub.status.busy": "2024-10-08T23:16:23.681971Z",
     "iopub.status.idle": "2024-10-08T23:16:23.692167Z",
     "shell.execute_reply": "2024-10-08T23:16:23.691237Z",
     "shell.execute_reply.started": "2024-10-08T23:16:23.682244Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_image_paths_and_labels_from_df(df, data_dir):\n",
    "    article_ids = df[\"article_id\"].values\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for article_id in article_ids:\n",
    "        image_path = f\"{data_dir}/images/0{str(article_id)[:2]}/0{article_id}.jpg\"\n",
    "        # Check if the image file exists\n",
    "        if os.path.exists(image_path):\n",
    "            image_paths.append(image_path)\n",
    "            # Add corresponding label only if the image exists\n",
    "            labels.append(df[df[\"article_id\"] == article_id])\n",
    "\n",
    "    return image_paths, labels\n",
    "\n",
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_paths, processor=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.processor = processor\n",
    "        self.image_ids = []\n",
    "\n",
    "        for image_path in self.image_paths:\n",
    "            if not os.path.exists(image_path):\n",
    "                raise FileNotFoundError(f\"Image {image_path} not found.\")\n",
    "            else:\n",
    "                image_id = int(image_path.split(\"/\")[-1].split(\".\")[0])\n",
    "                self.image_ids.append(image_id)\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx])\n",
    "        if self.processor is not None:\n",
    "            inputs = self.processor(images=image, return_tensors=\"pt\", padding=True)\n",
    "            image = inputs[\"pixel_values\"][0]\n",
    "        return image, self.image_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T23:16:23.693415Z",
     "iopub.status.busy": "2024-10-08T23:16:23.693151Z",
     "iopub.status.idle": "2024-10-08T23:16:23.712807Z",
     "shell.execute_reply": "2024-10-08T23:16:23.711985Z",
     "shell.execute_reply.started": "2024-10-08T23:16:23.693385Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# set random seed 42\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T23:16:23.715445Z",
     "iopub.status.busy": "2024-10-08T23:16:23.715179Z",
     "iopub.status.idle": "2024-10-08T23:16:41.065895Z",
     "shell.execute_reply": "2024-10-08T23:16:41.065076Z",
     "shell.execute_reply.started": "2024-10-08T23:16:23.715415Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", cache_dir=\"model\", local_files_only=False)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\", cache_dir=\"model\", local_files_only=False)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T23:16:41.067932Z",
     "iopub.status.busy": "2024-10-08T23:16:41.067176Z",
     "iopub.status.idle": "2024-10-08T23:16:42.006574Z",
     "shell.execute_reply": "2024-10-08T23:16:42.005334Z",
     "shell.execute_reply.started": "2024-10-08T23:16:41.067884Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "text_path = 'data/articles.csv'\n",
    "articles = pd.read_csv(text_path)\n",
    "data_dir = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T23:16:42.008508Z",
     "iopub.status.busy": "2024-10-08T23:16:42.008093Z",
     "iopub.status.idle": "2024-10-08T23:16:42.305622Z",
     "shell.execute_reply": "2024-10-08T23:16:42.304645Z",
     "shell.execute_reply.started": "2024-10-08T23:16:42.008443Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# map from article_id to df index\n",
    "article_id_to_idx = {article_id: idx for idx, article_id in enumerate(articles[\"article_id\"])}\n",
    "\n",
    "# get all classes of the dataframe\n",
    "class_names = articles.columns.tolist()\n",
    "label_names = dict()\n",
    "label_names_to_idx = dict()\n",
    "for class_name in class_names:\n",
    "    label_names[class_name] = articles[class_name].unique()\n",
    "    label_names_to_idx[class_name] = {label_name: idx for idx, label_name in enumerate(label_names[class_name])}\n",
    "\n",
    "article_ids = label_names[\"article_id\"]\n",
    "#selected_class_names = [\"product_type_name\", \"graphical_appearance_name\"]\n",
    "selected_class_names = [\"product_group_name\", \"product_type_name\", \"graphical_appearance_name\", \"colour_group_name\", \"perceived_colour_value_name\", \"perceived_colour_master_name\", \"department_name\", \"index_name\", \"index_group_name\", \"section_name\", \"garment_group_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T23:16:42.307287Z",
     "iopub.status.busy": "2024-10-08T23:16:42.306975Z",
     "iopub.status.idle": "2024-10-08T23:17:04.605101Z",
     "shell.execute_reply": "2024-10-08T23:17:04.604169Z",
     "shell.execute_reply.started": "2024-10-08T23:16:42.307254Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# grouped by product_code\n",
    "grouped = articles.groupby(\"product_code\")\n",
    "groups = [group for _, group in grouped]\n",
    "\n",
    "# split 0.8/0.1/0.1\n",
    "train_groups, test_groups = train_test_split(groups, test_size=0.2, random_state=42) \n",
    "val_groups, test_groups = train_test_split(test_groups, test_size=0.5, random_state=42) \n",
    "\n",
    "train_df = pd.concat(train_groups)\n",
    "val_df = pd.concat(val_groups)\n",
    "test_df = pd.concat(test_groups)\n",
    "\n",
    "print(f\"{len(train_df)=} {len(val_df)=} {len(test_df)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T23:17:04.606390Z",
     "iopub.status.busy": "2024-10-08T23:17:04.606114Z",
     "iopub.status.idle": "2024-10-08T23:22:28.822877Z",
     "shell.execute_reply": "2024-10-08T23:22:28.822067Z",
     "shell.execute_reply.started": "2024-10-08T23:17:04.606360Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_paths, train_labels = get_image_paths_and_labels_from_df(train_df, data_dir)\n",
    "val_paths, val_labels = get_image_paths_and_labels_from_df(val_df, data_dir)\n",
    "test_paths, test_labels = get_image_paths_and_labels_from_df(test_df, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T23:22:28.824716Z",
     "iopub.status.busy": "2024-10-08T23:22:28.824358Z",
     "iopub.status.idle": "2024-10-08T23:22:28.831472Z",
     "shell.execute_reply": "2024-10-08T23:22:28.830508Z",
     "shell.execute_reply.started": "2024-10-08T23:22:28.824678Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiOutputLayer(torch.nn.Module):\n",
    "    def __init__(self, input_size, inter_size, output_size):\n",
    "        super(MultiOutputLayer, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, inter_size)\n",
    "        self.fc2 = torch.nn.Linear(inter_size, output_size)\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.act = torch.nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T23:22:28.836711Z",
     "iopub.status.busy": "2024-10-08T23:22:28.836295Z",
     "iopub.status.idle": "2024-10-08T23:22:28.848992Z",
     "shell.execute_reply": "2024-10-08T23:22:28.848117Z",
     "shell.execute_reply.started": "2024-10-08T23:22:28.836676Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiOutputClipModel(torch.nn.Module):\n",
    "    def __init__(self, clip_model, class_names, vision_hidden_size, inter_size, output_size, num_virtual_tokens):\n",
    "        super(MultiOutputClipModel, self).__init__()\n",
    "        self.clip_model = clip_model\n",
    "        self.class_names = class_names\n",
    "        self.output_layers = torch.nn.ModuleDict({\n",
    "            class_name: MultiOutputLayer(vision_hidden_size, inter_size, output_size)\n",
    "            for class_name in self.class_names\n",
    "        })\n",
    "\n",
    "        # Soft prompt embeddings per class\n",
    "        self.num_virtual_tokens = num_virtual_tokens\n",
    "        embedding_dim = self.clip_model.text_model.embeddings.token_embedding.embedding_dim\n",
    "        self.soft_prompt_embeddings = torch.nn.ParameterDict({\n",
    "            class_name: torch.nn.Parameter(torch.randn(num_virtual_tokens, embedding_dim))\n",
    "            for class_name in self.class_names\n",
    "        })\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        text_input_dict,\n",
    "        pixel_values,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.clip_model.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.clip_model.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.clip_model.config.use_return_dict\n",
    "\n",
    "        # Vision processing remains the same\n",
    "        vision_outputs = self.clip_model.vision_model(\n",
    "            pixel_values=pixel_values,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        vision_embeds = vision_outputs[1]\n",
    "        vision_embeds_dict = {\n",
    "            class_name: output_layer(vision_embeds) \n",
    "                for class_name, output_layer in self.output_layers.items()\n",
    "        }\n",
    "\n",
    "        # Text processing with soft prompts\n",
    "        text_embeds_dict = {}\n",
    "        for class_name in self.class_names:\n",
    "            text_inputs = text_input_dict[class_name]\n",
    "            input_ids = text_inputs[\"input_ids\"]\n",
    "            attention_mask = text_inputs[\"attention_mask\"]\n",
    "\n",
    "            input_embeds = self.clip_model.text_model.embeddings.token_embedding(input_ids)\n",
    "            batch_size = input_embeds.shape[0]\n",
    "\n",
    "            # Expand and Concatenate\n",
    "            soft_prompt = self.soft_prompt_embeddings[class_name].unsqueeze(0).expand(batch_size, -1, -1)\n",
    "            input_embeds = torch.cat([soft_prompt, input_embeds], dim=1)\n",
    "\n",
    "            # Adjust attention mask\n",
    "            soft_prompt_mask = torch.ones(batch_size, self.num_virtual_tokens).to(attention_mask.device)\n",
    "            attention_mask = torch.cat([soft_prompt_mask, attention_mask], dim=1)\n",
    "            valid_token_indices = (attention_mask.sum(dim=-1) - 1).long()\n",
    "            attention_mask = attention_mask[:, None, None, :]\n",
    "            attention_mask = attention_mask.expand(-1, 1, attention_mask.size(-1), attention_mask.size(-1))\n",
    "\n",
    "            encoder_outputs = self.clip_model.text_model.encoder(\n",
    "                inputs_embeds=input_embeds,\n",
    "                attention_mask=attention_mask,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "            last_hidden_state = encoder_outputs[0]\n",
    "            # print(last_hidden_state.shape)\n",
    "\n",
    "            pooled_output = last_hidden_state[torch.arange(last_hidden_state.shape[0]), valid_token_indices]\n",
    "\n",
    "            pooled_output = self.clip_model.text_model.final_layer_norm(pooled_output)\n",
    "\n",
    "            text_embeds = self.clip_model.text_projection(pooled_output)\n",
    "\n",
    "            text_embeds_dict[class_name] = text_embeds\n",
    "\n",
    "\n",
    "        # Compute logits\n",
    "        logits_per_image_dict = {\n",
    "            class_name: vision_embeds_dict[class_name] @ text_embeds_dict[class_name].T\n",
    "                for class_name in self.output_layers.keys()\n",
    "        }\n",
    "\n",
    "        return logits_per_image_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T23:22:28.850613Z",
     "iopub.status.busy": "2024-10-08T23:22:28.850234Z",
     "iopub.status.idle": "2024-10-08T23:22:28.863065Z",
     "shell.execute_reply": "2024-10-08T23:22:28.862214Z",
     "shell.execute_reply.started": "2024-10-08T23:22:28.850571Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# custom criterion: cross entropy loss across all classes\n",
    "class MultiOutputClipCriterion(torch.nn.Module):\n",
    "    def __init__(self, class_names):\n",
    "        super(MultiOutputClipCriterion, self).__init__()\n",
    "        self.class_names = class_names\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, logits_dict, labels_dict):\n",
    "        loss = 0\n",
    "        for class_name in self.class_names:\n",
    "            logits = logits_dict[class_name]\n",
    "            labels = labels_dict[class_name]\n",
    "            loss += self.criterion(logits, labels)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T23:24:16.468053Z",
     "iopub.status.busy": "2024-10-08T23:24:16.467616Z",
     "iopub.status.idle": "2024-10-08T23:24:57.730306Z",
     "shell.execute_reply": "2024-10-08T23:24:57.729503Z",
     "shell.execute_reply.started": "2024-10-08T23:24:16.468014Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "train_dataset = ImageDataset(train_paths, processor)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=8)\n",
    "\n",
    "val_dataset = ImageDataset(val_paths, processor)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=8)\n",
    "\n",
    "test_dataset = ImageDataset(test_paths, processor)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criteria, device, text_inputs, class_names):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = {class_name: 0 for class_name in class_names}\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, image_ids in tqdm(dataloader):\n",
    "            images = images.to(device)\n",
    "            logits_per_image_dict = model(pixel_values=images, text_input_dict=text_inputs)\n",
    "\n",
    "            # 获取真实标签\n",
    "            true_labels_dict = {\n",
    "                class_name: [label_names_to_idx[class_name][articles.loc[article_id_to_idx[image_id.item()], class_name]] \n",
    "                             for image_id in image_ids]\n",
    "                for class_name in class_names\n",
    "            }\n",
    "            true_labels_dict = {class_name: torch.tensor(true_labels).to(device)\n",
    "                                for class_name, true_labels in true_labels_dict.items()}\n",
    "            \n",
    "            # 计算损失\n",
    "            loss = criteria(logits_per_image_dict, true_labels_dict)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "\n",
    "            # 计算准确率\n",
    "            total_samples += images.size(0)\n",
    "            for class_name in class_names:\n",
    "                _, preds = torch.max(logits_per_image_dict[class_name], dim=1)\n",
    "                total_correct[class_name] += (preds == true_labels_dict[class_name]).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / total_samples / len(class_names)\n",
    "    accuracy = {class_name: total_correct[class_name] / total_samples for class_name in class_names}\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get current date and time\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "date = now.strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, val_dataloader, device, text_input_dict, num_epochs=10, lora_rank=8, lora_alpha=32, lr=1e-4, bias=\"none\", inter_size=128):\n",
    "\n",
    "    wandb.init(project=\"clip-lora-amp\", name=f\"r{lora_rank}_lr{lr}_b{bias}_is{inter_size}\")\n",
    "    step = 0\n",
    "\n",
    "    # Define LoRA configuration\n",
    "    lora_config = LoraConfig(\n",
    "        r=lora_rank,                  # Low-rank dimension (adjustable)\n",
    "        lora_alpha=lora_alpha,          # Scaling factor (adjustable)\n",
    "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\"],  # Specify which layers to apply LoRA to\n",
    "        lora_dropout=0.05,       # Dropout rate (optional)\n",
    "        bias=bias,            # Whether to include biases (\"none\", \"all\", \"lora_only\")\n",
    "        task_type=\"classification\"  # Task type (\"classification\" or \"regression\")\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    mo_model = MultiOutputClipModel(model, selected_class_names, 768, inter_size, 512).to(device)\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    criteria = MultiOutputClipCriterion(selected_class_names)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        mo_model.train()\n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for images, image_ids in tqdm(train_dataloader):\n",
    "\n",
    "            with torch.autocast(device_type=\"cuda\"):\n",
    "                images = images.to(device)\n",
    "                logits_per_image_dict = mo_model(pixel_values=images, text_input_dict=text_input_dict)\n",
    "\n",
    "                # 获取真实标签\n",
    "                true_labels_dict = {\n",
    "                    class_name: [label_names_to_idx[class_name][articles.loc[article_id_to_idx[image_id.item()], class_name]] \n",
    "                                for image_id in image_ids]\n",
    "                    for class_name in selected_class_names\n",
    "                }\n",
    "                true_labels_dict = {class_name: torch.tensor(true_labels).to(device) \n",
    "                                    for class_name, true_labels in true_labels_dict.items()}\n",
    "\n",
    "                # 计算损失\n",
    "                loss = criteria(logits_per_image_dict, true_labels_dict)\n",
    "                total_loss += loss.item() * images.size(0)\n",
    "\n",
    "            # 计算准确率\n",
    "            correct = 0\n",
    "            total_samples += images.size(0)\n",
    "            for class_name in selected_class_names:\n",
    "                _, preds = torch.max(logits_per_image_dict[class_name], dim=1)\n",
    "                correct += (preds == true_labels_dict[class_name]).sum().item()\n",
    "            total_correct += correct\n",
    "\n",
    "            # 反向传播和优化\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 记录训练损失和准确率到 wandb\n",
    "            # 在训练循环中，记录每个类别的准确率\n",
    "            log_dict = {\n",
    "                \"train_loss\": loss.item(),\n",
    "                \"train_accuracy\": correct / images.size(0) / len(selected_class_names)\n",
    "            }\n",
    "    #         for class_name in selected_class_names:\n",
    "    #             accuracy = total_correct_per_class[class_name] / total_samples\n",
    "    #             log_dict[f\"train_accuracy_{class_name}\"] = accuracy\n",
    "\n",
    "            wandb.log(log_dict, step=step)\n",
    "            step += 1\n",
    "\n",
    "        avg_loss = total_loss / total_samples / len(selected_class_names)\n",
    "        accuracy = total_correct / total_samples / len(selected_class_names)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_loss:.4f}, Train Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        # 在每个 epoch 结束后进行验证\n",
    "        val_loss, val_accuracy_dict = validate(mo_model, val_dataloader, criteria, device, text_input_dict, selected_class_names)\n",
    "        val_accuracy = sum(val_accuracy_dict.values()) / len(val_accuracy_dict)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "            # 记录验证损失和每个类别的准确率到 wandb\n",
    "        log_dict = {\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_accuracy\n",
    "        }\n",
    "        for class_name, accuracy in val_accuracy_dict.items():\n",
    "            log_dict[f\"val_accuracy_{class_name}\"] = accuracy\n",
    "\n",
    "        wandb.log(log_dict, step=step)\n",
    "\n",
    "    torch.save(mo_model.state_dict(), f\"{date}_r{lora_rank}_lr{lr}_bias{bias}_inter{inter_size}.pth\")\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate text input\n",
    "text_input_dict = {\n",
    "    class_name: processor(text=[f\"A photo of a {label}\" for label in label_names[class_name]], \n",
    "                          return_tensors=\"pt\", padding=True).to(device)\n",
    "    for class_name in selected_class_names\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search for lora dimension, learning rate, and inter size\n",
    "lora_dimensions = [8, 16, 32]\n",
    "learning_rates = [3e-5, 1e-4, 3e-4]\n",
    "inter_sizes = [64, 128, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_dimensions = [128, 256]\n",
    "\n",
    "lr = 1e-4\n",
    "for lora_rank in lora_dimensions:\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", cache_dir=\"model\", local_files_only=False)\n",
    "    train(model, train_dataloader, val_dataloader, device, text_input_dict, num_epochs=10, lora_rank=lora_rank, lora_alpha=lora_rank*2, lr=lr, bias=\"none\", inter_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_dimensions = [64, 128, 256]\n",
    "\n",
    "lr = 3e-5\n",
    "for lora_rank in lora_dimensions:\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", cache_dir=\"model\", local_files_only=False)\n",
    "    train(model, train_dataloader, val_dataloader, device, text_input_dict, num_epochs=10, lora_rank=lora_rank, lora_alpha=lora_rank*2, lr=lr, bias=\"none\", inter_size=128)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 3103714,
     "sourceId": 31254,
     "sourceType": "competition"
    },
    {
     "sourceId": 199081181,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
