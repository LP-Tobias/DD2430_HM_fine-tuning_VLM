{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T23:16:02.749950Z",
     "iopub.status.busy": "2024-10-08T23:16:02.749600Z",
     "iopub.status.idle": "2024-10-08T23:16:23.680407Z",
     "shell.execute_reply": "2024-10-08T23:16:23.679521Z",
     "shell.execute_reply.started": "2024-10-08T23:16:02.749912Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in /home/user/miniforge3/envs/hf/lib/python3.12/site-packages (0.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/user/miniforge3/envs/hf/lib/python3.12/site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/user/miniforge3/envs/hf/lib/python3.12/site-packages (from peft) (24.1)\n",
      "Requirement already satisfied: psutil in /home/user/miniforge3/envs/hf/lib/python3.12/site-packages (from peft) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /home/user/miniforge3/envs/hf/lib/python3.12/site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/user/miniforge3/envs/hf/lib/python3.12/site-packages (from peft) (2.4.0.post301)\n",
      "Requirement already satisfied: transformers in /home/user/miniforge3/envs/hf/lib/python3.12/site-packages (from peft) (4.44.2)\n",
      "Requirement already satisfied: tqdm in /home/user/miniforge3/envs/hf/lib/python3.12/site-packages (from peft) (4.66.5)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /home/user/miniforge3/envs/hf/lib/python3.12/site-packages (from peft) (0.34.2)\n",
      "Requirement already satisfied: safetensors in /home/user/miniforge3/envs/hf/lib/python3.12/site-packages (from peft) (0.4.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /home/user/miniforge3/envs/hf/lib/python3.12/site-packages (from peft) (0.24.6)\n",
      "Requirement already satisfied: filelock in /home/user/miniforge3/envs/hf/lib/python3.12/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/user/miniforge3/envs/hf/lib/python3.12/site-packages (from huggingface-hub>=0.17.0->peft) (2024.5.0)\n",
      "Requirement already satisfied: requests in /home/user/miniforge3/envs/hf/lib/python3.12/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/user/miniforge3/envs/hf/lib/python3.12/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/user/miniforge3/envs/hf/lib/python3.12/site-packages (from torch>=1.13.0->peft) (1.13.2)\n",
      "Requirement already satisfied: networkx in /home/user/miniforge3/envs/hf/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/user/miniforge3/envs/hf/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /home/user/miniforge3/envs/hf/lib/python3.12/site-packages (from torch>=1.13.0->peft) (73.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/user/miniforge3/envs/hf/lib/python3.12/site-packages (from transformers->peft) (2024.7.24)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/user/miniforge3/envs/hf/lib/python3.12/site-packages (from transformers->peft) (0.19.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/user/miniforge3/envs/hf/lib/python3.12/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/user/miniforge3/envs/hf/lib/python3.12/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/user/miniforge3/envs/hf/lib/python3.12/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/user/miniforge3/envs/hf/lib/python3.12/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/user/miniforge3/envs/hf/lib/python3.12/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/user/miniforge3/envs/hf/lib/python3.12/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install peft\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T23:16:23.682277Z",
     "iopub.status.busy": "2024-10-08T23:16:23.681971Z",
     "iopub.status.idle": "2024-10-08T23:16:23.692167Z",
     "shell.execute_reply": "2024-10-08T23:16:23.691237Z",
     "shell.execute_reply.started": "2024-10-08T23:16:23.682244Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_image_paths_and_labels_from_df(df, data_dir):\n",
    "    article_ids = df[\"article_id\"].values\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for article_id in article_ids:\n",
    "        image_path = f\"{data_dir}/images/0{str(article_id)[:2]}/0{article_id}.jpg\"\n",
    "        # Check if the image file exists\n",
    "        if os.path.exists(image_path):\n",
    "            image_paths.append(image_path)\n",
    "            # Add corresponding label only if the image exists\n",
    "            labels.append(df[df[\"article_id\"] == article_id])\n",
    "\n",
    "    return image_paths, labels\n",
    "\n",
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_paths, processor=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.processor = processor\n",
    "        self.image_ids = []\n",
    "\n",
    "        for image_path in self.image_paths:\n",
    "            if not os.path.exists(image_path):\n",
    "                raise FileNotFoundError(f\"Image {image_path} not found.\")\n",
    "            else:\n",
    "                image_id = int(image_path.split(\"/\")[-1].split(\".\")[0])\n",
    "                self.image_ids.append(image_id)\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx])\n",
    "        if self.processor is not None:\n",
    "            inputs = self.processor(images=image, return_tensors=\"pt\", padding=True)\n",
    "            image = inputs[\"pixel_values\"][0]\n",
    "        return image, self.image_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T23:16:23.693415Z",
     "iopub.status.busy": "2024-10-08T23:16:23.693151Z",
     "iopub.status.idle": "2024-10-08T23:16:23.712807Z",
     "shell.execute_reply": "2024-10-08T23:16:23.711985Z",
     "shell.execute_reply.started": "2024-10-08T23:16:23.693385Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f657bede6d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set random seed 42\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T23:16:23.715445Z",
     "iopub.status.busy": "2024-10-08T23:16:23.715179Z",
     "iopub.status.idle": "2024-10-08T23:16:41.065895Z",
     "shell.execute_reply": "2024-10-08T23:16:41.065076Z",
     "shell.execute_reply.started": "2024-10-08T23:16:23.715415Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniforge3/envs/hf/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", cache_dir=\"model\", local_files_only=False)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\", cache_dir=\"model\", local_files_only=False)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T23:16:41.067932Z",
     "iopub.status.busy": "2024-10-08T23:16:41.067176Z",
     "iopub.status.idle": "2024-10-08T23:16:42.006574Z",
     "shell.execute_reply": "2024-10-08T23:16:42.005334Z",
     "shell.execute_reply.started": "2024-10-08T23:16:41.067884Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "text_path = 'data/articles.csv'\n",
    "articles = pd.read_csv(text_path)\n",
    "data_dir = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T23:16:42.008508Z",
     "iopub.status.busy": "2024-10-08T23:16:42.008093Z",
     "iopub.status.idle": "2024-10-08T23:16:42.305622Z",
     "shell.execute_reply": "2024-10-08T23:16:42.304645Z",
     "shell.execute_reply.started": "2024-10-08T23:16:42.008443Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# map from article_id to df index\n",
    "article_id_to_idx = {article_id: idx for idx, article_id in enumerate(articles[\"article_id\"])}\n",
    "\n",
    "# get all classes of the dataframe\n",
    "class_names = articles.columns.tolist()\n",
    "label_names = dict()\n",
    "label_names_to_idx = dict()\n",
    "for class_name in class_names:\n",
    "    label_names[class_name] = articles[class_name].unique()\n",
    "    label_names_to_idx[class_name] = {label_name: idx for idx, label_name in enumerate(label_names[class_name])}\n",
    "\n",
    "article_ids = label_names[\"article_id\"]\n",
    "#selected_class_names = [\"product_type_name\", \"graphical_appearance_name\"]\n",
    "selected_class_names = [\"product_group_name\", \"product_type_name\", \"graphical_appearance_name\", \"colour_group_name\", \"perceived_colour_value_name\", \"perceived_colour_master_name\", \"department_name\", \"index_name\", \"index_group_name\", \"section_name\", \"garment_group_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T23:16:42.307287Z",
     "iopub.status.busy": "2024-10-08T23:16:42.306975Z",
     "iopub.status.idle": "2024-10-08T23:17:04.605101Z",
     "shell.execute_reply": "2024-10-08T23:17:04.604169Z",
     "shell.execute_reply.started": "2024-10-08T23:16:42.307254Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_df)=84445 len(val_df)=10534 len(test_df)=10563\n"
     ]
    }
   ],
   "source": [
    "# grouped by product_code\n",
    "grouped = articles.groupby(\"product_code\")\n",
    "groups = [group for _, group in grouped]\n",
    "\n",
    "# split 0.8/0.1/0.1\n",
    "train_groups, test_groups = train_test_split(groups, test_size=0.2, random_state=42) \n",
    "val_groups, test_groups = train_test_split(test_groups, test_size=0.5, random_state=42) \n",
    "\n",
    "train_df = pd.concat(train_groups)\n",
    "val_df = pd.concat(val_groups)\n",
    "test_df = pd.concat(test_groups)\n",
    "\n",
    "print(f\"{len(train_df)=} {len(val_df)=} {len(test_df)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T23:17:04.606390Z",
     "iopub.status.busy": "2024-10-08T23:17:04.606114Z",
     "iopub.status.idle": "2024-10-08T23:22:28.822877Z",
     "shell.execute_reply": "2024-10-08T23:22:28.822067Z",
     "shell.execute_reply.started": "2024-10-08T23:17:04.606360Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_paths, train_labels = get_image_paths_and_labels_from_df(train_df, data_dir)\n",
    "val_paths, val_labels = get_image_paths_and_labels_from_df(val_df, data_dir)\n",
    "test_paths, test_labels = get_image_paths_and_labels_from_df(test_df, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T23:22:28.824716Z",
     "iopub.status.busy": "2024-10-08T23:22:28.824358Z",
     "iopub.status.idle": "2024-10-08T23:22:28.831472Z",
     "shell.execute_reply": "2024-10-08T23:22:28.830508Z",
     "shell.execute_reply.started": "2024-10-08T23:22:28.824678Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiOutputLayer(torch.nn.Module):\n",
    "    def __init__(self, input_size, inter_size, output_size):\n",
    "        super(MultiOutputLayer, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, inter_size)\n",
    "        self.fc2 = torch.nn.Linear(inter_size, output_size)\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.act = torch.nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T23:22:28.836711Z",
     "iopub.status.busy": "2024-10-08T23:22:28.836295Z",
     "iopub.status.idle": "2024-10-08T23:22:28.848992Z",
     "shell.execute_reply": "2024-10-08T23:22:28.848117Z",
     "shell.execute_reply.started": "2024-10-08T23:22:28.836676Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiOutputClipModel(torch.nn.Module):\n",
    "    def __init__(self, clip_model, class_names, vision_hidden_size, inter_size, output_size):\n",
    "        super(MultiOutputClipModel, self).__init__()\n",
    "        self.clip_model = clip_model\n",
    "        self.class_names = class_names\n",
    "        self.output_layers = torch.nn.ModuleDict({\n",
    "            class_name: MultiOutputLayer(vision_hidden_size, inter_size, output_size)\n",
    "            for class_name in self.class_names\n",
    "        })\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        text_input_dict,\n",
    "        pixel_values,\n",
    "        # position_ids = None,\n",
    "        output_attentions = None,\n",
    "        output_hidden_states = None,\n",
    "        return_dict = None,\n",
    "    ):\n",
    "\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.clip_model.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.clip_model.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.clip_model.config.use_return_dict\n",
    "\n",
    "        vision_outputs = self.clip_model.vision_model(\n",
    "            pixel_values=pixel_values,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        vision_embeds = vision_outputs[1]\n",
    "        vision_embeds_dict = {\n",
    "            class_name: output_layer(vision_embeds) \n",
    "                for class_name, output_layer in self.output_layers.items()\n",
    "        }\n",
    "\n",
    "        text_outputs_dict = {\n",
    "            class_name: self.clip_model.text_model(\n",
    "                input_ids=text_input_dict[class_name][\"input_ids\"],\n",
    "                attention_mask=text_input_dict[class_name][\"attention_mask\"],\n",
    "                # position_ids=position_ids,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            ) for class_name in self.class_names\n",
    "        }\n",
    "\n",
    "        text_embeds_dict = {\n",
    "            class_name: self.clip_model.text_projection(text_outputs[1])\n",
    "                for class_name, text_outputs in text_outputs_dict.items()\n",
    "        }\n",
    "\n",
    "        logits_per_image_dict = {\n",
    "            class_name: vision_embeds_dict[class_name] @ text_embeds_dict[class_name].T\n",
    "                for class_name in self.output_layers.keys()\n",
    "        }\n",
    "\n",
    "        return logits_per_image_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T23:22:28.850613Z",
     "iopub.status.busy": "2024-10-08T23:22:28.850234Z",
     "iopub.status.idle": "2024-10-08T23:22:28.863065Z",
     "shell.execute_reply": "2024-10-08T23:22:28.862214Z",
     "shell.execute_reply.started": "2024-10-08T23:22:28.850571Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# custom criterion: cross entropy loss across all classes\n",
    "class MultiOutputClipCriterion(torch.nn.Module):\n",
    "    def __init__(self, class_names):\n",
    "        super(MultiOutputClipCriterion, self).__init__()\n",
    "        self.class_names = class_names\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, logits_dict, labels_dict):\n",
    "        loss = 0\n",
    "        for class_name in self.class_names:\n",
    "            logits = logits_dict[class_name]\n",
    "            labels = labels_dict[class_name]\n",
    "            loss += self.criterion(logits, labels)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T23:24:16.468053Z",
     "iopub.status.busy": "2024-10-08T23:24:16.467616Z",
     "iopub.status.idle": "2024-10-08T23:24:57.730306Z",
     "shell.execute_reply": "2024-10-08T23:24:57.729503Z",
     "shell.execute_reply.started": "2024-10-08T23:24:16.468014Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "train_dataset = ImageDataset(train_paths, processor)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=8)\n",
    "\n",
    "val_dataset = ImageDataset(val_paths, processor)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=8)\n",
    "\n",
    "test_dataset = ImageDataset(test_paths, processor)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criteria, device, text_inputs, class_names):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = {class_name: 0 for class_name in class_names}\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, image_ids in tqdm(dataloader):\n",
    "            images = images.to(device)\n",
    "            logits_per_image_dict = model(pixel_values=images, text_input_dict=text_inputs)\n",
    "\n",
    "            # 获取真实标签\n",
    "            true_labels_dict = {\n",
    "                class_name: [label_names_to_idx[class_name][articles.loc[article_id_to_idx[image_id.item()], class_name]] \n",
    "                             for image_id in image_ids]\n",
    "                for class_name in class_names\n",
    "            }\n",
    "            true_labels_dict = {class_name: torch.tensor(true_labels).to(device)\n",
    "                                for class_name, true_labels in true_labels_dict.items()}\n",
    "            \n",
    "            # 计算损失\n",
    "            loss = criteria(logits_per_image_dict, true_labels_dict)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "\n",
    "            # 计算准确率\n",
    "            total_samples += images.size(0)\n",
    "            for class_name in class_names:\n",
    "                _, preds = torch.max(logits_per_image_dict[class_name], dim=1)\n",
    "                total_correct[class_name] += (preds == true_labels_dict[class_name]).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / total_samples / len(class_names)\n",
    "    accuracy = {class_name: total_correct[class_name] / total_samples for class_name in class_names}\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get current date and time\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "date = now.strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, val_dataloader, device, text_input_dict, num_epochs=10, lora_rank=8, lora_alpha=32, lr=1e-4, bias=\"none\", inter_size=128):\n",
    "\n",
    "    wandb.init(project=\"multi-output-clip\", name=f\"{date}_r{lora_rank}_lr{lr}_bias{bias}_inter{inter_size}\")\n",
    "    step = 0\n",
    "\n",
    "    # Define LoRA configuration\n",
    "    lora_config = LoraConfig(\n",
    "        r=lora_rank,                  # Low-rank dimension (adjustable)\n",
    "        lora_alpha=lora_alpha,          # Scaling factor (adjustable)\n",
    "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\"],  # Specify which layers to apply LoRA to\n",
    "        lora_dropout=0.05,       # Dropout rate (optional)\n",
    "        bias=bias,            # Whether to include biases (\"none\", \"all\", \"lora_only\")\n",
    "        task_type=\"classification\"  # Task type (\"classification\" or \"regression\")\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    mo_model = MultiOutputClipModel(model, selected_class_names, 768, inter_size, 512).to(device)\n",
    "\n",
    "    criteria = MultiOutputClipCriterion(selected_class_names)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        mo_model.train()\n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for images, image_ids in tqdm(train_dataloader):\n",
    "            images = images.to(device)\n",
    "            logits_per_image_dict = mo_model(pixel_values=images, text_input_dict=text_input_dict)\n",
    "\n",
    "            # 获取真实标签\n",
    "            true_labels_dict = {\n",
    "                class_name: [label_names_to_idx[class_name][articles.loc[article_id_to_idx[image_id.item()], class_name]] \n",
    "                            for image_id in image_ids]\n",
    "                for class_name in selected_class_names\n",
    "            }\n",
    "            true_labels_dict = {class_name: torch.tensor(true_labels).to(device) \n",
    "                                for class_name, true_labels in true_labels_dict.items()}\n",
    "\n",
    "            # 计算损失\n",
    "            loss = criteria(logits_per_image_dict, true_labels_dict)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "\n",
    "            # 计算准确率\n",
    "            correct = 0\n",
    "            total_samples += images.size(0)\n",
    "            for class_name in selected_class_names:\n",
    "                _, preds = torch.max(logits_per_image_dict[class_name], dim=1)\n",
    "                correct += (preds == true_labels_dict[class_name]).sum().item()\n",
    "            total_correct += correct\n",
    "\n",
    "            # 反向传播和优化\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 记录训练损失和准确率到 wandb\n",
    "            # 在训练循环中，记录每个类别的准确率\n",
    "            log_dict = {\n",
    "                \"train_loss\": loss.item(),\n",
    "                \"train_accuracy\": correct / images.size(0) / len(selected_class_names)\n",
    "            }\n",
    "    #         for class_name in selected_class_names:\n",
    "    #             accuracy = total_correct_per_class[class_name] / total_samples\n",
    "    #             log_dict[f\"train_accuracy_{class_name}\"] = accuracy\n",
    "\n",
    "            wandb.log(log_dict, step=step)\n",
    "            step += 1\n",
    "\n",
    "        avg_loss = total_loss / total_samples / len(selected_class_names)\n",
    "        accuracy = total_correct / total_samples / len(selected_class_names)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_loss:.4f}, Train Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        # 在每个 epoch 结束后进行验证\n",
    "        val_loss, val_accuracy_dict = validate(mo_model, val_dataloader, criteria, device, text_input_dict, selected_class_names)\n",
    "        val_accuracy = sum(val_accuracy_dict.values()) / len(val_accuracy_dict)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "            # 记录验证损失和每个类别的准确率到 wandb\n",
    "        log_dict = {\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_accuracy\n",
    "        }\n",
    "        for class_name, accuracy in val_accuracy_dict.items():\n",
    "            log_dict[f\"val_accuracy_{class_name}\"] = accuracy\n",
    "\n",
    "        wandb.log(log_dict, step=step)\n",
    "        torch.save(mo_model.state_dict(), f\"{date}_r{lora_rank}_lr{lr}_bias{bias}_inter{inter_size}_epoch{epoch}.pth\")\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate text input\n",
    "text_input_dict = {\n",
    "    class_name: processor(text=[f\"A photo of a {label}\" for label in label_names[class_name]], \n",
    "                          return_tensors=\"pt\", padding=True).to(device)\n",
    "    for class_name in selected_class_names\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", cache_dir=\"model\", local_files_only=False)\n",
    "# train(model, train_dataloader, val_dataloader, device, text_input_dict, num_epochs=10, lora_rank=8, lr=1e-4, bias=\"none\", inter_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search for lora dimension, learning rate, and inter size\n",
    "lora_dimensions = [8, 16, 32]\n",
    "learning_rates = [1e-5, 1e-4, 1e-3]\n",
    "inter_sizes = [64, 128, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33monjackay\u001b[0m (\u001b[33monjackay-kth-royal-institute-of-technology\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/kth/dd2430/DD2430_HM_fine-tuning_VLM/wandb/run-20241112_143503-ic4bqc1i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/multi-output-clip/runs/ic4bqc1i' target=\"_blank\">multi-output-clip</a></strong> to <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/multi-output-clip' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/multi-output-clip' target=\"_blank\">https://wandb.ai/onjackay-kth-royal-institute-of-technology/multi-output-clip</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/multi-output-clip/runs/ic4bqc1i' target=\"_blank\">https://wandb.ai/onjackay-kth-royal-institute-of-technology/multi-output-clip/runs/ic4bqc1i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [23:10<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 2.0505, Train Accuracy: 0.4325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [03:24<00:00,  4.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.4391, Validation Accuracy: 0.5794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [23:29<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Train Loss: 1.4475, Train Accuracy: 0.5730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [03:02<00:00,  4.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.2475, Validation Accuracy: 0.6285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [23:19<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Train Loss: 1.3020, Train Accuracy: 0.6111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [03:21<00:00,  4.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.1616, Validation Accuracy: 0.6509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [25:33<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Train Loss: 1.2174, Train Accuracy: 0.6344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [03:17<00:00,  4.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.1078, Validation Accuracy: 0.6651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [23:50<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Train Loss: 1.1563, Train Accuracy: 0.6509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [03:26<00:00,  5.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.0618, Validation Accuracy: 0.6783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [24:15<00:00,  2.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Train Loss: 1.1111, Train Accuracy: 0.6629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [03:12<00:00,  4.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.0343, Validation Accuracy: 0.6860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [23:57<00:00,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Train Loss: 1.0736, Train Accuracy: 0.6738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [03:01<00:00,  4.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.0075, Validation Accuracy: 0.6933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [23:53<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Train Loss: 1.0401, Train Accuracy: 0.6827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [03:17<00:00,  4.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.9931, Validation Accuracy: 0.6973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [24:02<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Train Loss: 1.0127, Train Accuracy: 0.6907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [03:17<00:00,  4.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.9765, Validation Accuracy: 0.7043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [28:07<00:00,  2.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Train Loss: 0.9896, Train Accuracy: 0.6971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [03:32<00:00,  5.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.9752, Validation Accuracy: 0.7028\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a181ad1662f94a0fa95973db817cb710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.007 MB of 0.007 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>▁▃▄▄▅▅▅▆▆▇▆▆▆▆▇▇▆▇▇▇▇▇▇▇▇▇█▇▇██▇▇▇██████</td></tr><tr><td>train_loss</td><td>█▇▅▃▃▃▃▃▂▃▂▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▂</td></tr><tr><td>val_accuracy</td><td>▁▄▅▆▇▇▇███</td></tr><tr><td>val_accuracy_colour_group_name</td><td>▁▄▆▆▆▇▇███</td></tr><tr><td>val_accuracy_department_name</td><td>▁▃▅▆▆▇▇▇██</td></tr><tr><td>val_accuracy_garment_group_name</td><td>▁▄▅▆▆▇▇▇██</td></tr><tr><td>val_accuracy_graphical_appearance_name</td><td>▁▄▅▆▅▆▆█▇█</td></tr><tr><td>val_accuracy_index_group_name</td><td>▁▃▄▆▇▇▇███</td></tr><tr><td>val_accuracy_index_name</td><td>▁▄▅▆▇▇▇▇██</td></tr><tr><td>val_accuracy_perceived_colour_master_name</td><td>▁▅▆▆▆▇▇███</td></tr><tr><td>val_accuracy_perceived_colour_value_name</td><td>▁▅▆▆▇▆███▇</td></tr><tr><td>val_accuracy_product_group_name</td><td>▁▄▅▆█▇█▇██</td></tr><tr><td>val_accuracy_product_type_name</td><td>▁▄▅▆▇▇▇▇██</td></tr><tr><td>val_accuracy_section_name</td><td>▁▃▅▆▇▇▇███</td></tr><tr><td>val_loss</td><td>█▅▄▃▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>0.7</td></tr><tr><td>train_loss</td><td>10.32464</td></tr><tr><td>val_accuracy</td><td>0.70278</td></tr><tr><td>val_accuracy_colour_group_name</td><td>0.677</td></tr><tr><td>val_accuracy_department_name</td><td>0.45785</td></tr><tr><td>val_accuracy_garment_group_name</td><td>0.70294</td></tr><tr><td>val_accuracy_graphical_appearance_name</td><td>0.7346</td></tr><tr><td>val_accuracy_index_group_name</td><td>0.79687</td></tr><tr><td>val_accuracy_index_name</td><td>0.72277</td></tr><tr><td>val_accuracy_perceived_colour_master_name</td><td>0.78972</td></tr><tr><td>val_accuracy_perceived_colour_value_name</td><td>0.72964</td></tr><tr><td>val_accuracy_product_group_name</td><td>0.91226</td></tr><tr><td>val_accuracy_product_type_name</td><td>0.71944</td></tr><tr><td>val_accuracy_section_name</td><td>0.48751</td></tr><tr><td>val_loss</td><td>0.97519</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">multi-output-clip</strong> at: <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/multi-output-clip/runs/ic4bqc1i' target=\"_blank\">https://wandb.ai/onjackay-kth-royal-institute-of-technology/multi-output-clip/runs/ic4bqc1i</a><br/> View project at: <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/multi-output-clip' target=\"_blank\">https://wandb.ai/onjackay-kth-royal-institute-of-technology/multi-output-clip</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241112_143503-ic4bqc1i/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/kth/dd2430/DD2430_HM_fine-tuning_VLM/wandb/run-20241112_191152-q78pjbj0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/multi-output-clip/runs/q78pjbj0' target=\"_blank\">multi-output-clip</a></strong> to <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/multi-output-clip' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/multi-output-clip' target=\"_blank\">https://wandb.ai/onjackay-kth-royal-institute-of-technology/multi-output-clip</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/multi-output-clip/runs/q78pjbj0' target=\"_blank\">https://wandb.ai/onjackay-kth-royal-institute-of-technology/multi-output-clip/runs/q78pjbj0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [29:34<00:00,  2.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 1.8596, Train Accuracy: 0.4760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [03:36<00:00,  5.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.3010, Validation Accuracy: 0.6153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [30:12<00:00,  2.76s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Train Loss: 1.3200, Train Accuracy: 0.6054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [03:17<00:00,  4.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.1455, Validation Accuracy: 0.6531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [30:48<00:00,  2.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Train Loss: 1.1822, Train Accuracy: 0.6428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [03:47<00:00,  5.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.0590, Validation Accuracy: 0.6789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [30:57<00:00,  2.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Train Loss: 1.0923, Train Accuracy: 0.6686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [03:35<00:00,  5.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.0130, Validation Accuracy: 0.6900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [29:53<00:00,  2.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Train Loss: 1.0310, Train Accuracy: 0.6855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [03:27<00:00,  5.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.9803, Validation Accuracy: 0.7001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [28:47<00:00,  2.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Train Loss: 0.9813, Train Accuracy: 0.6994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [03:20<00:00,  4.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.9596, Validation Accuracy: 0.7039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [30:20<00:00,  2.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Train Loss: 0.9415, Train Accuracy: 0.7102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/41 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "lr = 1e-4\n",
    "for lora_rank in lora_dimensions:\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", cache_dir=\"model\", local_files_only=False)\n",
    "    train(model, train_dataloader, val_dataloader, device, text_input_dict, num_epochs=10, lora_rank=lora_rank, lora_alpha=lora_rank*2, lr=lr, bias=\"none\", inter_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33monjackay\u001b[0m (\u001b[33monjackay-kth-royal-institute-of-technology\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/kth/dd2430/DD2430_HM_fine-tuning_VLM/wandb/run-20241113_101559-92tsdi8d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/multi-output-clip/runs/92tsdi8d' target=\"_blank\">20241113-101331_r32_lr0.0001_biasnone_inter128</a></strong> to <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/multi-output-clip' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/multi-output-clip' target=\"_blank\">https://wandb.ai/onjackay-kth-royal-institute-of-technology/multi-output-clip</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/multi-output-clip/runs/92tsdi8d' target=\"_blank\">https://wandb.ai/onjackay-kth-royal-institute-of-technology/multi-output-clip/runs/92tsdi8d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [25:49<00:00,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 1.7068, Train Accuracy: 0.5119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [03:25<00:00,  5.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.2077, Validation Accuracy: 0.6344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [27:11<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Train Loss: 1.2023, Train Accuracy: 0.6364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [03:29<00:00,  5.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.0431, Validation Accuracy: 0.6808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [26:05<00:00,  2.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Train Loss: 1.0620, Train Accuracy: 0.6769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [03:21<00:00,  4.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.9803, Validation Accuracy: 0.6984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [27:03<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Train Loss: 0.9719, Train Accuracy: 0.7020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [03:20<00:00,  4.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.9360, Validation Accuracy: 0.7116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [24:46<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Train Loss: 0.9046, Train Accuracy: 0.7216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [03:07<00:00,  4.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.9096, Validation Accuracy: 0.7193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [26:06<00:00,  2.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Train Loss: 0.8501, Train Accuracy: 0.7367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [03:19<00:00,  4.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.8968, Validation Accuracy: 0.7262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [26:50<00:00,  2.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Train Loss: 0.8026, Train Accuracy: 0.7506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [03:17<00:00,  4.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.8963, Validation Accuracy: 0.7273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [26:15<00:00,  2.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Train Loss: 0.7603, Train Accuracy: 0.7625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [03:14<00:00,  4.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.8939, Validation Accuracy: 0.7311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [26:09<00:00,  2.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Train Loss: 0.7237, Train Accuracy: 0.7736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [03:17<00:00,  4.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.9015, Validation Accuracy: 0.7335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [26:47<00:00,  2.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Train Loss: 0.6891, Train Accuracy: 0.7835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [03:16<00:00,  4.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.9123, Validation Accuracy: 0.7338\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d1b1d93102400988dc4481308733a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.007 MB of 0.007 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>▁▁▃▄▄▅▅▆▆▅▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇█▇▇▇▇▇█▇▇▇▇██▇█</td></tr><tr><td>train_loss</td><td>█▆▄▄▄▃▃▃▃▃▃▃▂▃▃▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▆▇▇████</td></tr><tr><td>val_accuracy_colour_group_name</td><td>▁▄▅▆▆▇▇█▇█</td></tr><tr><td>val_accuracy_department_name</td><td>▁▄▅▆▇▇▇███</td></tr><tr><td>val_accuracy_garment_group_name</td><td>▁▅▅▆▇█▇███</td></tr><tr><td>val_accuracy_graphical_appearance_name</td><td>▁▄▅▆▇▇▇███</td></tr><tr><td>val_accuracy_index_group_name</td><td>▁▄▆▇▇█▇███</td></tr><tr><td>val_accuracy_index_name</td><td>▁▅▆▇▇█████</td></tr><tr><td>val_accuracy_perceived_colour_master_name</td><td>▁▄▅▆▆▇██▇█</td></tr><tr><td>val_accuracy_perceived_colour_value_name</td><td>▁▄▆▆▆▇█▆▇█</td></tr><tr><td>val_accuracy_product_group_name</td><td>▁▄▆▇▇▇▇███</td></tr><tr><td>val_accuracy_product_type_name</td><td>▁▄▅▆▆▇▇███</td></tr><tr><td>val_accuracy_section_name</td><td>▁▄▅▆▇▇████</td></tr><tr><td>val_loss</td><td>█▄▃▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>0.78017</td></tr><tr><td>train_loss</td><td>7.58952</td></tr><tr><td>val_accuracy</td><td>0.7338</td></tr><tr><td>val_accuracy_colour_group_name</td><td>0.70256</td></tr><tr><td>val_accuracy_department_name</td><td>0.51364</td></tr><tr><td>val_accuracy_garment_group_name</td><td>0.73508</td></tr><tr><td>val_accuracy_graphical_appearance_name</td><td>0.74871</td></tr><tr><td>val_accuracy_index_group_name</td><td>0.82443</td></tr><tr><td>val_accuracy_index_name</td><td>0.76235</td></tr><tr><td>val_accuracy_perceived_colour_master_name</td><td>0.80374</td></tr><tr><td>val_accuracy_perceived_colour_value_name</td><td>0.74833</td></tr><tr><td>val_accuracy_product_group_name</td><td>0.92438</td></tr><tr><td>val_accuracy_product_type_name</td><td>0.76616</td></tr><tr><td>val_accuracy_section_name</td><td>0.54244</td></tr><tr><td>val_loss</td><td>0.91232</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">20241113-101331_r32_lr0.0001_biasnone_inter128</strong> at: <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/multi-output-clip/runs/92tsdi8d' target=\"_blank\">https://wandb.ai/onjackay-kth-royal-institute-of-technology/multi-output-clip/runs/92tsdi8d</a><br/> View project at: <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/multi-output-clip' target=\"_blank\">https://wandb.ai/onjackay-kth-royal-institute-of-technology/multi-output-clip</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241113_101559-92tsdi8d/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lora_dimensions = [32]\n",
    "\n",
    "lr = 1e-4\n",
    "for lora_rank in lora_dimensions:\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", cache_dir=\"model\", local_files_only=False)\n",
    "    train(model, train_dataloader, val_dataloader, device, text_input_dict, num_epochs=10, lora_rank=lora_rank, lora_alpha=lora_rank*2, lr=lr, bias=\"none\", inter_size=128)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 3103714,
     "sourceId": 31254,
     "sourceType": "competition"
    },
    {
     "sourceId": 199081181,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
