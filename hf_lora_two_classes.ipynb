{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "from src import util\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff2491c9030>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set random seed 42\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniforge3/envs/hf/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", \n",
    "                                  cache_dir=\"model\", local_files_only=True)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\", \n",
    "                                          cache_dir=\"model\", local_files_only=True)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data\"\n",
    "# articles_filtered: remove articles with no images\n",
    "articles = pd.read_csv(f\"{data_dir}/articles_filtered.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map from article_id to df index\n",
    "article_id_to_idx = {article_id: idx for idx, article_id in enumerate(articles[\"article_id\"])}\n",
    "\n",
    "# get all classes of the dataframe\n",
    "class_names = articles.columns.tolist()\n",
    "label_names = dict()\n",
    "label_names_to_idx = dict()\n",
    "for class_name in class_names:\n",
    "    label_names[class_name] = articles[class_name].unique()\n",
    "    label_names_to_idx[class_name] = {label_name: idx for idx, label_name in enumerate(label_names[class_name])}\n",
    "\n",
    "article_ids = label_names[\"article_id\"]\n",
    "# selected_class_names = [\"product_group_name\", \"product_type_name\", \"graphical_appearance_name\", \"colour_group_name\", \"perceived_colour_value_name\", \"perceived_colour_master_name\", \"department_name\", \"index_name\", \"index_group_name\", \"section_name\", \"garment_group_name\"]\n",
    "selected_class_names = [\"product_type_name\", \"graphical_appearance_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_df)=63272 len(val_df)=20882 len(test_df)=20946\n"
     ]
    }
   ],
   "source": [
    "# grouped by product_code\n",
    "grouped = articles.groupby(\"product_code\")\n",
    "groups = [group for _, group in grouped]\n",
    "\n",
    "# split 0.6/0.2/0.2\n",
    "train_groups, test_groups = train_test_split(groups, test_size=0.4, random_state=42) \n",
    "val_groups, test_groups = train_test_split(test_groups, test_size=0.5, random_state=42) \n",
    "\n",
    "train_df = pd.concat(train_groups)\n",
    "val_df = pd.concat(val_groups)\n",
    "test_df = pd.concat(test_groups)\n",
    "\n",
    "print(f\"{len(train_df)=} {len(val_df)=} {len(test_df)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths, train_labels = util.get_image_paths_and_labels_from_df(train_df, data_dir)\n",
    "val_paths, val_labels = util.get_image_paths_and_labels_from_df(val_df, data_dir)\n",
    "test_paths, test_labels = util.get_image_paths_and_labels_from_df(test_df, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiOutputLayer(torch.nn.Module):\n",
    "    def __init__(self, input_size, inter_size, output_size):\n",
    "        super(MultiOutputLayer, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, inter_size)\n",
    "        self.fc2 = torch.nn.Linear(inter_size, output_size)\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.act = torch.nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiOutputClipModel(torch.nn.Module):\n",
    "    def __init__(self, clip_model, class_names, vision_hidden_size, inter_size, output_size):\n",
    "        super(MultiOutputClipModel, self).__init__()\n",
    "        self.clip_model = clip_model\n",
    "        self.class_names = class_names\n",
    "        self.output_layers = torch.nn.ModuleDict({\n",
    "            class_name: MultiOutputLayer(vision_hidden_size, inter_size, output_size)\n",
    "            for class_name in self.class_names\n",
    "        })\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        text_input_dict,\n",
    "        pixel_values,\n",
    "        # position_ids = None,\n",
    "        output_attentions = None,\n",
    "        output_hidden_states = None,\n",
    "        return_dict = None,\n",
    "    ):\n",
    "\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.clip_model.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.clip_model.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.clip_model.config.use_return_dict\n",
    "\n",
    "        vision_outputs = self.clip_model.vision_model(\n",
    "            pixel_values=pixel_values,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        vision_embeds = vision_outputs[1]\n",
    "        vision_embeds_dict = {\n",
    "            class_name: output_layer(vision_embeds) \n",
    "                for class_name, output_layer in self.output_layers.items()\n",
    "        }\n",
    "\n",
    "        text_outputs_dict = {\n",
    "            class_name: self.clip_model.text_model(\n",
    "                input_ids=text_input_dict[class_name][\"input_ids\"],\n",
    "                attention_mask=text_input_dict[class_name][\"attention_mask\"],\n",
    "                # position_ids=position_ids,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            ) for class_name in self.class_names\n",
    "        }\n",
    "\n",
    "        text_embeds_dict = {\n",
    "            class_name: self.clip_model.text_projection(text_outputs[1])\n",
    "                for class_name, text_outputs in text_outputs_dict.items()\n",
    "        }\n",
    "\n",
    "        logits_per_image_dict = {\n",
    "            class_name: vision_embeds_dict[class_name] @ text_embeds_dict[class_name].T\n",
    "                for class_name in self.output_layers.keys()\n",
    "        }\n",
    "\n",
    "        return logits_per_image_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom criterion: cross entropy loss across all classes\n",
    "class MultiOutputClipCriterion(torch.nn.Module):\n",
    "    def __init__(self, class_names):\n",
    "        super(MultiOutputClipCriterion, self).__init__()\n",
    "        self.class_names = class_names\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, logits_dict, labels_dict):\n",
    "        loss = 0\n",
    "        for class_name in self.class_names:\n",
    "            logits = logits_dict[class_name]\n",
    "            labels = labels_dict[class_name]\n",
    "            loss += self.criterion(logits, labels)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = util.ImageDataset(train_paths, processor)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all parameters in model\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiOutputClipModel(\n",
       "  (clip_model): CLIPModel(\n",
       "    (text_model): CLIPTextTransformer(\n",
       "      (embeddings): CLIPTextEmbeddings(\n",
       "        (token_embedding): Embedding(49408, 512)\n",
       "        (position_embedding): Embedding(77, 512)\n",
       "      )\n",
       "      (encoder): CLIPEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x CLIPEncoderLayer(\n",
       "            (self_attn): CLIPSdpaAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (vision_model): CLIPVisionTransformer(\n",
       "      (embeddings): CLIPVisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "        (position_embedding): Embedding(50, 768)\n",
       "      )\n",
       "      (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder): CLIPEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x CLIPEncoderLayer(\n",
       "            (self_attn): CLIPSdpaAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "    (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       "  )\n",
       "  (output_layers): ModuleDict(\n",
       "    (product_type_name): MultiOutputLayer(\n",
       "      (fc1): Linear(in_features=768, out_features=32, bias=True)\n",
       "      (fc2): Linear(in_features=32, out_features=512, bias=True)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (graphical_appearance_name): MultiOutputLayer(\n",
       "      (fc1): Linear(in_features=768, out_features=32, bias=True)\n",
       "      (fc2): Linear(in_features=32, out_features=512, bias=True)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mo_model = MultiOutputClipModel(model, selected_class_names, 768, 32, 512).to(device)\n",
    "mo_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_layers.product_type_name.fc1.weight\n",
      "output_layers.product_type_name.fc1.bias\n",
      "output_layers.product_type_name.fc2.weight\n",
      "output_layers.product_type_name.fc2.bias\n",
      "output_layers.graphical_appearance_name.fc1.weight\n",
      "output_layers.graphical_appearance_name.fc1.bias\n",
      "output_layers.graphical_appearance_name.fc2.weight\n",
      "output_layers.graphical_appearance_name.fc2.bias\n"
     ]
    }
   ],
   "source": [
    "# show all trainable parameters in mo_model\n",
    "for name, param in mo_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate text input\n",
    "text_input_dict = {\n",
    "    class_name: processor(text=[f\"A photo of a {label}\" for label in label_names[class_name]], \n",
    "                          return_tensors=\"pt\", padding=True).to(device)\n",
    "    for class_name in selected_class_names\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33monjackay\u001b[0m (\u001b[33monjackay-kth-royal-institute-of-technology\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/kth/2430/DD2430_HM_fine-tuning_VLM/wandb/run-20241002_121345-2g6awgi2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-multi-output/runs/2g6awgi2' target=\"_blank\">genial-grass-9</a></strong> to <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-multi-output' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-multi-output' target=\"_blank\">https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-multi-output</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-multi-output/runs/2g6awgi2' target=\"_blank\">https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-multi-output/runs/2g6awgi2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 495/495 [35:22<00:00,  4.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: 2.1472, Accuracy: 0.4863\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06a55355e10c4f54b36b5633a5ba1ac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▃▄▄▅▅▅▅▆▆▆▆▇▇▆▆▆▇▇▇▇█▇▇▇▇▇▇▇█▇▇█▇██▇▇▇▇</td></tr><tr><td>loss</td><td>███▇▆▅▅▅▄▄▃▃▃▃▃▃▂▂▂▃▂▃▂▂▂▂▂▂▂▁▂▂▂▁▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.5625</td></tr><tr><td>loss</td><td>3.68765</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">genial-grass-9</strong> at: <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-multi-output/runs/2g6awgi2' target=\"_blank\">https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-multi-output/runs/2g6awgi2</a><br/> View project at: <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-multi-output' target=\"_blank\">https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-multi-output</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241002_121345-2g6awgi2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs = 1  # Adjust as needed\n",
    "criteria = MultiOutputClipCriterion(class_names=selected_class_names)\n",
    "optimizer = torch.optim.AdamW(mo_model.parameters(), lr=1e-4)\n",
    "step = 0\n",
    "wandb.init(project=\"clip-multi-output\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    mo_model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for images, image_ids in tqdm(train_dataloader):\n",
    "        images = images.to(device)\n",
    "        logits_per_image_dict = mo_model(pixel_values=images, text_input_dict=text_input_dict)\n",
    "\n",
    "        # Get true labels from image_ids\n",
    "        true_labels_dict = {\n",
    "            class_name: [label_names_to_idx[class_name][articles.loc[article_id_to_idx[image_id.item()], class_name]] \n",
    "                       for image_id in image_ids]\n",
    "            for class_name in selected_class_names\n",
    "        }\n",
    "        true_labels_dict = {class_name: torch.tensor(true_labels).to(device) \n",
    "                            for class_name, true_labels in true_labels_dict.items()}\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criteria(logits_per_image_dict, true_labels_dict)\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "\n",
    "        # Predictions and accuracy\n",
    "        correct = 0\n",
    "        total_samples += images.size(0)\n",
    "        for class_name in selected_class_names:\n",
    "            _, preds = torch.max(logits_per_image_dict[class_name], dim=1)\n",
    "            correct += (preds == true_labels_dict[class_name]).sum().item()\n",
    "        total_correct += correct\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # log the loss and accuracy to wandb\n",
    "        wandb.log({\"loss\": loss.item(), \"accuracy\": correct / images.size(0) / len(selected_class_names)},\n",
    "                  step=step)\n",
    "        step += 1\n",
    "\n",
    "    avg_loss = total_loss / total_samples / len(selected_class_names)\n",
    "    accuracy = total_correct / total_samples / len(selected_class_names)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Validate after each epoch\n",
    "    # val_loss, val_accuracy = validate(model, val_dataloader, criteria, device, text_inputs, class_name)\n",
    "    # print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "# Save the model\n",
    "torch.save(mo_model.state_dict(), \"model/2_output_clip_model-2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criteria, device, text_inputs, class_names):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = {class_name: 0 for class_name in class_names}\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, image_ids in tqdm(dataloader):\n",
    "            images = images.to(device)\n",
    "            logits_per_image_dict = model(pixel_values=images, text_input_dict=text_inputs)\n",
    "\n",
    "            # Get true labels from image_ids\n",
    "            true_labels_dict = {\n",
    "                class_name: [label_names_to_idx[class_name][articles.loc[article_id_to_idx[image_id.item()], class_name]] \n",
    "                           for image_id in image_ids]\n",
    "                for class_name in class_names\n",
    "            }\n",
    "            true_labels_dict = {class_name: torch.tensor(true_labels).to(device)\n",
    "                                for class_name, true_labels in true_labels_dict.items()}\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criteria(logits_per_image_dict, true_labels_dict)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "\n",
    "            # Predictions and accuracy\n",
    "            total_samples += images.size(0)\n",
    "            for class_name in class_names:\n",
    "                _, preds = torch.max(logits_per_image_dict[class_name], dim=1)\n",
    "                total_correct[class_name] += (preds == true_labels_dict[class_name]).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / total_samples / len(class_names)\n",
    "    accuracy = {class_name: total_correct[class_name] / total_samples for class_name in class_names}\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = util.ImageDataset(val_paths, processor)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "test_dataset = util.ImageDataset(test_paths, processor)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [09:21<00:00, 26.76s/it]\n"
     ]
    }
   ],
   "source": [
    "avg_loss, accuracy = validate(mo_model, val_dataloader, criteria, device, text_input_dict, selected_class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3311598339182198\n",
      "{'product_type_name': 0.6103342591705775, 'graphical_appearance_name': 0.7141557322095585}\n"
     ]
    }
   ],
   "source": [
    "print(avg_loss)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
