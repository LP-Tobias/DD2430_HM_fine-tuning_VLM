{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T23:16:02.749950Z","iopub.status.busy":"2024-10-08T23:16:02.749600Z","iopub.status.idle":"2024-10-08T23:16:23.680407Z","shell.execute_reply":"2024-10-08T23:16:23.679521Z","shell.execute_reply.started":"2024-10-08T23:16:02.749912Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting peft\n","  Downloading peft-0.13.1-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\n","Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\n","Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.45.1)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\n","Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.34.2)\n","Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\n","Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.25.1)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.20.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n","Downloading peft-0.13.1-py3-none-any.whl (320 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: peft\n","Successfully installed peft-0.13.1\n"]}],"source":["!pip install peft\n","from PIL import Image\n","import requests\n","from transformers import CLIPProcessor, CLIPModel\n","import torch\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","from sklearn.model_selection import train_test_split\n","import os\n","from peft import LoraConfig, get_peft_model\n","import wandb"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T23:16:23.682277Z","iopub.status.busy":"2024-10-08T23:16:23.681971Z","iopub.status.idle":"2024-10-08T23:16:23.692167Z","shell.execute_reply":"2024-10-08T23:16:23.691237Z","shell.execute_reply.started":"2024-10-08T23:16:23.682244Z"},"trusted":true},"outputs":[],"source":["def get_image_paths_and_labels_from_df(df, data_dir):\n","    article_ids = df[\"article_id\"].values\n","    image_paths = []\n","    labels = []\n","    \n","    for article_id in article_ids:\n","        image_path = f\"{data_dir}/images/0{str(article_id)[:2]}/0{article_id}.jpg\"\n","        # Check if the image file exists\n","        if os.path.exists(image_path):\n","            image_paths.append(image_path)\n","            # Add corresponding label only if the image exists\n","            labels.append(df[df[\"article_id\"] == article_id])\n","\n","    return image_paths, labels\n","\n","class ImageDataset(torch.utils.data.Dataset):\n","    def __init__(self, image_paths, processor=None):\n","        self.image_paths = image_paths\n","        self.processor = processor\n","        self.image_ids = []\n","\n","        for image_path in self.image_paths:\n","            if not os.path.exists(image_path):\n","                raise FileNotFoundError(f\"Image {image_path} not found.\")\n","            else:\n","                image_id = int(image_path.split(\"/\")[-1].split(\".\")[0])\n","                self.image_ids.append(image_id)\n","            \n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        image = Image.open(self.image_paths[idx])\n","        if self.processor is not None:\n","            inputs = self.processor(images=image, return_tensors=\"pt\", padding=True)\n","            image = inputs[\"pixel_values\"][0]\n","        return image, self.image_ids[idx]"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T23:16:23.693415Z","iopub.status.busy":"2024-10-08T23:16:23.693151Z","iopub.status.idle":"2024-10-08T23:16:23.712807Z","shell.execute_reply":"2024-10-08T23:16:23.711985Z","shell.execute_reply.started":"2024-10-08T23:16:23.693385Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x78f2d2d98310>"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# set random seed 42\n","torch.manual_seed(42)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T23:16:23.715445Z","iopub.status.busy":"2024-10-08T23:16:23.715179Z","iopub.status.idle":"2024-10-08T23:16:41.065895Z","shell.execute_reply":"2024-10-08T23:16:41.065076Z","shell.execute_reply.started":"2024-10-08T23:16:23.715415Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"08d27b4545b6439c9490c728b4547538","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2f0a2456400342b9a5446b8b1feda0b3","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"514540980acd4f88adee5618b468c4f0","version_major":2,"version_minor":0},"text/plain":["preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e318f0d8df2849e48e5a6ff643f1f179","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e387513e56134e6aa1a5180d4b9cac8c","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8d64b74b56b745f4b0cf77517b9cc31a","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"452928e71f5b475193264fd657bf901f","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"94a8bb9168f74e5a891cf0f7b56df4cb","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", cache_dir=\"model\", local_files_only=False)\n","processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\", cache_dir=\"model\", local_files_only=False)\n","\n","model = model.to(device)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T23:16:41.067932Z","iopub.status.busy":"2024-10-08T23:16:41.067176Z","iopub.status.idle":"2024-10-08T23:16:42.006574Z","shell.execute_reply":"2024-10-08T23:16:42.005334Z","shell.execute_reply.started":"2024-10-08T23:16:41.067884Z"},"trusted":true},"outputs":[],"source":["text_path = '/kaggle/input/h-and-m-personalized-fashion-recommendations/articles.csv'\n","articles = pd.read_csv(text_path)\n","data_dir = '/kaggle/input/h-and-m-personalized-fashion-recommendations'"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T23:16:42.008508Z","iopub.status.busy":"2024-10-08T23:16:42.008093Z","iopub.status.idle":"2024-10-08T23:16:42.305622Z","shell.execute_reply":"2024-10-08T23:16:42.304645Z","shell.execute_reply.started":"2024-10-08T23:16:42.008443Z"},"trusted":true},"outputs":[],"source":["# map from article_id to df index\n","article_id_to_idx = {article_id: idx for idx, article_id in enumerate(articles[\"article_id\"])}\n","\n","# get all classes of the dataframe\n","class_names = articles.columns.tolist()\n","label_names = dict()\n","label_names_to_idx = dict()\n","for class_name in class_names:\n","    label_names[class_name] = articles[class_name].unique()\n","    label_names_to_idx[class_name] = {label_name: idx for idx, label_name in enumerate(label_names[class_name])}\n","\n","article_ids = label_names[\"article_id\"]\n","#selected_class_names = [\"product_type_name\", \"graphical_appearance_name\"]\n","selected_class_names = [\"product_group_name\", \"product_type_name\", \"graphical_appearance_name\", \"colour_group_name\", \"perceived_colour_value_name\", \"perceived_colour_master_name\", \"department_name\", \"index_name\", \"index_group_name\", \"section_name\", \"garment_group_name\"]"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T23:16:42.307287Z","iopub.status.busy":"2024-10-08T23:16:42.306975Z","iopub.status.idle":"2024-10-08T23:17:04.605101Z","shell.execute_reply":"2024-10-08T23:17:04.604169Z","shell.execute_reply.started":"2024-10-08T23:16:42.307254Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["len(train_df)=84445 len(val_df)=10534 len(test_df)=10563\n"]}],"source":["# grouped by product_code\n","grouped = articles.groupby(\"product_code\")\n","groups = [group for _, group in grouped]\n","\n","# split 0.8/0.1/0.1\n","train_groups, test_groups = train_test_split(groups, test_size=0.2, random_state=42) \n","val_groups, test_groups = train_test_split(test_groups, test_size=0.5, random_state=42) \n","\n","train_df = pd.concat(train_groups)\n","val_df = pd.concat(val_groups)\n","test_df = pd.concat(test_groups)\n","\n","print(f\"{len(train_df)=} {len(val_df)=} {len(test_df)=}\")"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T23:17:04.606390Z","iopub.status.busy":"2024-10-08T23:17:04.606114Z","iopub.status.idle":"2024-10-08T23:22:28.822877Z","shell.execute_reply":"2024-10-08T23:22:28.822067Z","shell.execute_reply.started":"2024-10-08T23:17:04.606360Z"},"trusted":true},"outputs":[],"source":["train_paths, train_labels = get_image_paths_and_labels_from_df(train_df, data_dir)\n","val_paths, val_labels = get_image_paths_and_labels_from_df(val_df, data_dir)\n","test_paths, test_labels = get_image_paths_and_labels_from_df(test_df, data_dir)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T23:22:28.824716Z","iopub.status.busy":"2024-10-08T23:22:28.824358Z","iopub.status.idle":"2024-10-08T23:22:28.831472Z","shell.execute_reply":"2024-10-08T23:22:28.830508Z","shell.execute_reply.started":"2024-10-08T23:22:28.824678Z"},"trusted":true},"outputs":[],"source":["class MultiOutputLayer(torch.nn.Module):\n","    def __init__(self, input_size, inter_size, output_size):\n","        super(MultiOutputLayer, self).__init__()\n","        self.fc1 = torch.nn.Linear(input_size, inter_size)\n","        self.fc2 = torch.nn.Linear(inter_size, output_size)\n","        self.dropout = torch.nn.Dropout(0.5)\n","        self.act = torch.nn.SiLU()\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.act(x)\n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","        return x"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T23:22:28.836711Z","iopub.status.busy":"2024-10-08T23:22:28.836295Z","iopub.status.idle":"2024-10-08T23:22:28.848992Z","shell.execute_reply":"2024-10-08T23:22:28.848117Z","shell.execute_reply.started":"2024-10-08T23:22:28.836676Z"},"trusted":true},"outputs":[],"source":["class MultiOutputClipModel(torch.nn.Module):\n","    def __init__(self, clip_model, class_names, vision_hidden_size, inter_size, output_size):\n","        super(MultiOutputClipModel, self).__init__()\n","        self.clip_model = clip_model\n","        self.class_names = class_names\n","        self.output_layers = torch.nn.ModuleDict({\n","            class_name: MultiOutputLayer(vision_hidden_size, inter_size, output_size)\n","            for class_name in self.class_names\n","        })\n","    \n","    def forward(\n","        self,\n","        text_input_dict,\n","        pixel_values,\n","        # position_ids = None,\n","        output_attentions = None,\n","        output_hidden_states = None,\n","        return_dict = None,\n","    ):\n","\n","        output_attentions = output_attentions if output_attentions is not None else self.clip_model.config.output_attentions\n","        output_hidden_states = (\n","            output_hidden_states if output_hidden_states is not None else self.clip_model.config.output_hidden_states\n","        )\n","        return_dict = return_dict if return_dict is not None else self.clip_model.config.use_return_dict\n","\n","        vision_outputs = self.clip_model.vision_model(\n","            pixel_values=pixel_values,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        vision_embeds = vision_outputs[1]\n","        vision_embeds_dict = {\n","            class_name: output_layer(vision_embeds) \n","                for class_name, output_layer in self.output_layers.items()\n","        }\n","\n","        text_outputs_dict = {\n","            class_name: self.clip_model.text_model(\n","                input_ids=text_input_dict[class_name][\"input_ids\"],\n","                attention_mask=text_input_dict[class_name][\"attention_mask\"],\n","                # position_ids=position_ids,\n","                output_attentions=output_attentions,\n","                output_hidden_states=output_hidden_states,\n","                return_dict=return_dict,\n","            ) for class_name in self.class_names\n","        }\n","\n","        text_embeds_dict = {\n","            class_name: self.clip_model.text_projection(text_outputs[1])\n","                for class_name, text_outputs in text_outputs_dict.items()\n","        }\n","\n","        logits_per_image_dict = {\n","            class_name: vision_embeds_dict[class_name] @ text_embeds_dict[class_name].T\n","                for class_name in self.output_layers.keys()\n","        }\n","\n","        return logits_per_image_dict"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T23:22:28.850613Z","iopub.status.busy":"2024-10-08T23:22:28.850234Z","iopub.status.idle":"2024-10-08T23:22:28.863065Z","shell.execute_reply":"2024-10-08T23:22:28.862214Z","shell.execute_reply.started":"2024-10-08T23:22:28.850571Z"},"trusted":true},"outputs":[],"source":["# custom criterion: cross entropy loss across all classes\n","class MultiOutputClipCriterion(torch.nn.Module):\n","    def __init__(self, class_names):\n","        super(MultiOutputClipCriterion, self).__init__()\n","        self.class_names = class_names\n","        self.criterion = torch.nn.CrossEntropyLoss()\n","\n","    def forward(self, logits_dict, labels_dict):\n","        loss = 0\n","        for class_name in self.class_names:\n","            logits = logits_dict[class_name]\n","            labels = labels_dict[class_name]\n","            loss += self.criterion(logits, labels)\n","        return loss"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T23:24:16.468053Z","iopub.status.busy":"2024-10-08T23:24:16.467616Z","iopub.status.idle":"2024-10-08T23:24:57.730306Z","shell.execute_reply":"2024-10-08T23:24:57.729503Z","shell.execute_reply.started":"2024-10-08T23:24:16.468014Z"},"trusted":true},"outputs":[],"source":["train_dataset = ImageDataset(train_paths, processor)\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n","\n","val_dataset = ImageDataset(val_paths, processor)\n","val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=256, shuffle=False)\n","\n","test_dataset = ImageDataset(test_paths, processor)\n","test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=False)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T23:25:04.205756Z","iopub.status.busy":"2024-10-08T23:25:04.205368Z","iopub.status.idle":"2024-10-08T23:25:04.329544Z","shell.execute_reply":"2024-10-08T23:25:04.328798Z","shell.execute_reply.started":"2024-10-08T23:25:04.205721Z"},"trusted":true},"outputs":[],"source":["# freeze all parameters in model\n","\n","# for param in model.parameters():\n","#     param.requires_grad = False\n","\n","# Define LoRA configuration\n","lora_config = LoraConfig(\n","    r=8,                  # Low-rank dimension (adjustable)\n","    lora_alpha=32,          # Scaling factor (adjustable)\n","    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\"],  # Specify which layers to apply LoRA to\n","    lora_dropout=0.05,       # Dropout rate (optional)\n","    bias=\"none\",            # Whether to include biases (\"none\", \"all\", \"lora_only\")\n","    task_type=\"classification\"  # Task type (\"classification\" or \"regression\")\n",")\n","\n","# Apply LoRA to the CLIP model\n","model = get_peft_model(model, lora_config)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T23:25:08.549573Z","iopub.status.busy":"2024-10-08T23:25:08.548522Z","iopub.status.idle":"2024-10-08T23:25:08.703561Z","shell.execute_reply":"2024-10-08T23:25:08.702507Z","shell.execute_reply.started":"2024-10-08T23:25:08.549515Z"},"trusted":true},"outputs":[{"data":{"text/plain":["MultiOutputClipModel(\n","  (clip_model): PeftModel(\n","    (base_model): LoraModel(\n","      (model): CLIPModel(\n","        (text_model): CLIPTextTransformer(\n","          (embeddings): CLIPTextEmbeddings(\n","            (token_embedding): Embedding(49408, 512)\n","            (position_embedding): Embedding(77, 512)\n","          )\n","          (encoder): CLIPEncoder(\n","            (layers): ModuleList(\n","              (0-11): 12 x CLIPEncoderLayer(\n","                (self_attn): CLIPSdpaAttention(\n","                  (k_proj): lora.Linear(\n","                    (base_layer): Linear(in_features=512, out_features=512, bias=True)\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.05, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=512, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=512, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                    (lora_magnitude_vector): ModuleDict()\n","                  )\n","                  (v_proj): lora.Linear(\n","                    (base_layer): Linear(in_features=512, out_features=512, bias=True)\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.05, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=512, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=512, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                    (lora_magnitude_vector): ModuleDict()\n","                  )\n","                  (q_proj): lora.Linear(\n","                    (base_layer): Linear(in_features=512, out_features=512, bias=True)\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.05, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=512, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=512, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                    (lora_magnitude_vector): ModuleDict()\n","                  )\n","                  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","                )\n","                (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","                (mlp): CLIPMLP(\n","                  (activation_fn): QuickGELUActivation()\n","                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","                )\n","                (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","              )\n","            )\n","          )\n","          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (vision_model): CLIPVisionTransformer(\n","          (embeddings): CLIPVisionEmbeddings(\n","            (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n","            (position_embedding): Embedding(50, 768)\n","          )\n","          (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (encoder): CLIPEncoder(\n","            (layers): ModuleList(\n","              (0-11): 12 x CLIPEncoderLayer(\n","                (self_attn): CLIPSdpaAttention(\n","                  (k_proj): lora.Linear(\n","                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.05, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=768, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=768, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                    (lora_magnitude_vector): ModuleDict()\n","                  )\n","                  (v_proj): lora.Linear(\n","                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.05, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=768, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=768, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                    (lora_magnitude_vector): ModuleDict()\n","                  )\n","                  (q_proj): lora.Linear(\n","                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.05, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=768, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=768, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                    (lora_magnitude_vector): ModuleDict()\n","                  )\n","                  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","                )\n","                (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (mlp): CLIPMLP(\n","                  (activation_fn): QuickGELUActivation()\n","                  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","                  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","                )\n","                (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              )\n","            )\n","          )\n","          (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n","        (text_projection): Linear(in_features=512, out_features=512, bias=False)\n","      )\n","    )\n","  )\n","  (output_layers): ModuleDict(\n","    (product_group_name): MultiOutputLayer(\n","      (fc1): Linear(in_features=768, out_features=128, bias=True)\n","      (fc2): Linear(in_features=128, out_features=512, bias=True)\n","      (dropout): Dropout(p=0.5, inplace=False)\n","      (act): SiLU()\n","    )\n","    (product_type_name): MultiOutputLayer(\n","      (fc1): Linear(in_features=768, out_features=128, bias=True)\n","      (fc2): Linear(in_features=128, out_features=512, bias=True)\n","      (dropout): Dropout(p=0.5, inplace=False)\n","      (act): SiLU()\n","    )\n","    (graphical_appearance_name): MultiOutputLayer(\n","      (fc1): Linear(in_features=768, out_features=128, bias=True)\n","      (fc2): Linear(in_features=128, out_features=512, bias=True)\n","      (dropout): Dropout(p=0.5, inplace=False)\n","      (act): SiLU()\n","    )\n","    (colour_group_name): MultiOutputLayer(\n","      (fc1): Linear(in_features=768, out_features=128, bias=True)\n","      (fc2): Linear(in_features=128, out_features=512, bias=True)\n","      (dropout): Dropout(p=0.5, inplace=False)\n","      (act): SiLU()\n","    )\n","    (perceived_colour_value_name): MultiOutputLayer(\n","      (fc1): Linear(in_features=768, out_features=128, bias=True)\n","      (fc2): Linear(in_features=128, out_features=512, bias=True)\n","      (dropout): Dropout(p=0.5, inplace=False)\n","      (act): SiLU()\n","    )\n","    (perceived_colour_master_name): MultiOutputLayer(\n","      (fc1): Linear(in_features=768, out_features=128, bias=True)\n","      (fc2): Linear(in_features=128, out_features=512, bias=True)\n","      (dropout): Dropout(p=0.5, inplace=False)\n","      (act): SiLU()\n","    )\n","    (department_name): MultiOutputLayer(\n","      (fc1): Linear(in_features=768, out_features=128, bias=True)\n","      (fc2): Linear(in_features=128, out_features=512, bias=True)\n","      (dropout): Dropout(p=0.5, inplace=False)\n","      (act): SiLU()\n","    )\n","    (index_name): MultiOutputLayer(\n","      (fc1): Linear(in_features=768, out_features=128, bias=True)\n","      (fc2): Linear(in_features=128, out_features=512, bias=True)\n","      (dropout): Dropout(p=0.5, inplace=False)\n","      (act): SiLU()\n","    )\n","    (index_group_name): MultiOutputLayer(\n","      (fc1): Linear(in_features=768, out_features=128, bias=True)\n","      (fc2): Linear(in_features=128, out_features=512, bias=True)\n","      (dropout): Dropout(p=0.5, inplace=False)\n","      (act): SiLU()\n","    )\n","    (section_name): MultiOutputLayer(\n","      (fc1): Linear(in_features=768, out_features=128, bias=True)\n","      (fc2): Linear(in_features=128, out_features=512, bias=True)\n","      (dropout): Dropout(p=0.5, inplace=False)\n","      (act): SiLU()\n","    )\n","    (garment_group_name): MultiOutputLayer(\n","      (fc1): Linear(in_features=768, out_features=128, bias=True)\n","      (fc2): Linear(in_features=128, out_features=512, bias=True)\n","      (dropout): Dropout(p=0.5, inplace=False)\n","      (act): SiLU()\n","    )\n","  )\n",")"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["mo_model = MultiOutputClipModel(model, selected_class_names, 768, 128, 512).to(device)\n","mo_model.train()"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T23:25:25.544345Z","iopub.status.busy":"2024-10-08T23:25:25.543931Z","iopub.status.idle":"2024-10-08T23:25:25.557935Z","shell.execute_reply":"2024-10-08T23:25:25.556995Z","shell.execute_reply.started":"2024-10-08T23:25:25.544308Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["clip_model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight\n","clip_model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight\n","clip_model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight\n","output_layers.product_group_name.fc1.weight\n","output_layers.product_group_name.fc1.bias\n","output_layers.product_group_name.fc2.weight\n","output_layers.product_group_name.fc2.bias\n","output_layers.product_type_name.fc1.weight\n","output_layers.product_type_name.fc1.bias\n","output_layers.product_type_name.fc2.weight\n","output_layers.product_type_name.fc2.bias\n","output_layers.graphical_appearance_name.fc1.weight\n","output_layers.graphical_appearance_name.fc1.bias\n","output_layers.graphical_appearance_name.fc2.weight\n","output_layers.graphical_appearance_name.fc2.bias\n","output_layers.colour_group_name.fc1.weight\n","output_layers.colour_group_name.fc1.bias\n","output_layers.colour_group_name.fc2.weight\n","output_layers.colour_group_name.fc2.bias\n","output_layers.perceived_colour_value_name.fc1.weight\n","output_layers.perceived_colour_value_name.fc1.bias\n","output_layers.perceived_colour_value_name.fc2.weight\n","output_layers.perceived_colour_value_name.fc2.bias\n","output_layers.perceived_colour_master_name.fc1.weight\n","output_layers.perceived_colour_master_name.fc1.bias\n","output_layers.perceived_colour_master_name.fc2.weight\n","output_layers.perceived_colour_master_name.fc2.bias\n","output_layers.department_name.fc1.weight\n","output_layers.department_name.fc1.bias\n","output_layers.department_name.fc2.weight\n","output_layers.department_name.fc2.bias\n","output_layers.index_name.fc1.weight\n","output_layers.index_name.fc1.bias\n","output_layers.index_name.fc2.weight\n","output_layers.index_name.fc2.bias\n","output_layers.index_group_name.fc1.weight\n","output_layers.index_group_name.fc1.bias\n","output_layers.index_group_name.fc2.weight\n","output_layers.index_group_name.fc2.bias\n","output_layers.section_name.fc1.weight\n","output_layers.section_name.fc1.bias\n","output_layers.section_name.fc2.weight\n","output_layers.section_name.fc2.bias\n","output_layers.garment_group_name.fc1.weight\n","output_layers.garment_group_name.fc1.bias\n","output_layers.garment_group_name.fc2.weight\n","output_layers.garment_group_name.fc2.bias\n"]}],"source":["# show all trainable parameters in mo_model\n","for name, param in mo_model.named_parameters():\n","    if param.requires_grad:\n","        print(name)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T23:25:49.096859Z","iopub.status.busy":"2024-10-08T23:25:49.095990Z","iopub.status.idle":"2024-10-08T23:25:49.131896Z","shell.execute_reply":"2024-10-08T23:25:49.131148Z","shell.execute_reply.started":"2024-10-08T23:25:49.096818Z"},"trusted":true},"outputs":[],"source":["# generate text input\n","text_input_dict = {\n","    class_name: processor(text=[f\"A photo of a {label}\" for label in label_names[class_name]], \n","                          return_tensors=\"pt\", padding=True).to(device)\n","    for class_name in selected_class_names\n","}"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T23:25:54.154589Z","iopub.status.busy":"2024-10-08T23:25:54.154218Z","iopub.status.idle":"2024-10-08T23:25:59.430180Z","shell.execute_reply":"2024-10-08T23:25:59.429333Z","shell.execute_reply.started":"2024-10-08T23:25:54.154556Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mliulianhang\u001b[0m (\u001b[33mliulianhang-kth-royal-institute-of-technology\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a945fb5b28f845fdaeba33ce4152e97b","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112844911109379, max=1.0…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.18.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20241008_232556-xodjqa9f</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/liulianhang-kth-royal-institute-of-technology/clip-lora_2/runs/xodjqa9f' target=\"_blank\">multiclass_layer</a></strong> to <a href='https://wandb.ai/liulianhang-kth-royal-institute-of-technology/clip-lora_2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/liulianhang-kth-royal-institute-of-technology/clip-lora_2' target=\"_blank\">https://wandb.ai/liulianhang-kth-royal-institute-of-technology/clip-lora_2</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/liulianhang-kth-royal-institute-of-technology/clip-lora_2/runs/xodjqa9f' target=\"_blank\">https://wandb.ai/liulianhang-kth-royal-institute-of-technology/clip-lora_2/runs/xodjqa9f</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["\n","from kaggle_secrets import UserSecretsClient\n","user_secrets = UserSecretsClient()\n","secret_value_0 = user_secrets.get_secret(\"wandb_key\")\n","wandb.login(key=secret_value_0)\n","wandb.init(project=\"clip-lora_2\", name='multiclass_layer2')\n","criteria = MultiOutputClipCriterion(class_names=selected_class_names)\n","optimizer = torch.optim.AdamW(mo_model.parameters(), lr=1e-4)\n","num_epochs = 10  # 根据需要调整\n","step = 0\n","def validate(model, dataloader, criteria, device, text_inputs, class_names):\n","    model.eval()\n","    total_loss = 0.0\n","    total_correct = {class_name: 0 for class_name in class_names}\n","    total_samples = 0\n","\n","    with torch.no_grad():\n","        for images, image_ids in tqdm(dataloader):\n","            images = images.to(device)\n","            logits_per_image_dict = model(pixel_values=images, text_input_dict=text_inputs)\n","\n","            # 获取真实标签\n","            true_labels_dict = {\n","                class_name: [label_names_to_idx[class_name][articles.loc[article_id_to_idx[image_id.item()], class_name]] \n","                             for image_id in image_ids]\n","                for class_name in class_names\n","            }\n","            true_labels_dict = {class_name: torch.tensor(true_labels).to(device)\n","                                for class_name, true_labels in true_labels_dict.items()}\n","            \n","            # 计算损失\n","            loss = criteria(logits_per_image_dict, true_labels_dict)\n","            total_loss += loss.item() * images.size(0)\n","\n","            # 计算准确率\n","            total_samples += images.size(0)\n","            for class_name in class_names:\n","                _, preds = torch.max(logits_per_image_dict[class_name], dim=1)\n","                total_correct[class_name] += (preds == true_labels_dict[class_name]).sum().item()\n","\n","    avg_loss = total_loss / total_samples / len(class_names)\n","    accuracy = {class_name: total_correct[class_name] / total_samples for class_name in class_names}\n","    return avg_loss, accuracy\n"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T23:26:29.656194Z","iopub.status.busy":"2024-10-08T23:26:29.655494Z","iopub.status.idle":"2024-10-08T23:26:49.549818Z","shell.execute_reply":"2024-10-08T23:26:49.548503Z","shell.execute_reply.started":"2024-10-08T23:26:29.656155Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["  2%|▏         | 1/41 [00:19<12:43, 19.08s/it]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[21], line 55\u001b[0m\n\u001b[1;32m      5\u001b[0m     total_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#     for images, image_ids in tqdm(train_dataloader):\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#         images = images.to(device)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#         logits_per_image_dict = mo_model(pixel_values=images, text_input_dict=text_input_dict)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m \n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# 在每个 epoch 结束后进行验证\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m     val_loss, val_accuracy_dict \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmo_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriteria\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_input_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_class_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     val_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(val_accuracy_dict\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(val_accuracy_dict)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Validation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[18], line 17\u001b[0m, in \u001b[0;36mvalidate\u001b[0;34m(model, dataloader, criteria, device, text_inputs, class_names)\u001b[0m\n\u001b[1;32m     14\u001b[0m total_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, image_ids \u001b[38;5;129;01min\u001b[39;00m tqdm(dataloader):\n\u001b[1;32m     18\u001b[0m         images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     19\u001b[0m         logits_per_image_dict \u001b[38;5;241m=\u001b[39m model(pixel_values\u001b[38;5;241m=\u001b[39mimages, text_input_dict\u001b[38;5;241m=\u001b[39mtext_inputs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[0;32mIn[2], line 34\u001b[0m, in \u001b[0;36mImageDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 34\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor(images\u001b[38;5;241m=\u001b[39mimage, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3428\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3426\u001b[0m filename: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mbytes\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_path(fp):\n\u001b[0;32m-> 3428\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrealpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m   3431\u001b[0m     fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/posixpath.py:396\u001b[0m, in \u001b[0;36mrealpath\u001b[0;34m(filename, strict)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the canonical path of the specified filename, eliminating any\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;124;03msymbolic links encountered in the path.\"\"\"\u001b[39;00m\n\u001b[1;32m    395\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(filename)\n\u001b[0;32m--> 396\u001b[0m     path, ok \u001b[38;5;241m=\u001b[39m \u001b[43m_joinrealpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m abspath(path)\n","File \u001b[0;32m/opt/conda/lib/python3.10/posixpath.py:431\u001b[0m, in \u001b[0;36m_joinrealpath\u001b[0;34m(path, rest, strict, seen)\u001b[0m\n\u001b[1;32m    429\u001b[0m newpath \u001b[38;5;241m=\u001b[39m join(path, name)\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 431\u001b[0m     st \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnewpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m strict:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["\n","for epoch in range(num_epochs):\n","    mo_model.train()\n","    total_loss = 0.0\n","    total_correct = 0\n","    total_samples = 0\n","\n","    for images, image_ids in tqdm(train_dataloader):\n","        images = images.to(device)\n","        logits_per_image_dict = mo_model(pixel_values=images, text_input_dict=text_input_dict)\n","\n","        # 获取真实标签\n","        true_labels_dict = {\n","            class_name: [label_names_to_idx[class_name][articles.loc[article_id_to_idx[image_id.item()], class_name]] \n","                         for image_id in image_ids]\n","            for class_name in selected_class_names\n","        }\n","        true_labels_dict = {class_name: torch.tensor(true_labels).to(device) \n","                            for class_name, true_labels in true_labels_dict.items()}\n","\n","        # 计算损失\n","        loss = criteria(logits_per_image_dict, true_labels_dict)\n","        total_loss += loss.item() * images.size(0)\n","\n","        # 计算准确率\n","        correct = 0\n","        total_samples += images.size(0)\n","        for class_name in selected_class_names:\n","            _, preds = torch.max(logits_per_image_dict[class_name], dim=1)\n","            correct += (preds == true_labels_dict[class_name]).sum().item()\n","        total_correct += correct\n","\n","        # 反向传播和优化\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # 记录训练损失和准确率到 wandb\n","        # 在训练循环中，记录每个类别的准确率\n","        log_dict = {\n","            \"train_loss\": loss.item(),\n","            \"train_accuracy\": correct / images.size(0) / len(selected_class_names)\n","        }\n","#         for class_name in selected_class_names:\n","#             accuracy = total_correct_per_class[class_name] / total_samples\n","#             log_dict[f\"train_accuracy_{class_name}\"] = accuracy\n","\n","        wandb.log(log_dict, step=step)\n","        step += 1\n","\n","    avg_loss = total_loss / total_samples / len(selected_class_names)\n","    accuracy = total_correct / total_samples / len(selected_class_names)\n","    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_loss:.4f}, Train Accuracy: {accuracy:.4f}\")\n","\n","    # 在每个 epoch 结束后进行验证\n","    val_loss, val_accuracy_dict = validate(mo_model, val_dataloader, criteria, device, text_input_dict, selected_class_names)\n","    val_accuracy = sum(val_accuracy_dict.values()) / len(val_accuracy_dict)\n","    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n","\n","        # 记录验证损失和每个类别的准确率到 wandb\n","    log_dict = {\n","        \"val_loss\": val_loss,\n","        \"val_accuracy\": val_accuracy\n","    }\n","    for class_name, accuracy in val_accuracy_dict.items():\n","        log_dict[f\"val_accuracy_{class_name}\"] = accuracy\n","\n","    wandb.log(log_dict, step=step)\n","\n","\n","wandb.finish()\n"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T12:42:43.091463Z","iopub.status.busy":"2024-10-09T12:42:43.091134Z","iopub.status.idle":"2024-10-09T12:42:43.433004Z","shell.execute_reply":"2024-10-09T12:42:43.431790Z","shell.execute_reply.started":"2024-10-09T12:42:43.091427Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'torch' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 保存模型\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39msave(mo_model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel/final_output_clip_model2.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 在训练完成后进行测试\u001b[39;00m\n\u001b[1;32m      5\u001b[0m test_loss, test_accuracy_dict \u001b[38;5;241m=\u001b[39m validate(\n\u001b[1;32m      6\u001b[0m     mo_model, test_dataloader, criteria, device, text_input_dict, selected_class_names\n\u001b[1;32m      7\u001b[0m )\n","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"]}],"source":["# 保存模型\n","torch.save(mo_model.state_dict(), \"model/final_output_clip_model2.pth\")\n","\n","# 在训练完成后进行测试\n","test_loss, test_accuracy_dict = validate(\n","    mo_model, test_dataloader, criteria, device, text_input_dict, selected_class_names\n",")\n","\n","print(f\"Test Loss: {test_loss:.4f}\")\n","\n","# 显示每个类别的准确率\n","print(\"Test Accuracy per Class:\")\n","for class_name, accuracy in test_accuracy_dict.items():\n","    print(f\"{class_name}: {accuracy:.4f}\")\n","\n","# 计算并显示平均准确率\n","test_accuracy = sum(test_accuracy_dict.values()) / len(test_accuracy_dict)\n","print(f\"Average Test Accuracy: {test_accuracy:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T23:23:02.669090Z","iopub.status.idle":"2024-10-08T23:23:02.669592Z","shell.execute_reply":"2024-10-08T23:23:02.669349Z","shell.execute_reply.started":"2024-10-08T23:23:02.669323Z"},"trusted":true},"outputs":[],"source":["# from kaggle_secrets import UserSecretsClient\n","# num_epochs = 10  # Adjust as needed\n","# criteria = MultiOutputClipCriterion(class_names=selected_class_names)\n","# optimizer = torch.optim.AdamW(mo_model.parameters(), lr=1e-4)\n","# step = 0\n","# user_secrets = UserSecretsClient()\n","# secret = user_secrets.get_secret(\"wandb_key\")\n","# wandb.login(key=secret)\n","# wandb.init(project=\"clip-lora_2\", name='multiclass_layer')\n","# for epoch in range(num_epochs):\n","#     mo_model.train()\n","#     total_loss = 0.0\n","#     total_correct = 0\n","#     total_samples = 0\n","\n","#     for images, image_ids in tqdm(train_dataloader):\n","#         images = images.to(device)\n","#         logits_per_image_dict = mo_model(pixel_values=images, text_input_dict=text_input_dict)\n","\n","#         # Get true labels from image_ids\n","#         true_labels_dict = {\n","#             class_name: [label_names_to_idx[class_name][articles.loc[article_id_to_idx[image_id.item()], class_name]] \n","#                        for image_id in image_ids]\n","#             for class_name in selected_class_names\n","#         }\n","#         true_labels_dict = {class_name: torch.tensor(true_labels).to(device) \n","#                             for class_name, true_labels in true_labels_dict.items()}\n","        \n","#         # Compute loss\n","#         loss = criteria(logits_per_image_dict, true_labels_dict)\n","#         total_loss += loss.item() * images.size(0)\n","\n","#         # Predictions and accuracy\n","#         correct = 0\n","#         total_samples += images.size(0)\n","#         for class_name in selected_class_names:\n","#             _, preds = torch.max(logits_per_image_dict[class_name], dim=1)\n","#             correct += (preds == true_labels_dict[class_name]).sum().item()\n","#         total_correct += correct\n","\n","#         # Backward pass\n","#         optimizer.zero_grad()\n","#         loss.backward()\n","#         optimizer.step()\n","\n","#         # log the loss and accuracy to wandb\n","#         wandb.log({\"loss\": loss.item(), \"accuracy\": correct / images.size(0) / len(selected_class_names)},\n","#                   step=step)\n","#         step += 1\n","\n","#     avg_loss = total_loss / total_samples / len(selected_class_names)\n","#     accuracy = total_correct / total_samples / len(selected_class_names)\n","#     print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n","\n","#     # Validate after each epoch\n","#     # val_loss, val_accuracy = validate(model, val_dataloader, criteria, device, text_inputs, class_name)\n","#     # print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n","\n","# wandb.finish()\n","\n","# # Save the model\n","# torch.save(mo_model.state_dict(), \"model/2_output_clip_model-3.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T23:23:02.670835Z","iopub.status.idle":"2024-10-08T23:23:02.671363Z","shell.execute_reply":"2024-10-08T23:23:02.671129Z","shell.execute_reply.started":"2024-10-08T23:23:02.671102Z"},"trusted":true},"outputs":[],"source":["# def validate(model, dataloader, criteria, device, text_inputs, class_names):\n","#     model.eval()\n","#     total_loss = 0.0\n","#     total_correct = {class_name: 0 for class_name in class_names}\n","#     total_samples = 0\n","\n","#     with torch.no_grad():\n","#         for images, image_ids in tqdm(dataloader):\n","#             images = images.to(device)\n","#             logits_per_image_dict = model(pixel_values=images, text_input_dict=text_inputs)\n","\n","#             # Get true labels from image_ids\n","#             true_labels_dict = {\n","#                 class_name: [label_names_to_idx[class_name][articles.loc[article_id_to_idx[image_id.item()], class_name]] \n","#                            for image_id in image_ids]\n","#                 for class_name in class_names\n","#             }\n","#             true_labels_dict = {class_name: torch.tensor(true_labels).to(device)\n","#                                 for class_name, true_labels in true_labels_dict.items()}\n","            \n","#             # Compute loss\n","#             loss = criteria(logits_per_image_dict, true_labels_dict)\n","#             total_loss += loss.item() * images.size(0)\n","\n","#             # Predictions and accuracy\n","#             total_samples += images.size(0)\n","#             for class_name in class_names:\n","#                 _, preds = torch.max(logits_per_image_dict[class_name], dim=1)\n","#                 total_correct[class_name] += (preds == true_labels_dict[class_name]).sum().item()\n","\n","#     avg_loss = total_loss / total_samples / len(class_names)\n","#     accuracy = {class_name: total_correct[class_name] / total_samples for class_name in class_names}\n","#     return avg_loss, accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T23:23:02.672734Z","iopub.status.idle":"2024-10-08T23:23:02.673202Z","shell.execute_reply":"2024-10-08T23:23:02.672982Z","shell.execute_reply.started":"2024-10-08T23:23:02.672956Z"},"trusted":true},"outputs":[],"source":["# val_dataset = util.ImageDataset(val_paths, processor)\n","# val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=256, shuffle=False)\n","# test_dataset = util.ImageDataset(test_paths, processor)\n","# test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T23:23:02.674809Z","iopub.status.idle":"2024-10-08T23:23:02.675170Z","shell.execute_reply":"2024-10-08T23:23:02.675008Z","shell.execute_reply.started":"2024-10-08T23:23:02.674984Z"},"trusted":true},"outputs":[],"source":["# avg_loss, accuracy = validate(mo_model, val_dataloader, criteria, device, text_input_dict, selected_class_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T23:23:02.676287Z","iopub.status.idle":"2024-10-08T23:23:02.676642Z","shell.execute_reply":"2024-10-08T23:23:02.676478Z","shell.execute_reply.started":"2024-10-08T23:23:02.676442Z"},"trusted":true},"outputs":[],"source":["# print(avg_loss)\n","# print(accuracy)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-08T23:23:02.677996Z","iopub.status.idle":"2024-10-08T23:23:02.678368Z","shell.execute_reply":"2024-10-08T23:23:02.678207Z","shell.execute_reply.started":"2024-10-08T23:23:02.678188Z"},"trusted":true},"outputs":[],"source":["# torch.save(mo_model, \"model/2_output_clip_model.pt\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":3103714,"sourceId":31254,"sourceType":"competition"},{"sourceId":199081181,"sourceType":"kernelVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"hf","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"}},"nbformat":4,"nbformat_minor":4}
