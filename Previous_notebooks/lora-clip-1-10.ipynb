{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-10-01T23:42:02.346158Z","iopub.status.busy":"2024-10-01T23:42:02.345465Z","iopub.status.idle":"2024-10-01T23:42:14.622597Z","shell.execute_reply":"2024-10-01T23:42:14.621450Z","shell.execute_reply.started":"2024-10-01T23:42:02.346115Z"},"trusted":true},"outputs":[],"source":["!pip install peft"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-10-02T00:00:01.036048Z","iopub.status.busy":"2024-10-02T00:00:01.035343Z","iopub.status.idle":"2024-10-02T00:00:01.042936Z","shell.execute_reply":"2024-10-02T00:00:01.041949Z","shell.execute_reply.started":"2024-10-02T00:00:01.036003Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import random\n","from collections import defaultdict\n","from collections import Counter\n","from matplotlib import pyplot as plt\n","from tqdm.notebook import tqdm\n","import os\n","from pathlib import Path\n","from PIL import Image\n","import requests\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from datasets import load_dataset, Dataset\n","import itertools\n","from sklearn.model_selection import StratifiedShuffleSplit,train_test_split\n","from sklearn.metrics import confusion_matrix\n","from peft import LoraConfig, get_peft_model\n","from transformers import (\n","    DataCollator,\n","    CLIPProcessor, \n","    CLIPModel, \n","    TrainingArguments, \n","    Trainer\n",")"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-10-01T23:42:14.636182Z","iopub.status.busy":"2024-10-01T23:42:14.635798Z","iopub.status.idle":"2024-10-01T23:42:14.657243Z","shell.execute_reply":"2024-10-01T23:42:14.656363Z","shell.execute_reply.started":"2024-10-01T23:42:14.636139Z"},"trusted":true},"outputs":[],"source":["def hf_clip_predict(model, processor, text_labels, images):\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    model.to(device)\n","    text = [f\"A photo of a {label}\" for label in text_labels]\n","    inputs = processor(text=text, images=images, return_tensors=\"pt\", padding=True).to(device)\n","    \n","    model.eval()\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","    \n","    logits_per_image = outputs.logits_per_image\n","    probs = logits_per_image.softmax(dim=1)\n","    return probs\n","\n","def get_image_paths_and_labels_from_df(df, data_dir):\n","    article_ids = df[\"article_id\"].values\n","    image_paths = []\n","    labels = []\n","    \n","    for article_id in article_ids:\n","        image_path = f\"{data_dir}/images/0{str(article_id)[:2]}/0{article_id}.jpg\"\n","        # Check if the image file exists\n","        if os.path.exists(image_path):\n","            image_paths.append(image_path)\n","            # Add corresponding label only if the image exists\n","            labels.append(df[df[\"article_id\"] == article_id])\n","        else:\n","            print(f\"Image not found for article_id: {article_id}\")\n","    \n","    return image_paths, labels\n","\n","def get_image_paths_and_labels_ordered(df, data_dir):\n","    article_ids = df[\"article_id\"].values\n","    image_paths = []\n","    labels = []\n","    for article_id in article_ids:\n","        image_path = f\"{data_dir}/images/0{str(article_id)[:2]}/0{article_id}.jpg\"\n","        if os.path.exists(image_path):\n","            image_paths.append(image_path)\n","            labels.append(df[df[\"article_id\"] == article_id])\n","    \n","    return image_paths, labels\n","\n","def get_image_paths_and_labels(df, data_dir):\n","    image_paths = []\n","    labels = []\n","    for root, dirs, files in os.walk(data_dir):\n","        for file in files:\n","            if file.endswith(\".jpg\"):\n","                image_path = os.path.join(root, file)\n","                image_paths.append(image_path)\n","                article_id = int(file.split(\".\")[0])\n","                labels.append(df[df[\"article_id\"] == article_id])\n","\n","    return image_paths, labels\n","\n","class ImageDataset(torch.utils.data.Dataset):\n","    def __init__(self, image_paths, processor=None):\n","        self.image_paths = image_paths\n","        self.processor = processor\n","        self.image_ids = []\n","\n","        for image_path in self.image_paths:\n","            if not os.path.exists(image_path):\n","                raise FileNotFoundError(f\"Image {image_path} not found.\")\n","            else:\n","                image_id = int(image_path.split(\"/\")[-1].split(\".\")[0])\n","                self.image_ids.append(image_id)\n","            \n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        image = Image.open(self.image_paths[idx])\n","        if self.processor is not None:\n","            inputs = self.processor(images=image, return_tensors=\"pt\", padding=True)\n","            image = inputs[\"pixel_values\"][0]\n","        return image, self.image_ids[idx]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-01T23:42:14.659469Z","iopub.status.busy":"2024-10-01T23:42:14.659177Z","iopub.status.idle":"2024-10-01T23:42:18.346651Z","shell.execute_reply":"2024-10-01T23:42:18.345587Z","shell.execute_reply.started":"2024-10-01T23:42:14.659438Z"},"trusted":true},"outputs":[],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n","processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n","model = model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-01T23:42:18.348423Z","iopub.status.busy":"2024-10-01T23:42:18.348014Z","iopub.status.idle":"2024-10-01T23:42:19.261885Z","shell.execute_reply":"2024-10-01T23:42:19.260868Z","shell.execute_reply.started":"2024-10-01T23:42:18.348376Z"},"trusted":true},"outputs":[],"source":["text_path = '/kaggle/input/h-and-m-personalized-fashion-recommendations/articles.csv'\n","articles = pd.read_csv(text_path)\n","print(articles.shape) # 100k data points\n","articles.head(1)"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-10-01T23:42:19.263379Z","iopub.status.busy":"2024-10-01T23:42:19.263068Z","iopub.status.idle":"2024-10-01T23:42:19.528749Z","shell.execute_reply":"2024-10-01T23:42:19.527938Z","shell.execute_reply.started":"2024-10-01T23:42:19.263348Z"},"trusted":true},"outputs":[],"source":["# map from article_id to df index\n","article_id_to_idx = {article_id: idx for idx, article_id in enumerate(articles[\"article_id\"])}\n","\n","# get all classes of the dataframe\n","class_names = articles.columns.tolist()\n","label_names = dict()\n","label_names_to_idx = dict()\n","for class_name in class_names:\n","    label_names[class_name] = articles[class_name].unique()\n","    label_names_to_idx[class_name] = {label_name: idx for idx, label_name in enumerate(label_names[class_name])}\n","\n","article_ids = label_names[\"article_id\"]\n","selected_class_names = [\"product_group_name\", \"product_type_name\", \"graphical_appearance_name\", \"colour_group_name\", \"perceived_colour_value_name\", \"perceived_colour_master_name\", \"department_name\", \"index_name\", \"index_group_name\", \"section_name\", \"garment_group_name\"]"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-10-01T23:42:19.530450Z","iopub.status.busy":"2024-10-01T23:42:19.530047Z","iopub.status.idle":"2024-10-01T23:42:19.580594Z","shell.execute_reply":"2024-10-01T23:42:19.579858Z","shell.execute_reply.started":"2024-10-01T23:42:19.530404Z"},"trusted":true},"outputs":[],"source":["# get label names in product group name with less than 10 samples\n","product_group_name_cnts = articles[\"product_group_name\"].value_counts()\n","removed_label_names = product_group_name_cnts[product_group_name_cnts < 10]\n","\n","# remove data with the removed label name\n","removed_label_idxs = articles[articles[\"product_group_name\"].isin(removed_label_names.index)].index\n","articles = articles.drop(removed_label_idxs)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-01T23:42:19.582172Z","iopub.status.busy":"2024-10-01T23:42:19.581858Z","iopub.status.idle":"2024-10-01T23:45:03.914348Z","shell.execute_reply":"2024-10-01T23:45:03.913312Z","shell.execute_reply.started":"2024-10-01T23:42:19.582139Z"},"trusted":true},"outputs":[],"source":["data_dir = \"/kaggle/input/h-and-m-personalized-fashion-recommendations\"\n","image_paths, labels = get_image_paths_and_labels_from_df(articles, data_dir)\n","print(f\"Number of images: {len(image_paths)}\")"]},{"cell_type":"markdown","metadata":{},"source":["现在的数据集是按照prod_name直接来随机分的，训练：验证 = 8:2， 测试集为200个。同一个prod_name应该只会在三个数据集中的一个，满足fredrik的要求（待验证？）"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-01T23:48:39.583777Z","iopub.status.busy":"2024-10-01T23:48:39.582888Z","iopub.status.idle":"2024-10-01T23:48:40.714065Z","shell.execute_reply":"2024-10-01T23:48:40.713147Z","shell.execute_reply.started":"2024-10-01T23:48:39.583738Z"},"trusted":true},"outputs":[],"source":["prod_name_to_images = defaultdict(list)\n","for idx, label_df in enumerate(labels):\n","    prod_name = label_df['prod_name'].values[0]  # Extract scalar value\n","    prod_name_to_images[prod_name].append(idx)  # Store the index of the image\n","\n","all_prod_names = list(prod_name_to_images.keys())\n","random.seed(42)  # For reproducibility\n","random.shuffle(all_prod_names)\n","\n","#Allocate 'prod_name's to the test set until we have at least 200 images\n","test_prod_names = []\n","test_image_indices = []\n","total_test_images = 0\n","for prod_name in all_prod_names:\n","    indices = prod_name_to_images[prod_name]\n","    if total_test_images + len(indices) > 200:\n","        # Adjust to get exactly 200 images\n","        remaining_slots = 200 - total_test_images\n","        test_image_indices.extend(indices[:remaining_slots])\n","        total_test_images += remaining_slots\n","        break\n","    else:\n","        test_prod_names.append(prod_name)\n","        test_image_indices.extend(indices)\n","        total_test_images += len(indices)\n","    if total_test_images == 200:\n","        break\n","\n","# Remove selected 'prod_name's from the list of all 'prod_name's\n","remaining_prod_names = [pn for pn in all_prod_names if pn not in test_prod_names]\n","\n","# 8/2 split\n","num_train = int(0.8 * len(remaining_prod_names))\n","train_prod_names = remaining_prod_names[:num_train]\n","val_prod_names = remaining_prod_names[num_train:]\n","\n","train_image_indices = []\n","for prod_name in train_prod_names:\n","    train_image_indices.extend(prod_name_to_images[prod_name])\n","\n","val_image_indices = []\n","for prod_name in val_prod_names:\n","    val_image_indices.extend(prod_name_to_images[prod_name])\n","\n","train_image_paths = [image_paths[idx] for idx in train_image_indices]\n","train_labels = [labels[idx] for idx in train_image_indices]\n","\n","val_image_paths = [image_paths[idx] for idx in val_image_indices]\n","val_labels = [labels[idx] for idx in val_image_indices]\n","\n","test_image_paths = [image_paths[idx] for idx in test_image_indices]\n","test_labels = [labels[idx] for idx in test_image_indices]\n","\n","# #256 for test\n","# train_image_paths = train_image_paths[:256]\n","# train_labels = train_labels[:256]\n","\n","# val_image_paths = val_image_paths[:256]\n","# val_labels = val_labels[:256]\n","\n","print(f\"Number of training images: {len(train_image_paths)}\")\n","print(f\"Number of validation images: {len(val_image_paths)}\")\n","print(f\"Number of test images: {len(test_image_paths)}\")"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-10-01T23:48:47.316439Z","iopub.status.busy":"2024-10-01T23:48:47.315709Z","iopub.status.idle":"2024-10-01T23:48:47.718676Z","shell.execute_reply":"2024-10-01T23:48:47.717828Z","shell.execute_reply.started":"2024-10-01T23:48:47.316396Z"},"trusted":true},"outputs":[],"source":["train_dataset = ImageDataset(train_image_paths, processor)\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True)\n","\n","val_dataset = ImageDataset(val_image_paths, processor)\n","val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=256, shuffle=False)\n","\n","test_dataset = ImageDataset(test_image_paths, processor)\n","test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=False)"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-10-01T23:48:49.811627Z","iopub.status.busy":"2024-10-01T23:48:49.811222Z","iopub.status.idle":"2024-10-01T23:48:49.939867Z","shell.execute_reply":"2024-10-01T23:48:49.939106Z","shell.execute_reply.started":"2024-10-01T23:48:49.811590Z"},"trusted":true},"outputs":[],"source":["# Define LoRA configuration\n","lora_config = LoraConfig(\n","    r=8,                  # Low-rank dimension (adjustable)\n","    lora_alpha=32,          # Scaling factor (adjustable)\n","    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\"],  # Specify which layers to apply LoRA to\n","    lora_dropout=0.05,       # Dropout rate (optional)\n","    bias=\"none\",            # Whether to include biases (\"none\", \"all\", \"lora_only\")\n","    task_type=\"classification\"  # Task type (\"classification\" or \"regression\")\n",")\n","\n","# Apply LoRA to the CLIP model\n","model = get_peft_model(model, lora_config)"]},{"cell_type":"markdown","metadata":{},"source":["定义confusion matrix画几个类"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-10-01T23:59:34.667485Z","iopub.status.busy":"2024-10-01T23:59:34.666793Z","iopub.status.idle":"2024-10-01T23:59:34.671759Z","shell.execute_reply":"2024-10-01T23:59:34.670760Z","shell.execute_reply.started":"2024-10-01T23:59:34.667442Z"},"trusted":true},"outputs":[],"source":["N = 5  # Number of top classes to include in the confusion matrix"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-10-01T23:59:36.450856Z","iopub.status.busy":"2024-10-01T23:59:36.450487Z","iopub.status.idle":"2024-10-01T23:59:36.456726Z","shell.execute_reply":"2024-10-01T23:59:36.455770Z","shell.execute_reply.started":"2024-10-01T23:59:36.450822Z"},"trusted":true},"outputs":[],"source":["def plot_confusion_matrix(cm, class_labels, title='Confusion Matrix'):\n","    plt.figure(figsize=(10, 8))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","                xticklabels=class_labels, yticklabels=class_labels)\n","    plt.xlabel('Predicted Labels')\n","    plt.ylabel('True Labels')\n","    plt.title(title)\n","    plt.show()"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-10-01T23:59:38.141606Z","iopub.status.busy":"2024-10-01T23:59:38.140942Z","iopub.status.idle":"2024-10-01T23:59:38.160172Z","shell.execute_reply":"2024-10-01T23:59:38.159052Z","shell.execute_reply.started":"2024-10-01T23:59:38.141564Z"},"trusted":true},"outputs":[],"source":["def validate(model, val_dataloader, criteria, device, tokenized_texts, selected_class_names, epoch):\n","    model.eval()\n","    total_loss = 0.0\n","    total_correct = {class_name: 0 for class_name in selected_class_names}\n","    total_samples = 0\n","\n","    # Initialize containers for confusion matrices\n","    all_true_labels = {class_name: [] for class_name in selected_class_names}\n","    all_preds = {class_name: [] for class_name in selected_class_names}\n","\n","    with torch.no_grad():\n","        for images, image_ids in tqdm(val_dataloader):\n","            images = images.to(device)\n","            batch_size = images.size(0)\n","\n","            # Get true labels for all classes\n","            true_labels = {}\n","            for class_name in selected_class_names:\n","                labels = [\n","                    label_names_to_idx[class_name][\n","                        articles.loc[article_id_to_idx[image_id.item()], class_name]\n","                    ] for image_id in image_ids\n","                ]\n","                true_labels[class_name] = torch.tensor(labels).to(device)\n","\n","            # Compute image embeddings\n","            image_features = model.get_image_features(images)\n","            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n","\n","            loss = 0.0  # Reset loss for the batch\n","\n","            # Iterate over each class\n","            for class_name in selected_class_names:\n","                # Move tokenized text inputs to device\n","                inputs = tokenized_texts[class_name]\n","                inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","                # Compute text embeddings\n","                text_features = model.get_text_features(**inputs)\n","                text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n","\n","                # Compute similarity logits\n","                logits_per_image = image_features @ text_features.T  # Shape: [batch_size, num_labels]\n","\n","                # Compute loss for this class\n","                class_loss = criteria(logits_per_image, true_labels[class_name])\n","                loss += class_loss  # Sum losses from all classes\n","\n","                # Predictions and accuracy\n","                _, preds = torch.max(logits_per_image, dim=1)\n","                total_correct[class_name] += (preds == true_labels[class_name]).sum().item()\n","\n","                # Collect true labels and predictions for confusion matrix\n","                all_true_labels[class_name].extend(true_labels[class_name].cpu().numpy())\n","                all_preds[class_name].extend(preds.cpu().numpy())\n","\n","            total_loss += loss.item() * batch_size\n","            total_samples += batch_size\n","\n","    avg_loss = total_loss / total_samples\n","    accuracy = {class_name: total_correct[class_name] / total_samples for class_name in selected_class_names}\n","\n","    # Compute and display confusion matrices for validation\n","    for class_name in selected_class_names:\n","        true_labels_np = np.array(all_true_labels[class_name])\n","        preds_np = np.array(all_preds[class_name])\n","\n","        # Compute the frequency of each class in true labels\n","        label_counts = Counter(true_labels_np)\n","        # Get the top N classes\n","        top_N_classes = [label for label, _ in label_counts.most_common(N)]\n","        top_N_set = set(top_N_classes)\n","\n","        # Create a mapping from old labels to new indices\n","        label_to_new_index = {label: idx for idx, label in enumerate(top_N_classes)}\n","        other_label_index = N  # Index for 'Other' category\n","\n","        # Remap labels\n","        remapped_true_labels = np.array([\n","            label_to_new_index.get(label, other_label_index) for label in true_labels_np\n","        ])\n","        remapped_preds = np.array([\n","            label_to_new_index.get(label, other_label_index) for label in preds_np\n","        ])\n","\n","        # Update class labels for the confusion matrix\n","        class_labels_for_cm = [label_names[class_name][label] for label in top_N_classes] + ['Other']\n","\n","        # Compute confusion matrix\n","        cm = confusion_matrix(\n","            remapped_true_labels,\n","            remapped_preds,\n","            labels=list(range(N + 1))\n","        )\n","\n","        # Plot confusion matrix\n","        plot_confusion_matrix(\n","            cm,\n","            class_labels_for_cm,\n","            title=f'Validation Confusion Matrix for {class_name} (Epoch {epoch+1})'\n","        )\n","\n","    return avg_loss, accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-10-02T00:00:04.919645Z","iopub.status.busy":"2024-10-02T00:00:04.919273Z","iopub.status.idle":"2024-10-02T00:00:48.674783Z","shell.execute_reply":"2024-10-02T00:00:48.673624Z","shell.execute_reply.started":"2024-10-02T00:00:04.919611Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["num_epochs = 2  # Adjust as needed\n","criteria = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n","\n","model.to(device)\n","\n","# Tokenize text inputs for all labels in all classes outside the batch loop\n","tokenized_texts = {}\n","for class_name in selected_class_names:\n","    labels = label_names[class_name]\n","    texts = [f\"A photo of a {label}\" for label in labels]\n","    tokenized_texts[class_name] = processor(\n","        text=texts,\n","        return_tensors=\"pt\",\n","        padding=True\n","    )\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0.0\n","    total_correct = {class_name: 0 for class_name in selected_class_names}\n","    total_samples = 0\n","    \n","    # Initialize containers for confusion matrices\n","    all_true_labels = {class_name: [] for class_name in selected_class_names}\n","    all_preds = {class_name: [] for class_name in selected_class_names}\n","\n","    for images, image_ids in tqdm(train_dataloader):\n","        images = images.to(device)\n","        batch_size = images.size(0)\n","\n","        # Get true labels for all classes\n","        true_labels = {}\n","        for class_name in selected_class_names:\n","            labels = [\n","                label_names_to_idx[class_name][\n","                    articles.loc[article_id_to_idx[image_id.item()], class_name]\n","                ] for image_id in image_ids\n","            ]\n","            true_labels[class_name] = torch.tensor(labels).to(device)\n","\n","        # Forward pass: compute image embeddings\n","        image_features = model.get_image_features(images)\n","        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n","\n","        loss = 0.0  # Reset loss for the batch\n","\n","        # Iterate over each class\n","        for class_name in selected_class_names:\n","            # Move tokenized text inputs to device\n","            inputs = tokenized_texts[class_name]\n","            inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","            # Compute text embeddings\n","            text_features = model.get_text_features(**inputs)\n","            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n","\n","            # Compute similarity logits\n","            logits_per_image = image_features @ text_features.T  # Shape: [batch_size, num_labels]\n","\n","            # Compute loss for this class\n","            class_loss = criteria(logits_per_image, true_labels[class_name])\n","            loss += class_loss  # Sum losses from all classes\n","\n","            # Predictions and accuracy\n","            _, preds = torch.max(logits_per_image, dim=1)\n","            total_correct[class_name] += (preds == true_labels[class_name]).sum().item()\n","            \n","            # Collect true labels and predictions for confusion matrix\n","            all_true_labels[class_name].extend(true_labels[class_name].cpu().numpy())\n","            all_preds[class_name].extend(preds.cpu().numpy())\n","\n","        total_loss += loss.item() * batch_size\n","        total_samples += batch_size\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Compute average loss and accuracy\n","    avg_loss = total_loss / total_samples\n","    accuracy = {class_name: total_correct[class_name] / total_samples for class_name in selected_class_names}\n","    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n","    for class_name in selected_class_names:\n","        print(f\"Accuracy for {class_name}: {accuracy[class_name]:.4f}\")\n","    \n","    # Compute and display confusion matrices for training\n","    for class_name in selected_class_names:\n","        true_labels_np = np.array(all_true_labels[class_name])\n","        preds_np = np.array(all_preds[class_name])\n","\n","        # Compute the frequency of each class in true labels\n","        label_counts = Counter(true_labels_np)\n","        # Get the top N classes\n","        top_N_classes = [label for label, _ in label_counts.most_common(N)]\n","        top_N_set = set(top_N_classes)\n","\n","        # Create a mapping from old labels to new indices\n","        label_to_new_index = {label: idx for idx, label in enumerate(top_N_classes)}\n","        other_label_index = N  # Index for 'Other' category\n","\n","        # Remap labels\n","        remapped_true_labels = np.array([\n","            label_to_new_index.get(label, other_label_index) for label in true_labels_np\n","        ])\n","        remapped_preds = np.array([\n","            label_to_new_index.get(label, other_label_index) for label in preds_np\n","        ])\n","\n","        # Update class labels for the confusion matrix\n","        class_labels_for_cm = [label_names[class_name][label] for label in top_N_classes] + ['Other']\n","\n","        # Compute confusion matrix\n","        cm = confusion_matrix(\n","            remapped_true_labels,\n","            remapped_preds,\n","            labels=list(range(N + 1))\n","        )\n","\n","        # Plot confusion matrix\n","        plot_confusion_matrix(\n","            cm,\n","            class_labels_for_cm,\n","            title=f'Training Confusion Matrix for {class_name} (Epoch {epoch+1})'\n","        )\n","\n","    # Validate after each epoch\n","    val_loss, val_accuracy = validate(model, val_dataloader, criteria, device, tokenized_texts, selected_class_names, epoch)\n","    print(f\"Validation Loss: {val_loss:.4f}\")\n","    for class_name in selected_class_names:\n","        print(f\"Validation Accuracy for {class_name}: {val_accuracy[class_name]:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-01T23:45:03.990710Z","iopub.status.idle":"2024-10-01T23:45:03.991053Z","shell.execute_reply":"2024-10-01T23:45:03.990900Z","shell.execute_reply.started":"2024-10-01T23:45:03.990882Z"},"trusted":true},"outputs":[],"source":["model_save_path = 'model.pth'\n","torch.save(model.state_dict(), model_save_path)\n","print(f\"Model saved to {model_save_path}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-01T23:45:03.992477Z","iopub.status.idle":"2024-10-01T23:45:03.992827Z","shell.execute_reply":"2024-10-01T23:45:03.992675Z","shell.execute_reply.started":"2024-10-01T23:45:03.992657Z"},"trusted":true},"outputs":[],"source":["test_loss, test_accuracy = validate(model, test_dataloader, criteria, device, text_inputs, selected_class_names, 0)\n","print(f\"Test Loss: {test_loss:.4f}\")\n","for class_name in selected_class_names:\n","    print(f\"Test Accuracy for {class_name}: {test_accuracy[class_name]:.4f}\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":3103714,"sourceId":31254,"sourceType":"competition"}],"dockerImageVersionId":30776,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
