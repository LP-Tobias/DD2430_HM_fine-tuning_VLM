{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_file_path = Path().resolve()\n",
    "par_file_path = cur_file_path.parent\n",
    "#print(str(cur_file_path)+\"/h-and-m-personalized-fashion-recommendations\")\n",
    "articles = pd.read_csv(str(par_file_path)+\"/h-and-m-personalized-fashion-recommendations/articles.csv\")\n",
    "customers = pd.read_csv(str(par_file_path)+\"/h-and-m-personalized-fashion-recommendations/customers.csv\")\n",
    "transactions = pd.read_csv(str(par_file_path)+\"/h-and-m-personalized-fashion-recommendations/transactions_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in articles.columns:\n",
    "    if not 'no' in col and not 'code' in col and not 'id' in col:\n",
    "        un_n = articles[col].nunique()\n",
    "        print(f'n of unique {col}: {un_n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Clone the repository if it doesn't exist\n",
    "if not os.path.exists('./Transformer-MM-Explainability'):\n",
    "    !git clone https://github.com/hila-chefer/Transformer-MM-Explainability\n",
    "\n",
    "os.chdir(f'./Transformer-MM-Explainability')\n",
    "\n",
    "import torch\n",
    "import CLIP.clip as clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from captum.attr import visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Control context expansion (number of attention layers to consider)\n",
    "#@title Number of layers for image Transformer\n",
    "start_layer =  -1#@param {type:\"number\"}\n",
    "\n",
    "#@title Number of layers for text Transformer\n",
    "start_layer_text =  -1#@param {type:\"number\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret(image, texts, model, device, start_layer=start_layer, start_layer_text=start_layer_text):\n",
    "    batch_size = texts.shape[0]\n",
    "    images = image.repeat(batch_size, 1, 1, 1)\n",
    "    logits_per_image, logits_per_text = model(images, texts)\n",
    "    probs = logits_per_image.softmax(dim=-1).detach().cpu().numpy()\n",
    "    index = [i for i in range(batch_size)]\n",
    "    one_hot = np.zeros((logits_per_image.shape[0], logits_per_image.shape[1]), dtype=np.float32)\n",
    "    one_hot[torch.arange(logits_per_image.shape[0]), index] = 1\n",
    "    one_hot = torch.from_numpy(one_hot).requires_grad_(True)\n",
    "    # one_hot = torch.sum(one_hot.cuda() * logits_per_image)\n",
    "    one_hot = torch.sum(one_hot * logits_per_image)\n",
    "    model.zero_grad()\n",
    "\n",
    "    image_attn_blocks = list(dict(model.visual.transformer.resblocks.named_children()).values())\n",
    "\n",
    "    if start_layer == -1: \n",
    "      # calculate index of last layer \n",
    "      start_layer = len(image_attn_blocks) - 1\n",
    "    \n",
    "    num_tokens = image_attn_blocks[0].attn_probs.shape[-1]\n",
    "    R = torch.eye(num_tokens, num_tokens, dtype=image_attn_blocks[0].attn_probs.dtype).to(device)\n",
    "    R = R.unsqueeze(0).expand(batch_size, num_tokens, num_tokens)\n",
    "    for i, blk in enumerate(image_attn_blocks):\n",
    "        if i < start_layer:\n",
    "          continue\n",
    "        grad = torch.autograd.grad(one_hot, [blk.attn_probs], retain_graph=True)[0].detach()\n",
    "        cam = blk.attn_probs.detach()\n",
    "        cam = cam.reshape(-1, cam.shape[-1], cam.shape[-1])\n",
    "        grad = grad.reshape(-1, grad.shape[-1], grad.shape[-1])\n",
    "        cam = grad * cam\n",
    "        cam = cam.reshape(batch_size, -1, cam.shape[-1], cam.shape[-1])\n",
    "        cam = cam.clamp(min=0).mean(dim=1)\n",
    "        R = R + torch.bmm(cam, R)\n",
    "    image_relevance = R[:, 0, 1:]\n",
    "\n",
    "    \n",
    "    text_attn_blocks = list(dict(model.transformer.resblocks.named_children()).values())\n",
    "\n",
    "    if start_layer_text == -1: \n",
    "      # calculate index of last layer \n",
    "      start_layer_text = len(text_attn_blocks) - 1\n",
    "\n",
    "    num_tokens = text_attn_blocks[0].attn_probs.shape[-1]\n",
    "    R_text = torch.eye(num_tokens, num_tokens, dtype=text_attn_blocks[0].attn_probs.dtype).to(device)\n",
    "    R_text = R_text.unsqueeze(0).expand(batch_size, num_tokens, num_tokens)\n",
    "    for i, blk in enumerate(text_attn_blocks):\n",
    "        if i < start_layer_text:\n",
    "          continue\n",
    "        grad = torch.autograd.grad(one_hot, [blk.attn_probs], retain_graph=True)[0].detach()\n",
    "        cam = blk.attn_probs.detach()\n",
    "        cam = cam.reshape(-1, cam.shape[-1], cam.shape[-1])\n",
    "        grad = grad.reshape(-1, grad.shape[-1], grad.shape[-1])\n",
    "        cam = grad * cam\n",
    "        cam = cam.reshape(batch_size, -1, cam.shape[-1], cam.shape[-1])\n",
    "        cam = cam.clamp(min=0).mean(dim=1)\n",
    "        R_text = R_text + torch.bmm(cam, R_text)\n",
    "    text_relevance = R_text\n",
    "   \n",
    "    return text_relevance, image_relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_relevance(image_relevance, image, orig_image):\n",
    "    # create heatmap from mask on image\n",
    "    def show_cam_on_image(img, mask):\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
    "        heatmap = np.float32(heatmap) / 255\n",
    "        cam = heatmap + np.float32(img)\n",
    "        cam = cam / np.max(cam)\n",
    "        return cam\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    axs[0].imshow(orig_image);\n",
    "    axs[0].axis('off');\n",
    "\n",
    "    dim = int(image_relevance.numel() ** 0.5)\n",
    "    image_relevance = image_relevance.reshape(1, 1, dim, dim)\n",
    "    image_relevance = torch.nn.functional.interpolate(image_relevance, size=224, mode='bilinear')\n",
    "    # image_relevance = image_relevance.reshape(224, 224).cuda().data.cpu().numpy()\n",
    "    image_relevance = image_relevance.reshape(224, 224).data.cpu().numpy()\n",
    "    image_relevance = (image_relevance - image_relevance.min()) / (image_relevance.max() - image_relevance.min())\n",
    "    image = image[0].permute(1, 2, 0).data.cpu().numpy()\n",
    "    image = (image - image.min()) / (image.max() - image.min())\n",
    "    vis = show_cam_on_image(image, image_relevance)\n",
    "    vis = np.uint8(255 * vis)\n",
    "    vis = cv2.cvtColor(np.array(vis), cv2.COLOR_RGB2BGR)\n",
    "    axs[1].imshow(vis);\n",
    "    axs[1].axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CLIP.clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "_tokenizer = _Tokenizer()\n",
    "\n",
    "def show_heatmap_on_text(text, text_encoding, R_text):\n",
    "  CLS_idx = text_encoding.argmax(dim=-1)\n",
    "  R_text = R_text[CLS_idx, 1:CLS_idx]\n",
    "  text_scores = R_text / R_text.sum()\n",
    "  text_scores = text_scores.flatten()\n",
    "  print(text_scores)\n",
    "  text_tokens=_tokenizer.encode(text)\n",
    "  text_tokens_decoded=[_tokenizer.decode([a]) for a in text_tokens]\n",
    "  vis_data_records = [visualization.VisualizationDataRecord(text_scores,0,0,0,0,0,text_tokens_decoded,1)]\n",
    "  visualization.visualize_text(vis_data_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip.clip._MODELS = {\n",
    "    \"ViT-B/32\": \"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\",\n",
    "    \"ViT-B/16\": \"https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt\",\n",
    "    \"ViT-L/14\": \"https://openaipublic.azureedge.net/clip/models/b8cca3fd41ae0c99ba7e8951adf17d267cdb84cd88be6f7c2e0eca1737a03836/ViT-L-14.pt\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_group_name = articles['index_group_name'].unique()\n",
    "\n",
    "img_path = \"/Users/tobiaspeihengli/Downloads/DD2430/h-and-m-personalized-fashion-recommendations/images/010/0108775015.jpg\"\n",
    "img = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
    "texts = [f\"a photo of {index}\" for index in index_group_name]\n",
    "text = clip.tokenize(texts).to(device)\n",
    "\n",
    "R_text, R_image = interpret(model=model, image=img, texts=text, device=device)\n",
    "batch_size = text.shape[0]\n",
    "for i in range(batch_size):\n",
    "  show_heatmap_on_text(texts[i], text[i], R_text[i])\n",
    "  show_image_relevance(R_image[i], img, orig_image=Image.open(img_path))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceived_colour_value_name = articles['perceived_colour_value_name'].unique()\n",
    "\n",
    "img_path = \"/Users/tobiaspeihengli/Downloads/DD2430/h-and-m-personalized-fashion-recommendations/images/010/0108775015.jpg\"\n",
    "img = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
    "texts = [f\"It is a {index} cloth\" for index in perceived_colour_value_name]\n",
    "text = clip.tokenize(texts).to(device)\n",
    "\n",
    "R_text, R_image = interpret(model=model, image=img, texts=text, device=device)\n",
    "batch_size = text.shape[0]\n",
    "for i in range(batch_size):\n",
    "  show_heatmap_on_text(texts[i], text[i], R_text[i])\n",
    "  show_image_relevance(R_image[i], img, orig_image=Image.open(img_path))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceived_colour_value_name = articles['perceived_colour_value_name'].unique()\n",
    "\n",
    "img_path = \"/Users/tobiaspeihengli/Downloads/DD2430/h-and-m-personalized-fashion-recommendations/images/010/0108775015.jpg\"\n",
    "img = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
    "texts = [f\"a photo of {index}\" for index in perceived_colour_value_name]\n",
    "text = clip.tokenize(texts).to(device)\n",
    "\n",
    "R_text, R_image = interpret(model=model, image=img, texts=text, device=device)\n",
    "batch_size = text.shape[0]\n",
    "for i in range(batch_size):\n",
    "  show_heatmap_on_text(texts[i], text[i], R_text[i])\n",
    "  show_image_relevance(R_image[i], img, orig_image=Image.open(img_path))\n",
    "  plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
