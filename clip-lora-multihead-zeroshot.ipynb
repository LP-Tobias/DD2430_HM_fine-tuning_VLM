{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T23:16:02.749950Z","iopub.status.busy":"2024-10-08T23:16:02.749600Z","iopub.status.idle":"2024-10-08T23:16:23.680407Z","shell.execute_reply":"2024-10-08T23:16:23.679521Z","shell.execute_reply.started":"2024-10-08T23:16:02.749912Z"},"trusted":true},"outputs":[],"source":["# !pip install peft\n","from PIL import Image\n","import requests\n","from transformers import CLIPProcessor, CLIPModel\n","import torch\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","from sklearn.model_selection import train_test_split\n","import os\n","from peft import LoraConfig, get_peft_model\n","import wandb"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T23:16:23.682277Z","iopub.status.busy":"2024-10-08T23:16:23.681971Z","iopub.status.idle":"2024-10-08T23:16:23.692167Z","shell.execute_reply":"2024-10-08T23:16:23.691237Z","shell.execute_reply.started":"2024-10-08T23:16:23.682244Z"},"trusted":true},"outputs":[],"source":["def get_image_paths_and_labels_from_df(df, data_dir):\n","    article_ids = df[\"article_id\"].values\n","    image_paths = []\n","    labels = []\n","    \n","    for article_id in article_ids:\n","        image_path = f\"{data_dir}/images/0{str(article_id)[:2]}/0{article_id}.jpg\"\n","        # Check if the image file exists\n","        if os.path.exists(image_path):\n","            image_paths.append(image_path)\n","            # Add corresponding label only if the image exists\n","            labels.append(df[df[\"article_id\"] == article_id])\n","\n","    return image_paths, labels\n","\n","class ImageDataset(torch.utils.data.Dataset):\n","    def __init__(self, image_paths, processor=None):\n","        self.image_paths = image_paths\n","        self.processor = processor\n","        self.image_ids = []\n","\n","        for image_path in self.image_paths:\n","            if not os.path.exists(image_path):\n","                raise FileNotFoundError(f\"Image {image_path} not found.\")\n","            else:\n","                image_id = int(image_path.split(\"/\")[-1].split(\".\")[0])\n","                self.image_ids.append(image_id)\n","            \n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        image = Image.open(self.image_paths[idx])\n","        if self.processor is not None:\n","            inputs = self.processor(images=image, return_tensors=\"pt\", padding=True)\n","            image = inputs[\"pixel_values\"][0]\n","        return image, self.image_ids[idx]"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T23:16:23.693415Z","iopub.status.busy":"2024-10-08T23:16:23.693151Z","iopub.status.idle":"2024-10-08T23:16:23.712807Z","shell.execute_reply":"2024-10-08T23:16:23.711985Z","shell.execute_reply.started":"2024-10-08T23:16:23.693385Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x7fea451c90f0>"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["torch.manual_seed(42)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T23:16:23.715445Z","iopub.status.busy":"2024-10-08T23:16:23.715179Z","iopub.status.idle":"2024-10-08T23:16:41.065895Z","shell.execute_reply":"2024-10-08T23:16:41.065076Z","shell.execute_reply.started":"2024-10-08T23:16:23.715415Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/user/miniforge3/envs/hf/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\", cache_dir=\"model\", local_files_only=False)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T23:16:41.067932Z","iopub.status.busy":"2024-10-08T23:16:41.067176Z","iopub.status.idle":"2024-10-08T23:16:42.006574Z","shell.execute_reply":"2024-10-08T23:16:42.005334Z","shell.execute_reply.started":"2024-10-08T23:16:41.067884Z"},"trusted":true},"outputs":[],"source":["# text_path = '/kaggle/input/h-and-m-personalized-fashion-recommendations/articles.csv'\n","text_path = 'data/articles.csv'\n","articles = pd.read_csv(text_path)\n","data_dir = 'data'"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T23:16:42.008508Z","iopub.status.busy":"2024-10-08T23:16:42.008093Z","iopub.status.idle":"2024-10-08T23:16:42.305622Z","shell.execute_reply":"2024-10-08T23:16:42.304645Z","shell.execute_reply.started":"2024-10-08T23:16:42.008443Z"},"trusted":true},"outputs":[],"source":["# map from article_id to df index\n","article_id_to_idx = {article_id: idx for idx, article_id in enumerate(articles[\"article_id\"])}\n","\n","# get all classes of the dataframe\n","class_names = articles.columns.tolist()\n","label_names = dict()\n","label_names_to_idx = dict()\n","for class_name in class_names:\n","    label_names[class_name] = articles[class_name].unique()\n","    label_names_to_idx[class_name] = {label_name: idx for idx, label_name in enumerate(label_names[class_name])}\n","\n","article_ids = label_names[\"article_id\"]\n","#selected_class_names = [\"product_type_name\", \"graphical_appearance_name\"]\n","selected_class_names = [\"product_group_name\", \"product_type_name\", \"graphical_appearance_name\", \"colour_group_name\", \"perceived_colour_value_name\", \"perceived_colour_master_name\", \"department_name\", \"index_name\", \"index_group_name\", \"section_name\", \"garment_group_name\"]"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T23:16:42.307287Z","iopub.status.busy":"2024-10-08T23:16:42.306975Z","iopub.status.idle":"2024-10-08T23:17:04.605101Z","shell.execute_reply":"2024-10-08T23:17:04.604169Z","shell.execute_reply.started":"2024-10-08T23:16:42.307254Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["len(train_df)=84445 len(val_df)=10534 len(test_df)=10563\n"]}],"source":["# grouped by product_code\n","grouped = articles.groupby(\"product_code\")\n","groups = [group for _, group in grouped]\n","\n","# split 0.8/0.1/0.1\n","train_groups, test_groups = train_test_split(groups, test_size=0.2, random_state=42) \n","val_groups, test_groups = train_test_split(test_groups, test_size=0.5, random_state=42) \n","\n","train_df = pd.concat(train_groups)\n","val_df = pd.concat(val_groups)\n","test_df = pd.concat(test_groups)\n","\n","print(f\"{len(train_df)=} {len(val_df)=} {len(test_df)=}\")"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T23:17:04.606390Z","iopub.status.busy":"2024-10-08T23:17:04.606114Z","iopub.status.idle":"2024-10-08T23:22:28.822877Z","shell.execute_reply":"2024-10-08T23:22:28.822067Z","shell.execute_reply.started":"2024-10-08T23:17:04.606360Z"},"trusted":true},"outputs":[],"source":["train_paths, train_labels = get_image_paths_and_labels_from_df(train_df, data_dir)\n","val_paths, val_labels = get_image_paths_and_labels_from_df(val_df, data_dir)\n","test_paths, test_labels = get_image_paths_and_labels_from_df(test_df, data_dir)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T23:22:28.824716Z","iopub.status.busy":"2024-10-08T23:22:28.824358Z","iopub.status.idle":"2024-10-08T23:22:28.831472Z","shell.execute_reply":"2024-10-08T23:22:28.830508Z","shell.execute_reply.started":"2024-10-08T23:22:28.824678Z"},"trusted":true},"outputs":[],"source":["class MultiOutputLayer(torch.nn.Module):\n","    def __init__(self, input_size, inter_size, output_size):\n","        super(MultiOutputLayer, self).__init__()\n","        self.fc1 = torch.nn.Linear(input_size, inter_size)\n","        self.fc2 = torch.nn.Linear(inter_size, output_size)\n","        self.dropout = torch.nn.Dropout(0.5)\n","        self.act = torch.nn.SiLU()\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.act(x)\n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","        return x"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T23:22:28.836711Z","iopub.status.busy":"2024-10-08T23:22:28.836295Z","iopub.status.idle":"2024-10-08T23:22:28.848992Z","shell.execute_reply":"2024-10-08T23:22:28.848117Z","shell.execute_reply.started":"2024-10-08T23:22:28.836676Z"},"trusted":true},"outputs":[],"source":["class MultiOutputClipModel(torch.nn.Module):\n","    def __init__(self, clip_model, class_names, vision_hidden_size, inter_size, output_size):\n","        super(MultiOutputClipModel, self).__init__()\n","        self.clip_model = clip_model\n","        self.class_names = class_names\n","        self.output_layers = torch.nn.ModuleDict({\n","            class_name: MultiOutputLayer(vision_hidden_size, inter_size, output_size)\n","            for class_name in self.class_names\n","        })\n","    \n","    def forward(\n","        self,\n","        text_input_dict,\n","        pixel_values,\n","        # position_ids = None,\n","        output_attentions = None,\n","        output_hidden_states = None,\n","        return_dict = None,\n","    ):\n","\n","        output_attentions = output_attentions if output_attentions is not None else self.clip_model.config.output_attentions\n","        output_hidden_states = (\n","            output_hidden_states if output_hidden_states is not None else self.clip_model.config.output_hidden_states\n","        )\n","        return_dict = return_dict if return_dict is not None else self.clip_model.config.use_return_dict\n","\n","        vision_outputs = self.clip_model.vision_model(\n","            pixel_values=pixel_values,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        vision_embeds = vision_outputs[1]\n","        vision_embeds_dict = {\n","            class_name: output_layer(vision_embeds) \n","                for class_name, output_layer in self.output_layers.items()\n","        }\n","\n","        text_outputs_dict = {\n","            class_name: self.clip_model.text_model(\n","                input_ids=text_input_dict[class_name][\"input_ids\"],\n","                attention_mask=text_input_dict[class_name][\"attention_mask\"],\n","                # position_ids=position_ids,\n","                output_attentions=output_attentions,\n","                output_hidden_states=output_hidden_states,\n","                return_dict=return_dict,\n","            ) for class_name in self.class_names\n","        }\n","\n","        text_embeds_dict = {\n","            class_name: self.clip_model.text_projection(text_outputs[1])\n","                for class_name, text_outputs in text_outputs_dict.items()\n","        }\n","\n","        logits_per_image_dict = {\n","            class_name: vision_embeds_dict[class_name] @ text_embeds_dict[class_name].T\n","                for class_name in self.output_layers.keys()\n","        }\n","\n","        return logits_per_image_dict"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T23:22:28.850613Z","iopub.status.busy":"2024-10-08T23:22:28.850234Z","iopub.status.idle":"2024-10-08T23:22:28.863065Z","shell.execute_reply":"2024-10-08T23:22:28.862214Z","shell.execute_reply.started":"2024-10-08T23:22:28.850571Z"},"trusted":true},"outputs":[],"source":["# custom criterion: cross entropy loss across all classes\n","class MultiOutputClipCriterion(torch.nn.Module):\n","    def __init__(self, class_names):\n","        super(MultiOutputClipCriterion, self).__init__()\n","        self.class_names = class_names\n","        self.criterion = torch.nn.CrossEntropyLoss()\n","\n","    def forward(self, logits_dict, labels_dict):\n","        loss = 0\n","        for class_name in self.class_names:\n","            logits = logits_dict[class_name]\n","            labels = labels_dict[class_name]\n","            loss += self.criterion(logits, labels)\n","        return loss"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T23:24:16.468053Z","iopub.status.busy":"2024-10-08T23:24:16.467616Z","iopub.status.idle":"2024-10-08T23:24:57.730306Z","shell.execute_reply":"2024-10-08T23:24:57.729503Z","shell.execute_reply.started":"2024-10-08T23:24:16.468014Z"},"trusted":true},"outputs":[],"source":["os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","\n","train_dataset = ImageDataset(train_paths, processor)\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=8)\n","\n","val_dataset = ImageDataset(val_paths, processor)\n","val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=8)\n","\n","test_dataset = ImageDataset(test_paths, processor)\n","test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=8)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T23:25:49.096859Z","iopub.status.busy":"2024-10-08T23:25:49.095990Z","iopub.status.idle":"2024-10-08T23:25:49.131896Z","shell.execute_reply":"2024-10-08T23:25:49.131148Z","shell.execute_reply.started":"2024-10-08T23:25:49.096818Z"},"trusted":true},"outputs":[],"source":["# generate text input\n","text_input_dict = {\n","    class_name: processor(text=[f\"A photo of a {label}\" for label in label_names[class_name]], \n","                          return_tensors=\"pt\", padding=True).to(device)\n","    for class_name in selected_class_names\n","}"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T23:25:54.154589Z","iopub.status.busy":"2024-10-08T23:25:54.154218Z","iopub.status.idle":"2024-10-08T23:25:59.430180Z","shell.execute_reply":"2024-10-08T23:25:59.429333Z","shell.execute_reply.started":"2024-10-08T23:25:54.154556Z"},"trusted":true},"outputs":[],"source":["# from kaggle_secrets import UserSecretsClient\n","# user_secrets = UserSecretsClient()\n","# secret_value_0 = user_secrets.get_secret(\"wandb_key\")\n","# wandb.login(key=secret_value_0)\n","\n","\n","def validate(model, dataloader, criteria, device, text_inputs, class_names):\n","    model.eval()\n","    total_loss = 0.0\n","    total_correct = {class_name: 0 for class_name in class_names}\n","    total_samples = 0\n","\n","    with torch.no_grad():\n","        for images, image_ids in tqdm(dataloader):\n","            images = images.to(device)\n","            logits_per_image_dict = model(pixel_values=images, text_input_dict=text_inputs)\n","\n","            # 获取真实标签\n","            true_labels_dict = {\n","                class_name: [label_names_to_idx[class_name][articles.loc[article_id_to_idx[image_id.item()], class_name]] \n","                             for image_id in image_ids]\n","                for class_name in class_names\n","            }\n","            true_labels_dict = {class_name: torch.tensor(true_labels).to(device)\n","                                for class_name, true_labels in true_labels_dict.items()}\n","            \n","            # 计算损失\n","            loss = criteria(logits_per_image_dict, true_labels_dict)\n","            total_loss += loss.item() * images.size(0)\n","\n","            # 计算准确率\n","            total_samples += images.size(0)\n","            for class_name in class_names:\n","                _, preds = torch.max(logits_per_image_dict[class_name], dim=1)\n","                total_correct[class_name] += (preds == true_labels_dict[class_name]).sum().item()\n","\n","    avg_loss = total_loss / total_samples / len(class_names)\n","    accuracy = {class_name: total_correct[class_name] / total_samples for class_name in class_names}\n","    return avg_loss, accuracy\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# get current date and time\n","from datetime import datetime\n","now = datetime.now()\n","date = now.strftime(\"%Y%m%d-%H%M%S\")"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T23:26:29.656194Z","iopub.status.busy":"2024-10-08T23:26:29.655494Z","iopub.status.idle":"2024-10-08T23:26:49.549818Z","shell.execute_reply":"2024-10-08T23:26:49.548503Z","shell.execute_reply.started":"2024-10-08T23:26:29.656155Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33monjackay\u001b[0m (\u001b[33monjackay-kth-royal-institute-of-technology\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["Tracking run with wandb version 0.18.2"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/user/kth/2430/DD2430_HM_fine-tuning_VLM/wandb/run-20241019_174154-8x4g22ph</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora/runs/8x4g22ph' target=\"_blank\">20241019-173904_zeroshot_128</a></strong> to <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora' target=\"_blank\">https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora/runs/8x4g22ph' target=\"_blank\">https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora/runs/8x4g22ph</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1/1 [00:10<00:00, 10.54s/it]\n","100%|██████████| 42/42 [02:49<00:00,  4.03s/it]\n","100%|██████████| 1/1 [00:10<00:00, 10.94s/it]\n","100%|██████████| 42/42 [03:47<00:00,  5.42s/it]\n","100%|██████████| 1/1 [00:10<00:00, 10.33s/it]\n","100%|██████████| 42/42 [04:12<00:00,  6.01s/it]\n","100%|██████████| 1/1 [00:12<00:00, 12.94s/it]\n","100%|██████████| 42/42 [04:51<00:00,  6.95s/it]\n","100%|██████████| 1/1 [00:12<00:00, 12.10s/it]\n","100%|██████████| 42/42 [04:33<00:00,  6.52s/it]\n","100%|██████████| 1/1 [00:12<00:00, 12.41s/it]\n","100%|██████████| 42/42 [05:03<00:00,  7.22s/it]\n","100%|██████████| 1/1 [00:12<00:00, 12.92s/it]\n","100%|██████████| 42/42 [04:56<00:00,  7.06s/it]\n","100%|██████████| 1/1 [00:12<00:00, 12.93s/it]\n","100%|██████████| 42/42 [04:33<00:00,  6.50s/it]\n","100%|██████████| 1/1 [00:11<00:00, 11.80s/it]\n","100%|██████████| 42/42 [04:52<00:00,  6.96s/it]\n","100%|██████████| 1/1 [00:11<00:00, 11.92s/it]\n","100%|██████████| 42/42 [05:55<00:00,  8.46s/it]\n"]},{"data":{"text/html":["Finishing last run (ID:8x4g22ph) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8ffc683653fd4cba81f18d92872d6361","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy_128_colour_group_name</td><td>▁▅▅▅▂▂▅███</td></tr><tr><td>test_accuracy_128_department_name</td><td>▁▂▁▃▃▄▇██▇</td></tr><tr><td>test_accuracy_128_garment_group_name</td><td>▁▆████████</td></tr><tr><td>test_accuracy_128_graphical_appearance_name</td><td>▃███▁▁▁▄██</td></tr><tr><td>test_accuracy_128_index_group_name</td><td>▁▁▆▆▇▁▁███</td></tr><tr><td>test_accuracy_128_index_name</td><td>▁▄███▆▇███</td></tr><tr><td>test_accuracy_128_perceived_colour_master_name</td><td>▁▄▆▆▄████▇</td></tr><tr><td>test_accuracy_128_perceived_colour_value_name</td><td>▁▁▆█▇█████</td></tr><tr><td>test_accuracy_128_product_group_name</td><td>██▂▁▃█████</td></tr><tr><td>test_accuracy_128_product_type_name</td><td>▂▄█▇▇▇▇▅▁▃</td></tr><tr><td>test_accuracy_128_section_name</td><td>▂▇▁▁█▇▇▇▇▇</td></tr><tr><td>test_loss_128</td><td>█▆▅▅▃▃▂▂▁▁</td></tr><tr><td>train_accuracy</td><td>▁▄▆▆▇▆▆▇▇█</td></tr><tr><td>train_loss</td><td>█▄▃▃▃▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy_128_colour_group_name</td><td>0.28853</td></tr><tr><td>test_accuracy_128_department_name</td><td>0.03464</td></tr><tr><td>test_accuracy_128_garment_group_name</td><td>0.20625</td></tr><tr><td>test_accuracy_128_graphical_appearance_name</td><td>0.45729</td></tr><tr><td>test_accuracy_128_index_group_name</td><td>0.38335</td></tr><tr><td>test_accuracy_128_index_name</td><td>0.23984</td></tr><tr><td>test_accuracy_128_perceived_colour_master_name</td><td>0.24943</td></tr><tr><td>test_accuracy_128_perceived_colour_value_name</td><td>0.54556</td></tr><tr><td>test_accuracy_128_product_group_name</td><td>0.4012</td></tr><tr><td>test_accuracy_128_product_type_name</td><td>0.04157</td></tr><tr><td>test_accuracy_128_section_name</td><td>0.04746</td></tr><tr><td>test_loss_128</td><td>2.72378</td></tr><tr><td>train_accuracy</td><td>0.28196</td></tr><tr><td>train_loss</td><td>28.67637</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">20241019-173904_zeroshot_128</strong> at: <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora/runs/8x4g22ph' target=\"_blank\">https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora/runs/8x4g22ph</a><br/> View project at: <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora' target=\"_blank\">https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora</a><br/>Synced 4 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20241019_174154-8x4g22ph/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:8x4g22ph). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.18.2"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/user/kth/2430/DD2430_HM_fine-tuning_VLM/wandb/run-20241019_182940-rizy6hn9</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora/runs/rizy6hn9' target=\"_blank\">20241019-173904_zeroshot_256</a></strong> to <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora' target=\"_blank\">https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora/runs/rizy6hn9' target=\"_blank\">https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora/runs/rizy6hn9</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2/2 [00:23<00:00, 11.54s/it]\n","100%|██████████| 42/42 [05:29<00:00,  7.84s/it]\n","100%|██████████| 2/2 [00:21<00:00, 10.65s/it]\n","100%|██████████| 42/42 [05:21<00:00,  7.66s/it]\n","100%|██████████| 2/2 [00:24<00:00, 12.09s/it]\n","100%|██████████| 42/42 [05:03<00:00,  7.22s/it]\n","100%|██████████| 2/2 [00:23<00:00, 11.68s/it]\n","100%|██████████| 42/42 [05:44<00:00,  8.19s/it]\n","100%|██████████| 2/2 [00:24<00:00, 12.32s/it]\n","100%|██████████| 42/42 [05:55<00:00,  8.47s/it]\n","100%|██████████| 2/2 [00:22<00:00, 11.38s/it]\n","100%|██████████| 42/42 [05:39<00:00,  8.09s/it]\n","100%|██████████| 2/2 [00:21<00:00, 10.58s/it]\n","100%|██████████| 42/42 [05:51<00:00,  8.37s/it]\n","100%|██████████| 2/2 [00:24<00:00, 12.43s/it]\n","100%|██████████| 42/42 [05:17<00:00,  7.55s/it]\n","100%|██████████| 2/2 [00:22<00:00, 11.19s/it]\n","100%|██████████| 42/42 [04:54<00:00,  7.00s/it]\n","100%|██████████| 2/2 [00:21<00:00, 10.71s/it]\n","100%|██████████| 42/42 [05:41<00:00,  8.14s/it]\n"]},{"data":{"text/html":["Finishing last run (ID:rizy6hn9) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0f79db663a87405e80bd0622e0976fd7","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy_256_colour_group_name</td><td>▁▁▁▁▇▇▇▆▇█</td></tr><tr><td>test_accuracy_256_department_name</td><td>▁▁▃▃▅██▆▆█</td></tr><tr><td>test_accuracy_256_garment_group_name</td><td>█▁▃███████</td></tr><tr><td>test_accuracy_256_graphical_appearance_name</td><td>█▁██▁█████</td></tr><tr><td>test_accuracy_256_index_group_name</td><td>█▅▅▁▁▅▅▆▆▁</td></tr><tr><td>test_accuracy_256_index_name</td><td>▁█████████</td></tr><tr><td>test_accuracy_256_perceived_colour_master_name</td><td>▅▁▄███████</td></tr><tr><td>test_accuracy_256_perceived_colour_value_name</td><td>▅▁▁▅██████</td></tr><tr><td>test_accuracy_256_product_group_name</td><td>▅▁▃██▆████</td></tr><tr><td>test_accuracy_256_product_type_name</td><td>▁▄▄▆▆█▄▅█▆</td></tr><tr><td>test_accuracy_256_section_name</td><td>▁▂▅▃▄██▇▄▆</td></tr><tr><td>test_loss_256</td><td>▆█▄▃▂▂▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▃▅▆▄▄▅▆▇▆▆█▇███████</td></tr><tr><td>train_loss</td><td>█▅▄▃▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy_256_colour_group_name</td><td>0.30609</td></tr><tr><td>test_accuracy_256_department_name</td><td>0.04252</td></tr><tr><td>test_accuracy_256_garment_group_name</td><td>0.20606</td></tr><tr><td>test_accuracy_256_graphical_appearance_name</td><td>0.45729</td></tr><tr><td>test_accuracy_256_index_group_name</td><td>0.34472</td></tr><tr><td>test_accuracy_256_index_name</td><td>0.23984</td></tr><tr><td>test_accuracy_256_perceived_colour_master_name</td><td>0.32697</td></tr><tr><td>test_accuracy_256_perceived_colour_value_name</td><td>0.54926</td></tr><tr><td>test_accuracy_256_product_group_name</td><td>0.4012</td></tr><tr><td>test_accuracy_256_product_type_name</td><td>0.10877</td></tr><tr><td>test_accuracy_256_section_name</td><td>0.04897</td></tr><tr><td>test_loss_256</td><td>2.63849</td></tr><tr><td>train_accuracy</td><td>0.27983</td></tr><tr><td>train_loss</td><td>27.46965</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">20241019-173904_zeroshot_256</strong> at: <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora/runs/rizy6hn9' target=\"_blank\">https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora/runs/rizy6hn9</a><br/> View project at: <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora' target=\"_blank\">https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20241019_182940-rizy6hn9/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:rizy6hn9). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.18.2"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/user/kth/2430/DD2430_HM_fine-tuning_VLM/wandb/run-20241019_192850-evxe0p9c</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora/runs/evxe0p9c' target=\"_blank\">20241019-173904_zeroshot_512</a></strong> to <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora' target=\"_blank\">https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora/runs/evxe0p9c' target=\"_blank\">https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora/runs/evxe0p9c</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4/4 [00:43<00:00, 10.86s/it]\n","100%|██████████| 42/42 [05:53<00:00,  8.42s/it]\n","100%|██████████| 4/4 [00:41<00:00, 10.40s/it]\n","100%|██████████| 42/42 [05:58<00:00,  8.54s/it]\n","100%|██████████| 4/4 [00:48<00:00, 12.03s/it]\n","100%|██████████| 42/42 [06:30<00:00,  9.30s/it]\n","100%|██████████| 4/4 [00:45<00:00, 11.41s/it]\n","100%|██████████| 42/42 [06:21<00:00,  9.08s/it]\n","100%|██████████| 4/4 [00:47<00:00, 11.91s/it]\n","100%|██████████| 42/42 [05:50<00:00,  8.35s/it]\n","100%|██████████| 4/4 [00:46<00:00, 11.69s/it]\n","100%|██████████| 42/42 [05:56<00:00,  8.50s/it]\n","100%|██████████| 4/4 [00:47<00:00, 11.91s/it]\n","100%|██████████| 42/42 [05:48<00:00,  8.30s/it]\n","100%|██████████| 4/4 [00:46<00:00, 11.74s/it]\n","100%|██████████| 42/42 [05:14<00:00,  7.49s/it]\n","100%|██████████| 4/4 [00:46<00:00, 11.74s/it]\n","100%|██████████| 42/42 [05:42<00:00,  8.15s/it]\n","100%|██████████| 4/4 [00:47<00:00, 11.75s/it]\n","100%|██████████| 42/42 [06:31<00:00,  9.33s/it]\n"]},{"data":{"text/html":["Finishing last run (ID:evxe0p9c) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0f8dcda6e424473ca2bd5daed7dfd4d0","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy_512_colour_group_name</td><td>▁▆▆▅▆▆▅▇▅█</td></tr><tr><td>test_accuracy_512_department_name</td><td>▁▄▃▇▆▇▆▇██</td></tr><tr><td>test_accuracy_512_garment_group_name</td><td>▃▃▃▃▃▃▃▃█▁</td></tr><tr><td>test_accuracy_512_graphical_appearance_name</td><td>▁▁▁▁▁▁▁▁█▅</td></tr><tr><td>test_accuracy_512_index_group_name</td><td>▁▃▁▃▃▄▆▆██</td></tr><tr><td>test_accuracy_512_index_name</td><td>▁▁▁▁▁▁▁█▇█</td></tr><tr><td>test_accuracy_512_perceived_colour_master_name</td><td>▂▁▅▃▅▅▄▅▆█</td></tr><tr><td>test_accuracy_512_perceived_colour_value_name</td><td>▁▇▅▅██▆███</td></tr><tr><td>test_accuracy_512_product_group_name</td><td>▁█████████</td></tr><tr><td>test_accuracy_512_product_type_name</td><td>▁▁▂▂▁▄▅██▆</td></tr><tr><td>test_accuracy_512_section_name</td><td>▁▄▁▄▄▄▇▄▆█</td></tr><tr><td>test_loss_512</td><td>█▅▃▃▂▂▂▂▁▁</td></tr><tr><td>train_accuracy</td><td>▁▃▅▄▅▆▅▆▆▇▇▇▆▆▇▆▆▇▇▇▇▇▇▆▇▇▇▇▇▇█▇▇▇▇█▇███</td></tr><tr><td>train_loss</td><td>█▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy_512_colour_group_name</td><td>0.3265</td></tr><tr><td>test_accuracy_512_department_name</td><td>0.03872</td></tr><tr><td>test_accuracy_512_garment_group_name</td><td>0.20644</td></tr><tr><td>test_accuracy_512_graphical_appearance_name</td><td>0.47172</td></tr><tr><td>test_accuracy_512_index_group_name</td><td>0.43821</td></tr><tr><td>test_accuracy_512_index_name</td><td>0.25408</td></tr><tr><td>test_accuracy_512_perceived_colour_master_name</td><td>0.40689</td></tr><tr><td>test_accuracy_512_perceived_colour_value_name</td><td>0.56634</td></tr><tr><td>test_accuracy_512_product_group_name</td><td>0.40414</td></tr><tr><td>test_accuracy_512_product_type_name</td><td>0.11133</td></tr><tr><td>test_accuracy_512_section_name</td><td>0.07536</td></tr><tr><td>test_loss_512</td><td>2.54663</td></tr><tr><td>train_accuracy</td><td>0.33097</td></tr><tr><td>train_loss</td><td>26.42476</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">20241019-173904_zeroshot_512</strong> at: <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora/runs/evxe0p9c' target=\"_blank\">https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora/runs/evxe0p9c</a><br/> View project at: <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora' target=\"_blank\">https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20241019_192850-evxe0p9c/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:evxe0p9c). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.18.2"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/user/kth/2430/DD2430_HM_fine-tuning_VLM/wandb/run-20241019_203641-8u4w2e27</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora/runs/8u4w2e27' target=\"_blank\">20241019-173904_zeroshot_1024</a></strong> to <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora' target=\"_blank\">https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora/runs/8u4w2e27' target=\"_blank\">https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora/runs/8u4w2e27</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["100%|██████████| 8/8 [01:49<00:00, 13.68s/it]\n","100%|██████████| 42/42 [06:38<00:00,  9.49s/it]\n","100%|██████████| 8/8 [01:52<00:00, 14.06s/it]\n","100%|██████████| 42/42 [06:43<00:00,  9.61s/it]\n","100%|██████████| 8/8 [01:56<00:00, 14.53s/it]\n","100%|██████████| 42/42 [06:36<00:00,  9.43s/it]\n","100%|██████████| 8/8 [01:50<00:00, 13.80s/it]\n","100%|██████████| 42/42 [06:13<00:00,  8.89s/it]\n","100%|██████████| 8/8 [01:59<00:00, 14.95s/it]\n","100%|██████████| 42/42 [06:17<00:00,  8.99s/it]\n","100%|██████████| 8/8 [01:51<00:00, 13.95s/it]\n","100%|██████████| 42/42 [05:59<00:00,  8.56s/it]\n","100%|██████████| 8/8 [01:51<00:00, 13.97s/it]\n","100%|██████████| 42/42 [06:00<00:00,  8.58s/it]\n","100%|██████████| 8/8 [01:53<00:00, 14.14s/it]\n","100%|██████████| 42/42 [06:16<00:00,  8.96s/it]\n","100%|██████████| 8/8 [01:52<00:00, 14.12s/it]\n","100%|██████████| 42/42 [05:36<00:00,  8.00s/it]\n","100%|██████████| 8/8 [01:52<00:00, 14.07s/it]\n","100%|██████████| 42/42 [05:32<00:00,  7.92s/it]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4f3fa0b73dec4640895e9d2597ca35e2","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy_1024_colour_group_name</td><td>▁▂▄▄▄▄▅▅▇█</td></tr><tr><td>test_accuracy_1024_department_name</td><td>▁▇▇▄▄▆▆▆▅█</td></tr><tr><td>test_accuracy_1024_garment_group_name</td><td>▂▁▆▃▄▆▅▆▅█</td></tr><tr><td>test_accuracy_1024_graphical_appearance_name</td><td>▄▄▄▄▄▄▁▅▃█</td></tr><tr><td>test_accuracy_1024_index_group_name</td><td>▁▁▁▁▁▃▂▄▇█</td></tr><tr><td>test_accuracy_1024_index_name</td><td>▁▁▁▁▁▃▂▄▆█</td></tr><tr><td>test_accuracy_1024_perceived_colour_master_name</td><td>▁▃▂▃▃▃▅▆▅█</td></tr><tr><td>test_accuracy_1024_perceived_colour_value_name</td><td>▄▁▅▄▄▅▅▄▆█</td></tr><tr><td>test_accuracy_1024_product_group_name</td><td>▃▃▃▃▃▃▃▄▁█</td></tr><tr><td>test_accuracy_1024_product_type_name</td><td>▁▃▅▄▅▅▆▆▅█</td></tr><tr><td>test_accuracy_1024_section_name</td><td>▁▁▁▃▃▂▃▅▄█</td></tr><tr><td>test_loss_1024</td><td>█▆▅▅▅▄▄▃▃▁</td></tr><tr><td>train_accuracy</td><td>▁▄▃▄▄▆▆▆▅▆▆▇▅▆▇▇▆▆▆▇▇▆▇▇▆▇▇▆▆▇▇▆▇█▇████▇</td></tr><tr><td>train_loss</td><td>█▅▅▅▅▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy_1024_colour_group_name</td><td>0.3899</td></tr><tr><td>test_accuracy_1024_department_name</td><td>0.05011</td></tr><tr><td>test_accuracy_1024_garment_group_name</td><td>0.22656</td></tr><tr><td>test_accuracy_1024_graphical_appearance_name</td><td>0.47893</td></tr><tr><td>test_accuracy_1024_index_group_name</td><td>0.45995</td></tr><tr><td>test_accuracy_1024_index_name</td><td>0.28436</td></tr><tr><td>test_accuracy_1024_perceived_colour_master_name</td><td>0.46336</td></tr><tr><td>test_accuracy_1024_perceived_colour_value_name</td><td>0.60232</td></tr><tr><td>test_accuracy_1024_product_group_name</td><td>0.43745</td></tr><tr><td>test_accuracy_1024_product_type_name</td><td>0.14522</td></tr><tr><td>test_accuracy_1024_section_name</td><td>0.10374</td></tr><tr><td>test_loss_1024</td><td>2.37</td></tr><tr><td>train_accuracy</td><td>0.31392</td></tr><tr><td>train_loss</td><td>25.80361</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">20241019-173904_zeroshot_1024</strong> at: <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora/runs/8u4w2e27' target=\"_blank\">https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora/runs/8u4w2e27</a><br/> View project at: <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora' target=\"_blank\">https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20241019_203641-8u4w2e27/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# 定义不同数量的训练图像\n","train_image_counts = [128,256,512,1024]\n","num_epochs = 10\n","\n","for train_image_count in train_image_counts:\n","    wandb.init(project=\"clip-lora\", name=f\"{date}_zeroshot_{train_image_count}\")\n","    step = 0\n","\n","    # 创建子数据集\n","    subset_indices = torch.randperm(len(train_dataset))[:train_image_count]\n","    subset_dataset = torch.utils.data.Subset(train_dataset, subset_indices)\n","    subset_dataloader = torch.utils.data.DataLoader(subset_dataset, batch_size=128, shuffle=True)\n","\n","    # 初始化模型和优化器\n","    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", cache_dir=\"model\", local_files_only=False)\n","    model = MultiOutputClipModel(clip_model, selected_class_names, 768, 128, 512).to(device)\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n","    criteria = MultiOutputClipCriterion(selected_class_names)\n","\n","    # 训练模型\n","    for epoch in range(num_epochs):\n","        model.train()\n","        total_loss = 0.0\n","        total_correct = 0\n","        total_samples = 0\n","\n","        for images, image_ids in tqdm(subset_dataloader):\n","            images = images.to(device)\n","            logits_per_image_dict = model(pixel_values=images, text_input_dict=text_input_dict)\n","\n","            # 获取真实标签\n","            true_labels_dict = {\n","                class_name: [label_names_to_idx[class_name][articles.loc[article_id_to_idx[image_id.item()], class_name]] \n","                             for image_id in image_ids]\n","                for class_name in selected_class_names\n","            }\n","            true_labels_dict = {class_name: torch.tensor(true_labels).to(device) \n","                                for class_name, true_labels in true_labels_dict.items()}\n","\n","            # 计算损失\n","            loss = criteria(logits_per_image_dict, true_labels_dict)\n","            total_loss += loss.item() * images.size(0)\n","\n","            # 计算准确率\n","            correct = 0\n","            total_samples += images.size(0)\n","            for class_name in selected_class_names:\n","                _, preds = torch.max(logits_per_image_dict[class_name], dim=1)\n","                correct += (preds == true_labels_dict[class_name]).sum().item()\n","            total_correct += correct\n","\n","            # 反向传播和优化\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            # 记录训练损失和准确率到 wandb\n","            log_dict = {\n","                \"train_loss\": loss.item(),\n","                \"train_accuracy\": correct / images.size(0) / len(selected_class_names)\n","            }\n","            wandb.log(log_dict, step=step)\n","            step += 1\n","\n","        # 验证模型\n","        test_loss, test_accuracy_dict = validate(model, test_dataloader, criteria, device, text_input_dict, selected_class_names)\n","        wandb.log({f\"test_loss_{train_image_count}\": test_loss, **{f\"test_accuracy_{train_image_count}_{class_name}\": acc for class_name, acc in test_accuracy_dict.items()}})\n","        torch.save(model.state_dict(), f\"model/{date}_{train_image_count}_{epoch}.pth\")\n","\n","wandb.finish()\n"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 42/42 [06:22<00:00,  9.10s/it]"]},{"name":"stdout","output_type":"stream","text":["3.6852146700860846\n","{'product_group_name': 0.012908124525436599, 'product_type_name': 0.0021829916476841307, 'graphical_appearance_name': 0.024677296886864084, 'colour_group_name': 0.01879271070615034, 'perceived_colour_value_name': 0.055998481397114656, 'perceived_colour_master_name': 0.03597190584662111, 'department_name': 0.004176157934700076, 'index_name': 0.07137433561123765, 'index_group_name': 0.2737281700835232, 'section_name': 0.01224373576309795, 'garment_group_name': 0.05258162490508732}\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# 初始化模型和优化器\n","clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", cache_dir=\"model\", local_files_only=False)\n","model = MultiOutputClipModel(clip_model, selected_class_names, 768, 128, 512).to(device)\n","\n","criteria = MultiOutputClipCriterion(selected_class_names)\n","test_loss, test_accuracy_dict = validate(model, test_dataloader, criteria, device, text_input_dict, selected_class_names)\n","print(test_loss)\n","print(test_accuracy_dict)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"text/html":["Tracking run with wandb version 0.18.2"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/user/kth/2430/DD2430_HM_fine-tuning_VLM/wandb/run-20241019_225652-ugedhjdb</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora/runs/ugedhjdb' target=\"_blank\">20241019-173904_zeroshot_32</a></strong> to <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora' target=\"_blank\">https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora/runs/ugedhjdb' target=\"_blank\">https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora/runs/ugedhjdb</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1/1 [00:06<00:00,  6.53s/it]\n","100%|██████████| 42/42 [04:02<00:00,  5.78s/it]\n","100%|██████████| 1/1 [00:05<00:00,  5.49s/it]\n","100%|██████████| 42/42 [04:20<00:00,  6.21s/it]\n","100%|██████████| 1/1 [00:06<00:00,  6.07s/it]\n","100%|██████████| 42/42 [04:41<00:00,  6.71s/it]\n","100%|██████████| 1/1 [00:06<00:00,  6.59s/it]\n","100%|██████████| 42/42 [04:26<00:00,  6.35s/it]\n","100%|██████████| 1/1 [00:06<00:00,  6.97s/it]\n","100%|██████████| 42/42 [04:32<00:00,  6.48s/it]\n","100%|██████████| 1/1 [00:07<00:00,  7.35s/it]\n","100%|██████████| 42/42 [04:26<00:00,  6.34s/it]\n","100%|██████████| 1/1 [00:06<00:00,  6.60s/it]\n","100%|██████████| 42/42 [04:25<00:00,  6.31s/it]\n","100%|██████████| 1/1 [00:06<00:00,  6.71s/it]\n","100%|██████████| 42/42 [04:13<00:00,  6.04s/it]\n","100%|██████████| 1/1 [00:06<00:00,  6.66s/it]\n","100%|██████████| 42/42 [04:08<00:00,  5.91s/it]\n","100%|██████████| 1/1 [00:06<00:00,  6.90s/it]\n","100%|██████████| 42/42 [04:05<00:00,  5.85s/it]\n"]},{"data":{"text/html":["Finishing last run (ID:ugedhjdb) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"849dde48d9c249798fe9c75e26926987","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy_32_colour_group_name</td><td>▁▄▄▄▄████▄</td></tr><tr><td>test_accuracy_32_department_name</td><td>▄▇▇▂█▃▃▇▂▁</td></tr><tr><td>test_accuracy_32_garment_group_name</td><td>▁▂▂▁▆▆████</td></tr><tr><td>test_accuracy_32_graphical_appearance_name</td><td>▁▄▆██▄████</td></tr><tr><td>test_accuracy_32_index_group_name</td><td>▁▂▂▃▂▂▂▂▂█</td></tr><tr><td>test_accuracy_32_index_name</td><td>▁▃▆▄▆▆▅▄▄█</td></tr><tr><td>test_accuracy_32_perceived_colour_master_name</td><td>▁▆▆▆▆▇▇▇█▆</td></tr><tr><td>test_accuracy_32_perceived_colour_value_name</td><td>▁▅▆▆▆▆▆▆██</td></tr><tr><td>test_accuracy_32_product_group_name</td><td>▁▁▁▁██████</td></tr><tr><td>test_accuracy_32_product_type_name</td><td>▅█▆█▁▁▂▂▂▄</td></tr><tr><td>test_accuracy_32_section_name</td><td>▁▂▂▂▆▆▄▃██</td></tr><tr><td>test_loss_32</td><td>█▆▄▂▄▁▁▂▄▂</td></tr><tr><td>train_accuracy</td><td>▁▂▆▇▇▇▆▇██</td></tr><tr><td>train_loss</td><td>█▅▄▃▃▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy_32_colour_group_name</td><td>0.13032</td></tr><tr><td>test_accuracy_32_department_name</td><td>0.00275</td></tr><tr><td>test_accuracy_32_garment_group_name</td><td>0.20653</td></tr><tr><td>test_accuracy_32_graphical_appearance_name</td><td>0.45729</td></tr><tr><td>test_accuracy_32_index_group_name</td><td>0.37149</td></tr><tr><td>test_accuracy_32_index_name</td><td>0.19286</td></tr><tr><td>test_accuracy_32_perceived_colour_master_name</td><td>0.17673</td></tr><tr><td>test_accuracy_32_perceived_colour_value_name</td><td>0.55173</td></tr><tr><td>test_accuracy_32_product_group_name</td><td>0.4012</td></tr><tr><td>test_accuracy_32_product_type_name</td><td>0.06435</td></tr><tr><td>test_accuracy_32_section_name</td><td>0.0486</td></tr><tr><td>test_loss_32</td><td>3.0083</td></tr><tr><td>train_accuracy</td><td>0.28977</td></tr><tr><td>train_loss</td><td>27.11527</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">20241019-173904_zeroshot_32</strong> at: <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora/runs/ugedhjdb' target=\"_blank\">https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora/runs/ugedhjdb</a><br/> View project at: <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora' target=\"_blank\">https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20241019_225652-ugedhjdb/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:ugedhjdb). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.18.2"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/user/kth/2430/DD2430_HM_fine-tuning_VLM/wandb/run-20241019_234135-xsn857c9</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora/runs/xsn857c9' target=\"_blank\">20241019-173904_zeroshot_64</a></strong> to <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora' target=\"_blank\">https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora/runs/xsn857c9' target=\"_blank\">https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora/runs/xsn857c9</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2/2 [00:15<00:00,  7.67s/it]\n","100%|██████████| 42/42 [04:32<00:00,  6.50s/it]\n","100%|██████████| 2/2 [00:11<00:00,  5.99s/it]\n","100%|██████████| 42/42 [04:06<00:00,  5.86s/it]\n","100%|██████████| 2/2 [00:12<00:00,  6.48s/it]\n","100%|██████████| 42/42 [04:04<00:00,  5.83s/it]\n","100%|██████████| 2/2 [00:11<00:00,  5.98s/it]\n","100%|██████████| 42/42 [04:57<00:00,  7.07s/it]\n","100%|██████████| 2/2 [00:15<00:00,  7.83s/it]\n","100%|██████████| 42/42 [04:54<00:00,  7.01s/it]\n","100%|██████████| 2/2 [00:14<00:00,  7.36s/it]\n","100%|██████████| 42/42 [04:49<00:00,  6.89s/it]\n","100%|██████████| 2/2 [00:16<00:00,  8.11s/it]\n","100%|██████████| 42/42 [04:23<00:00,  6.27s/it]\n","100%|██████████| 2/2 [00:13<00:00,  6.95s/it]\n","100%|██████████| 42/42 [04:18<00:00,  6.17s/it]\n","100%|██████████| 2/2 [00:12<00:00,  6.42s/it]\n","100%|██████████| 42/42 [04:06<00:00,  5.88s/it]\n","100%|██████████| 2/2 [00:12<00:00,  6.18s/it]\n","100%|██████████| 42/42 [04:10<00:00,  5.97s/it]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"88c6235b7c5a415e97c27b9b1bb041b8","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy_64_colour_group_name</td><td>▁▁▁▁█▁▃▄█▇</td></tr><tr><td>test_accuracy_64_department_name</td><td>▁▁▃▃▅█▇▂▇▇</td></tr><tr><td>test_accuracy_64_garment_group_name</td><td>▁█████▂▆██</td></tr><tr><td>test_accuracy_64_graphical_appearance_name</td><td>█▁▁███████</td></tr><tr><td>test_accuracy_64_index_group_name</td><td>▇▇▃▁█▇▇▅▆█</td></tr><tr><td>test_accuracy_64_index_name</td><td>▁▄▄▄▆▃▆▆▆█</td></tr><tr><td>test_accuracy_64_perceived_colour_master_name</td><td>▃▃▃▁▅▃▆▆▆█</td></tr><tr><td>test_accuracy_64_perceived_colour_value_name</td><td>▁▄▄▅█████▄</td></tr><tr><td>test_accuracy_64_product_group_name</td><td>▁█████████</td></tr><tr><td>test_accuracy_64_product_type_name</td><td>████▅███▁▄</td></tr><tr><td>test_accuracy_64_section_name</td><td>█████▃▃▃▃▁</td></tr><tr><td>test_loss_64</td><td>█▇▅▃▂▁▁▁▄▂</td></tr><tr><td>train_accuracy</td><td>▁▃▅▅▅▆▆▅▆▆▇▆▇▇█▇▇██▇</td></tr><tr><td>train_loss</td><td>█▅▅▇▄▄▄▃▃▃▃▂▂▂▂▂▁▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy_64_colour_group_name</td><td>0.27933</td></tr><tr><td>test_accuracy_64_department_name</td><td>0.02658</td></tr><tr><td>test_accuracy_64_garment_group_name</td><td>0.20653</td></tr><tr><td>test_accuracy_64_graphical_appearance_name</td><td>0.45757</td></tr><tr><td>test_accuracy_64_index_group_name</td><td>0.36836</td></tr><tr><td>test_accuracy_64_index_name</td><td>0.23168</td></tr><tr><td>test_accuracy_64_perceived_colour_master_name</td><td>0.30704</td></tr><tr><td>test_accuracy_64_perceived_colour_value_name</td><td>0.39806</td></tr><tr><td>test_accuracy_64_product_group_name</td><td>0.4012</td></tr><tr><td>test_accuracy_64_product_type_name</td><td>0.08685</td></tr><tr><td>test_accuracy_64_section_name</td><td>0.06758</td></tr><tr><td>test_loss_64</td><td>2.86914</td></tr><tr><td>train_accuracy</td><td>0.27841</td></tr><tr><td>train_loss</td><td>26.94374</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">20241019-173904_zeroshot_64</strong> at: <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora/runs/xsn857c9' target=\"_blank\">https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora/runs/xsn857c9</a><br/> View project at: <a href='https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora' target=\"_blank\">https://wandb.ai/onjackay-kth-royal-institute-of-technology/clip-lora</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20241019_234135-xsn857c9/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# 定义不同数量的训练图像\n","train_image_counts = [32, 64]\n","num_epochs = 10\n","\n","for train_image_count in train_image_counts:\n","    wandb.init(project=\"clip-lora\", name=f\"{date}_zeroshot_{train_image_count}\")\n","    step = 0\n","\n","    # 创建子数据集\n","    subset_indices = torch.randperm(len(train_dataset))[:train_image_count]\n","    subset_dataset = torch.utils.data.Subset(train_dataset, subset_indices)\n","    subset_dataloader = torch.utils.data.DataLoader(subset_dataset, batch_size=32, shuffle=True)\n","\n","    # 初始化模型和优化器\n","    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", cache_dir=\"model\", local_files_only=False)\n","    model = MultiOutputClipModel(clip_model, selected_class_names, 768, 128, 512).to(device)\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n","    criteria = MultiOutputClipCriterion(selected_class_names)\n","\n","    # 训练模型\n","    for epoch in range(num_epochs):\n","        model.train()\n","        total_loss = 0.0\n","        total_correct = 0\n","        total_samples = 0\n","\n","        for images, image_ids in tqdm(subset_dataloader):\n","            images = images.to(device)\n","            logits_per_image_dict = model(pixel_values=images, text_input_dict=text_input_dict)\n","\n","            # 获取真实标签\n","            true_labels_dict = {\n","                class_name: [label_names_to_idx[class_name][articles.loc[article_id_to_idx[image_id.item()], class_name]] \n","                             for image_id in image_ids]\n","                for class_name in selected_class_names\n","            }\n","            true_labels_dict = {class_name: torch.tensor(true_labels).to(device) \n","                                for class_name, true_labels in true_labels_dict.items()}\n","\n","            # 计算损失\n","            loss = criteria(logits_per_image_dict, true_labels_dict)\n","            total_loss += loss.item() * images.size(0)\n","\n","            # 计算准确率\n","            correct = 0\n","            total_samples += images.size(0)\n","            for class_name in selected_class_names:\n","                _, preds = torch.max(logits_per_image_dict[class_name], dim=1)\n","                correct += (preds == true_labels_dict[class_name]).sum().item()\n","            total_correct += correct\n","\n","            # 反向传播和优化\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            # 记录训练损失和准确率到 wandb\n","            log_dict = {\n","                \"train_loss\": loss.item(),\n","                \"train_accuracy\": correct / images.size(0) / len(selected_class_names)\n","            }\n","            wandb.log(log_dict, step=step)\n","            step += 1\n","\n","        # 验证模型\n","        test_loss, test_accuracy_dict = validate(model, test_dataloader, criteria, device, text_input_dict, selected_class_names)\n","        wandb.log({f\"test_loss_{train_image_count}\": test_loss, **{f\"test_accuracy_{train_image_count}_{class_name}\": acc for class_name, acc in test_accuracy_dict.items()}})\n","        torch.save(model.state_dict(), f\"model/{date}_{train_image_count}_{epoch}.pth\")\n","\n","wandb.finish()\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":3103714,"sourceId":31254,"sourceType":"competition"},{"sourceId":199081181,"sourceType":"kernelVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"hf","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"}},"nbformat":4,"nbformat_minor":4}
