{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "#ab8d0971276394e3d1d6dda698f2272810be2374  \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"ViT-B/32\"\n",
    "model, preprocess = clip.load(model_name, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP(\n",
      "  (visual): VisionTransformer(\n",
      "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
      "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (transformer): Transformer(\n",
      "      (resblocks): Sequential(\n",
      "        (0): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (6): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (7): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (8): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (9): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (10): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (11): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (resblocks): Sequential(\n",
      "      (0): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (6): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (7): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (8): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (9): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (10): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (11): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (token_embedding): Embedding(49408, 512)\n",
      "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# 确认正确的路径\n",
    "dataset_path = Path(\"/media/liulianhang/C20EE8FD0EE8EB7D/h-and-m-personalized-fashion-recommendations\")\n",
    "\n",
    "# 读取数据\n",
    "articles = pd.read_csv(dataset_path / \"articles.csv\")\n",
    "customers = pd.read_csv(dataset_path / \"customers.csv\")\n",
    "transactions = pd.read_csv(dataset_path / \"transactions_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, original_linear, r=4, lora_alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.in_features = original_linear.in_features\n",
    "        self.out_features = original_linear.out_features\n",
    "        self.r = r\n",
    "        self.lora_alpha = lora_alpha\n",
    "\n",
    "        # Original weights and biases\n",
    "        self.weight = original_linear.weight\n",
    "        self.bias = original_linear.bias\n",
    "\n",
    "        # Freeze original weights and biases\n",
    "        self.weight.requires_grad = False\n",
    "        if self.bias is not None:\n",
    "            self.bias.requires_grad = False\n",
    "\n",
    "        # LoRA parameters\n",
    "        self.lora_A = nn.Parameter(torch.zeros((r, self.in_features)))\n",
    "        self.lora_B = nn.Parameter(torch.zeros((self.out_features, r)))\n",
    "\n",
    "        # Initialize LoRA parameters\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "\n",
    "        # Scaling factor\n",
    "        self.scaling = self.lora_alpha / self.r\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Original output\n",
    "        result = nn.functional.linear(x, self.weight, self.bias)\n",
    "        # LoRA update\n",
    "        lora_update = (x @ self.lora_A.T) @ self.lora_B.T * self.scaling\n",
    "        return result + lora_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def replace_linear_with_lora(module, r=4, lora_alpha=1.0):\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, nn.Linear):\n",
    "            setattr(module, name, LoRALinear(child, r=r, lora_alpha=lora_alpha))\n",
    "        else:\n",
    "            replace_linear_with_lora(child, r=r, lora_alpha=lora_alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_linear_with_lora(model.transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设您已经有 articles DataFrame 和 image_path_pool\n",
    "# 我们需要创建一个数据集和数据加载器\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, preprocess):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.preprocess(Image.open(self.image_paths[idx])).to(device)\n",
    "        label = self.labels[idx]\n",
    "        return image, label\n",
    "# #Loop in all the images folders, choose the first three images in each folder, Make separate predictions for all the classes, using loop,and calculate the accuracy\n",
    "# image_folder_root = \"/media/liulianhang/C20EE8FD0EE8EB7D/h-and-m-personalized-fashion-recommendations/images\"\n",
    "# image_folder_3num = os.listdir(image_folder_root)\n",
    "# image_path_pool = []\n",
    "# #in each image folder, choose the first three images\n",
    "# for image_folder in image_folder_3num:\n",
    "#     image_folder_path = os.path.join(image_folder_root, image_folder)\n",
    "#     image_folder_images = os.listdir(image_folder_path)\n",
    "#     image_folder_images = image_folder_images[:2]\n",
    "#     for image_name in image_folder_images:\n",
    "#         image_path = os.path.join(image_folder_path, image_name)\n",
    "#         image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "#         image_path_pool.append(image_path)\n",
    "\n",
    "# # 准备 image_paths 和 labels\n",
    "# image_paths = image_path_pool  # 您之前收集的图像路径列表\n",
    "# labels = []  # 对应的标签列表，需要根据 image_paths 生成\n",
    "product_group_name = articles['product_group_name'].unique()\n",
    "# for image_path in image_paths:\n",
    "#     article_id = int(image_path.split('/')[-1].split('.')[0][1:])\n",
    "#     label = articles[articles['article_id'] == article_id]['product_group_name'].values[0]\n",
    "#     labels.append(label)\n",
    "\n",
    "# dataset = CustomDataset(image_paths, labels, preprocess)\n",
    "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if 'lora' in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"{name}: requires_grad={param.requires_grad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取所有图像路径和对应的标签\n",
    "def load_image_paths_and_labels(image_folder, articles_df):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for subdir, dirs, files in os.walk(image_folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.jpg'):\n",
    "                # 获取图像路径\n",
    "                image_path = os.path.join(subdir, file)\n",
    "                image_paths.append(image_path)\n",
    "                \n",
    "                # 从文件名提取 article_id\n",
    "                article_id = int(file.split('.')[0][1:])  # 假设文件名是 \"0108775015.jpg\"\n",
    "                # 获取标签\n",
    "                label = articles_df[articles_df['article_id'] == article_id]['product_group_name'].values[0]\n",
    "                labels.append(label)\n",
    "    return image_paths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# 加载数据\n",
    "image_folder = '/media/liulianhang/C20EE8FD0EE8EB7D/h-and-m-personalized-fashion-recommendations/images'\n",
    "image_paths, labels = load_image_paths_and_labels(image_folder, articles)\n",
    "\n",
    "# 划分训练集、验证集和测试集\n",
    "train_image_paths, temp_image_paths, train_labels, temp_labels = train_test_split(\n",
    "    image_paths, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "val_image_paths, test_image_paths, val_labels, test_labels = train_test_split(\n",
    "    temp_image_paths, temp_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "train_dataset = CustomDataset(train_image_paths, train_labels, preprocess)\n",
    "val_dataset = CustomDataset(val_image_paths, val_labels, preprocess)\n",
    "test_dataset = CustomDataset(test_image_paths, test_labels, preprocess)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练和验证函数\n",
    "import wandb\n",
    "def train_one_epoch(model, train_loader, optimizer, criterion, device,text_tokens, epoch):\n",
    "    total_loss = 0\n",
    "    total_batches = len(train_loader)\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        label_indices = torch.tensor([list(product_group_name).index(label) for label in labels]).to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        image_features = model.encode_image(images)\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        \n",
    "        text_features = model.encode_text(text_tokens)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # 计算相似度\n",
    "        logits_per_image = image_features @ text_features.T\n",
    "\n",
    "        # 计算损失\n",
    "        loss = criterion(logits_per_image, label_indices)\n",
    "\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        print('loss:',loss.item())\n",
    "        #want to use wandb to log the loss\n",
    "        wandb.log({'train_everystep_loss': loss.item()}, step=epoch * total_batches + batch_idx)\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    return avg_loss\n",
    "\n",
    "def validate(model, val_loader, criterion, device,text_tokens):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            label_indices = torch.tensor([list(product_group_name).index(label) for label in labels]).to(device)\n",
    "\n",
    "            # 前向传播\n",
    "            image_features = model.encode_image(images)\n",
    "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            \n",
    "            text_features = model.encode_text(text_tokens)\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            # 计算相似度\n",
    "            logits_per_image = image_features @ text_features.T\n",
    "\n",
    "            # 计算损失\n",
    "            loss = criterion(logits_per_image, label_indices)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            #wandb.log({'val_loss': loss.item()}, step=epoch * total_batches + batch_idx)\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# label_indices = torch.tensor([list(product_group_name).index(label) for label in labels]).to(device)\n",
    "# for images, labels in train_loader:\n",
    "#         images = images.to(device)\n",
    "#         label_indices = torch.tensor([list(product_group_name).index(label) for label in labels]).to(device)\n",
    "#         print(len(label_indices))\n",
    "\n",
    "#         # 前向传播\n",
    "#         image_features = model.encode_image(images)\n",
    "#         print(image_features.shape)\n",
    "#         image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "#         print(image_features.shape)\n",
    "\n",
    "#         text_inputs = clip.tokenize([f\"a photo of a {label}\" for label in labels]).to(device)\n",
    "#         print(text_inputs.shape)\n",
    "#         text_features = model.encode_text(text_inputs)\n",
    "#         print(text_features.shape)\n",
    "#         text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "#         print(text_features.shape)\n",
    "\n",
    "#         # 计算相似度\n",
    "#         logits_per_image = image_features @ text_features.T\n",
    "#         print(logits_per_image.shape )\n",
    "#         print('finish')\n",
    "\n",
    "#         # 计算损失\n",
    "#         loss = criterion(logits_per_image, label_indices)\n",
    "# 准备所有可能的标签文本\n",
    "# label_texts = [f\"a photo of a {label}\" for label in product_group_name]\n",
    "# text_tokens = clip.tokenize(label_texts).to(device)\n",
    "# print(text_tokens.shape)\n",
    "# for images, labels in train_loader:\n",
    "#         print(len(train_loader))\n",
    "#         images = images.to(device)\n",
    "#         # 获取对应标签在 product_group_name 中的索引\n",
    "#         label_indices = torch.tensor([list(product_group_name).index(label) for label in labels]).to(device)\n",
    "#         print(len(label_indices))\n",
    "\n",
    "#         # 前向传播 - 图像特征\n",
    "#         image_features = model.encode_image(images)\n",
    "#         image_features = image_features / image_features.norm(dim=-1, keepdim=True)  # 避免就地操作\n",
    "#         print(image_features.shape)\n",
    "\n",
    "#         # 前向传播 - 文本特征\n",
    "#         # 在每次训练中计算文本特征，这样它们的计算图不会被detach\n",
    "#         text_features = model.encode_text(text_tokens)\n",
    "#         text_features = text_features / text_features.norm(dim=-1, keepdim=True)  # 避免就地操作\n",
    "#         print(text_features.shape)\n",
    "#         # 计算相似度\n",
    "#         logits_per_image = image_features @ text_features.T\n",
    "#         print(logits_per_image.shape)\n",
    "#         # 计算损失\n",
    "#         loss = criterion(logits_per_image, label_indices)\n",
    "#         print(loss)\n",
    "#         print(f\"loss: {loss}, requires_grad: {loss.requires_grad}, grad_fn: {loss.grad_fn}\")\n",
    "\n",
    "#         # 反向传播和优化\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mliulianhang\u001b[0m (\u001b[33mliulianhang-kth-royal-institute-of-technology\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/liulianhang/DD2430_HM_fine-tuning_VLM/wandb/run-20240918_094150-tcm54eki</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/liulianhang-kth-royal-institute-of-technology/clip_training/runs/tcm54eki' target=\"_blank\">CLIP_Model_Run</a></strong> to <a href='https://wandb.ai/liulianhang-kth-royal-institute-of-technology/clip_training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/liulianhang-kth-royal-institute-of-technology/clip_training' target=\"_blank\">https://wandb.ai/liulianhang-kth-royal-institute-of-technology/clip_training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/liulianhang-kth-royal-institute-of-technology/clip_training/runs/tcm54eki' target=\"_blank\">https://wandb.ai/liulianhang-kth-royal-institute-of-technology/clip_training/runs/tcm54eki</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25966/2342775938.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_clip_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.0810, Test Accuracy: 45.62%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>45.625</td></tr><tr><td>test_loss</td><td>2.08102</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">CLIP_Model_Run</strong> at: <a href='https://wandb.ai/liulianhang-kth-royal-institute-of-technology/clip_training/runs/tcm54eki' target=\"_blank\">https://wandb.ai/liulianhang-kth-royal-institute-of-technology/clip_training/runs/tcm54eki</a><br/> View project at: <a href='https://wandb.ai/liulianhang-kth-royal-institute-of-technology/clip_training' target=\"_blank\">https://wandb.ai/liulianhang-kth-royal-institute-of-technology/clip_training</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240918_094150-tcm54eki/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project='clip_training', name='CLIP_Model_Run')\n",
    "num_epochs = 50\n",
    "best_val_loss = float('inf')\n",
    "label_texts = [f\"a photo of a {label}\" for label in product_group_name]\n",
    "text_tokens = clip.tokenize(label_texts).to(device)\n",
    "# for epoch in range(num_epochs):\n",
    "#     train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device,text_tokens, epoch)\n",
    "#     val_loss = validate(model, val_loader, criterion, device,text_tokens)\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "#     # 使用 WandB 记录训练和验证损失\n",
    "#     wandb.log({\n",
    "#         'epoch': epoch + 1,\n",
    "#         'train_loss': train_loss,\n",
    "#         'val_loss': val_loss\n",
    "#     })\n",
    "#     # 保存最佳模型\n",
    "#     if val_loss < best_val_loss:\n",
    "#         best_val_loss = val_loss\n",
    "#         torch.save(model.state_dict(), 'best_clip_model.pth')\n",
    "#         print(\"Saved Best Model\")\n",
    "#         wandb.save('best_clip_model.pth')\n",
    "\n",
    "# 测试模型性能\n",
    "def test(model, test_loader, criterion, device,text_tokens):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        #want the loop is ten times not the whole test_loader\n",
    "\n",
    "        for i, (images, labels) in enumerate(test_loader):\n",
    "            if i >= 10:\n",
    "                break  # 退出循环\n",
    "            images = images.to(device)\n",
    "            label_indices = torch.tensor([list(product_group_name).index(label) for label in labels]).to(device)\n",
    "\n",
    "            # 前向传播\n",
    "            image_features = model.encode_image(images)\n",
    "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            #text_inputs = clip.tokenize([f\"a photo of a {label}\" for label in labels]).to(device)\n",
    "            text_features = model.encode_text(text_tokens)\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            # 计算相似度\n",
    "            logits_per_image = image_features @ text_features.T\n",
    "\n",
    "            # 计算损失\n",
    "            loss = criterion(logits_per_image, label_indices)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = logits_per_image.max(1)\n",
    "            total += label_indices.size(0)\n",
    "            correct += (predicted == label_indices).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / 10\n",
    "    accuracy = 100.0 * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# 加载最佳模型并测试\n",
    "model.load_state_dict(torch.load('best_clip_model.pth'))\n",
    "test_loss, test_accuracy = test(model, test_loader, criterion, device,text_tokens)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "# 记录测试结果到 WandB\n",
    "wandb.log({\n",
    "    'test_loss': test_loss,\n",
    "    'test_accuracy': test_accuracy\n",
    "})\n",
    "\n",
    "# 结束 WandB 运行\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
