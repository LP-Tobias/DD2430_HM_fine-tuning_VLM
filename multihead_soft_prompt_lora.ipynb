{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x111d24390>"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "# from src import util\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from peft import LoraConfig, get_peft_model, PromptTuningConfig, PeftType\n",
    "import wandb\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_paths_and_labels_from_df(df, data_dir):\n",
    "    article_ids = df[\"article_id\"].values\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for article_id in article_ids:\n",
    "        image_path = f\"{data_dir}/images/0{str(article_id)[:2]}/0{article_id}.jpg\"\n",
    "        # Check if the image file exists\n",
    "        if os.path.exists(image_path):\n",
    "            image_paths.append(image_path)\n",
    "            # Add corresponding label only if the image exists\n",
    "            labels.append(df[df[\"article_id\"] == article_id])\n",
    "\n",
    "    return image_paths, labels\n",
    "\n",
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_paths, processor=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.processor = processor\n",
    "        self.image_ids = []\n",
    "\n",
    "        for image_path in self.image_paths:\n",
    "            if not os.path.exists(image_path):\n",
    "                raise FileNotFoundError(f\"Image {image_path} not found.\")\n",
    "            else:\n",
    "                image_id = int(image_path.split(\"/\")[-1].split(\".\")[0])\n",
    "                self.image_ids.append(image_id)\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx])\n",
    "        if self.processor is not None:\n",
    "            inputs = self.processor(images=image, return_tensors=\"pt\", padding=True)\n",
    "            image = inputs[\"pixel_values\"][0]\n",
    "        return image, self.image_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", \n",
    "                                  cache_dir=\"model\", local_files_only=False)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\", \n",
    "                                          cache_dir=\"model\", local_files_only=False)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = \"data\"\n",
    "data_dir = \"/kaggle/input/h-and-m-personalized-fashion-recommendations\"\n",
    "# data_dir = \"/Users/tobiaspeihengli/Downloads/DD2430/h-and-m-personalized-fashion-recommendations\"\n",
    "# articles_filtered: remove articles with no images\n",
    "articles = pd.read_csv(f\"{data_dir}/articles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map from article_id to df index\n",
    "article_id_to_idx = {article_id: idx for idx, article_id in enumerate(articles[\"article_id\"])}\n",
    "\n",
    "# get all classes of the dataframe\n",
    "class_names = articles.columns.tolist()\n",
    "label_names = dict()\n",
    "label_names_to_idx = dict()\n",
    "for class_name in class_names:\n",
    "    label_names[class_name] = articles[class_name].unique()\n",
    "    label_names_to_idx[class_name] = {label_name: idx for idx, label_name in enumerate(label_names[class_name])}\n",
    "\n",
    "article_ids = label_names[\"article_id\"]\n",
    "selected_class_names = [\"product_group_name\", \"product_type_name\", \"graphical_appearance_name\", \"colour_group_name\", \"perceived_colour_value_name\", \"perceived_colour_master_name\", \"department_name\", \"index_name\", \"index_group_name\", \"section_name\", \"garment_group_name\"]\n",
    "# selected_class_names = [\"product_type_name\", \"graphical_appearance_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_df)=256 len(val_df)=256 len(test_df)=256\n"
     ]
    }
   ],
   "source": [
    "# grouped by product_code\n",
    "grouped = articles.groupby(\"product_code\")\n",
    "groups = [group for _, group in grouped]\n",
    "\n",
    "# split 0.8/0.1/0.1\n",
    "train_groups, test_groups = train_test_split(groups, test_size=0.2, random_state=42) \n",
    "val_groups, test_groups = train_test_split(test_groups, test_size=0.5, random_state=42) \n",
    "\n",
    "train_df = pd.concat(train_groups)\n",
    "val_df = pd.concat(val_groups)\n",
    "test_df = pd.concat(test_groups)\n",
    "\n",
    "# 256 for local test\n",
    "# train_df = train_df.sample(256)\n",
    "# val_df = val_df.sample(256)\n",
    "# test_df = test_df.sample(256)\n",
    "\n",
    "print(f\"{len(train_df)=} {len(val_df)=} {len(test_df)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths, train_labels = get_image_paths_and_labels_from_df(train_df, data_dir)\n",
    "val_paths, val_labels = get_image_paths_and_labels_from_df(val_df, data_dir)\n",
    "test_paths, test_labels = get_image_paths_and_labels_from_df(test_df, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "train_dataset = ImageDataset(train_paths, processor)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "val_dataset = ImageDataset(val_paths, processor)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "test_dataset = ImageDataset(test_paths, processor)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiOutputLayer(torch.nn.Module):\n",
    "    def __init__(self, input_size, inter_size, output_size):\n",
    "        super(MultiOutputLayer, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, inter_size)\n",
    "        self.fc2 = torch.nn.Linear(inter_size, output_size)\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.act = torch.nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiOutputClipModel(torch.nn.Module):\n",
    "#     def __init__(self, clip_model, class_names, vision_hidden_size, inter_size, output_size):\n",
    "#         super(MultiOutputClipModel, self).__init__()\n",
    "#         self.clip_model = clip_model\n",
    "#         self.class_names = class_names\n",
    "#         self.output_layers = torch.nn.ModuleDict({\n",
    "#             class_name: MultiOutputLayer(vision_hidden_size, inter_size, output_size)\n",
    "#             for class_name in self.class_names\n",
    "#         })\n",
    "    \n",
    "#     def forward(\n",
    "#         self,\n",
    "#         text_input_dict,\n",
    "#         pixel_values,\n",
    "#         # position_ids = None,\n",
    "#         output_attentions = None,\n",
    "#         output_hidden_states = None,\n",
    "#         return_dict = None,\n",
    "#     ):\n",
    "\n",
    "#         output_attentions = output_attentions if output_attentions is not None else self.clip_model.config.output_attentions\n",
    "#         output_hidden_states = (\n",
    "#             output_hidden_states if output_hidden_states is not None else self.clip_model.config.output_hidden_states\n",
    "#         )\n",
    "#         return_dict = return_dict if return_dict is not None else self.clip_model.config.use_return_dict\n",
    "\n",
    "#         vision_outputs = self.clip_model.vision_model(\n",
    "#             pixel_values=pixel_values,\n",
    "#             output_attentions=output_attentions,\n",
    "#             output_hidden_states=output_hidden_states,\n",
    "#             return_dict=return_dict,\n",
    "#         )\n",
    "\n",
    "#         vision_embeds = vision_outputs[1]\n",
    "#         vision_embeds_dict = {\n",
    "#             class_name: output_layer(vision_embeds) \n",
    "#                 for class_name, output_layer in self.output_layers.items()\n",
    "#         }\n",
    "\n",
    "#         text_outputs_dict = {\n",
    "#             class_name: self.clip_model.text_model(\n",
    "#                 input_ids=text_input_dict[class_name][\"input_ids\"],\n",
    "#                 attention_mask=text_input_dict[class_name][\"attention_mask\"],\n",
    "#                 # position_ids=position_ids,\n",
    "#                 output_attentions=output_attentions,\n",
    "#                 output_hidden_states=output_hidden_states,\n",
    "#                 return_dict=return_dict,\n",
    "#             ) for class_name in self.class_names\n",
    "#         }\n",
    "\n",
    "#         text_embeds_dict = {\n",
    "#             class_name: self.clip_model.text_projection(text_outputs[1])\n",
    "#                 for class_name, text_outputs in text_outputs_dict.items()\n",
    "#         }\n",
    "\n",
    "#         logits_per_image_dict = {\n",
    "#             class_name: vision_embeds_dict[class_name] @ text_embeds_dict[class_name].T\n",
    "#                 for class_name in self.output_layers.keys()\n",
    "#         }\n",
    "\n",
    "#         return logits_per_image_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiOutputClipModel(torch.nn.Module):\n",
    "    def __init__(self, clip_model, class_names, vision_hidden_size, inter_size, output_size, num_virtual_tokens):\n",
    "        super(MultiOutputClipModel, self).__init__()\n",
    "        self.clip_model = clip_model\n",
    "        self.class_names = class_names\n",
    "        self.output_layers = torch.nn.ModuleDict({\n",
    "            class_name: MultiOutputLayer(vision_hidden_size, inter_size, output_size)\n",
    "            for class_name in self.class_names\n",
    "        })\n",
    "\n",
    "        # Soft prompt embeddings per class\n",
    "        self.num_virtual_tokens = num_virtual_tokens\n",
    "        embedding_dim = self.clip_model.text_model.embeddings.token_embedding.embedding_dim\n",
    "        self.soft_prompt_embeddings = torch.nn.ParameterDict({\n",
    "            class_name: torch.nn.Parameter(torch.randn(num_virtual_tokens, embedding_dim))\n",
    "            for class_name in self.class_names\n",
    "        })\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        text_input_dict,\n",
    "        pixel_values,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.clip_model.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.clip_model.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.clip_model.config.use_return_dict\n",
    "\n",
    "        # Vision processing remains the same\n",
    "        vision_outputs = self.clip_model.vision_model(\n",
    "            pixel_values=pixel_values,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        vision_embeds = vision_outputs[1]\n",
    "        vision_embeds_dict = {\n",
    "            class_name: output_layer(vision_embeds) \n",
    "                for class_name, output_layer in self.output_layers.items()\n",
    "        }\n",
    "\n",
    "        # Text processing with soft prompts\n",
    "        text_embeds_dict = {}\n",
    "        for class_name in self.class_names:\n",
    "            text_inputs = text_input_dict[class_name]\n",
    "            input_ids = text_inputs[\"input_ids\"]\n",
    "            attention_mask = text_inputs[\"attention_mask\"]\n",
    "\n",
    "            input_embeds = self.clip_model.text_model.embeddings.token_embedding(input_ids)\n",
    "            batch_size = input_embeds.shape[0]\n",
    "\n",
    "            # Expand and Concatenate\n",
    "            soft_prompt = self.soft_prompt_embeddings[class_name].unsqueeze(0).expand(batch_size, -1, -1)\n",
    "            input_embeds = torch.cat([soft_prompt, input_embeds], dim=1)\n",
    "\n",
    "            # Adjust attention mask\n",
    "            soft_prompt_mask = torch.ones(batch_size, self.num_virtual_tokens).to(attention_mask.device)\n",
    "            attention_mask = torch.cat([soft_prompt_mask, attention_mask], dim=1)\n",
    "            valid_token_indices = (attention_mask.sum(dim=-1) - 1).long()\n",
    "            attention_mask = attention_mask[:, None, None, :]\n",
    "            attention_mask = attention_mask.expand(-1, 1, attention_mask.size(-1), attention_mask.size(-1))\n",
    "\n",
    "            encoder_outputs = self.clip_model.text_model.encoder(\n",
    "                inputs_embeds=input_embeds,\n",
    "                attention_mask=attention_mask,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "            last_hidden_state = encoder_outputs[0]\n",
    "            # print(last_hidden_state.shape)\n",
    "\n",
    "            pooled_output = last_hidden_state[torch.arange(last_hidden_state.shape[0]), valid_token_indices]\n",
    "\n",
    "            pooled_output = self.clip_model.text_model.final_layer_norm(pooled_output)\n",
    "\n",
    "            text_embeds = self.clip_model.text_projection(pooled_output)\n",
    "\n",
    "            text_embeds_dict[class_name] = text_embeds\n",
    "\n",
    "\n",
    "        # Compute logits\n",
    "        logits_per_image_dict = {\n",
    "            class_name: vision_embeds_dict[class_name] @ text_embeds_dict[class_name].T\n",
    "                for class_name in self.output_layers.keys()\n",
    "        }\n",
    "\n",
    "        return logits_per_image_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom criterion: cross entropy loss across all classes\n",
    "class MultiOutputClipCriterion(torch.nn.Module):\n",
    "    def __init__(self, class_names):\n",
    "        super(MultiOutputClipCriterion, self).__init__()\n",
    "        self.class_names = class_names\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, logits_dict, labels_dict):\n",
    "        loss = 0\n",
    "        for class_name in self.class_names:\n",
    "            logits = logits_dict[class_name]\n",
    "            labels = labels_dict[class_name]\n",
    "            loss += self.criterion(logits, labels)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all parameters in model\n",
    "\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                  # Low-rank dimension (adjustable)\n",
    "    lora_alpha=32,          # Scaling factor (adjustable)\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\"],  # Specify which layers to apply LoRA to\n",
    "    lora_dropout=0.05,       # Dropout rate (optional)\n",
    "    bias=\"none\",            # Whether to include biases (\"none\", \"all\", \"lora_only\")\n",
    "    task_type=\"classification\"  # Task type (\"classification\" or \"regression\")\n",
    ")\n",
    "\n",
    "# Apply LoRA to the CLIP model\n",
    "model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiOutputClipModel(\n",
       "  (clip_model): PeftModel(\n",
       "    (base_model): LoraModel(\n",
       "      (model): CLIPModel(\n",
       "        (text_model): CLIPTextTransformer(\n",
       "          (embeddings): CLIPTextEmbeddings(\n",
       "            (token_embedding): Embedding(49408, 512)\n",
       "            (position_embedding): Embedding(77, 512)\n",
       "          )\n",
       "          (encoder): CLIPEncoder(\n",
       "            (layers): ModuleList(\n",
       "              (0-11): 12 x CLIPEncoderLayer(\n",
       "                (self_attn): CLIPAttention(\n",
       "                  (k_proj): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (v_proj): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (q_proj): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (vision_model): CLIPVisionTransformer(\n",
       "          (embeddings): CLIPVisionEmbeddings(\n",
       "            (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "            (position_embedding): Embedding(50, 768)\n",
       "          )\n",
       "          (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder): CLIPEncoder(\n",
       "            (layers): ModuleList(\n",
       "              (0-11): 12 x CLIPEncoderLayer(\n",
       "                (self_attn): CLIPAttention(\n",
       "                  (k_proj): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (v_proj): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (q_proj): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "        (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_layers): ModuleDict(\n",
       "    (product_type_name): MultiOutputLayer(\n",
       "      (fc1): Linear(in_features=768, out_features=128, bias=True)\n",
       "      (fc2): Linear(in_features=128, out_features=512, bias=True)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (graphical_appearance_name): MultiOutputLayer(\n",
       "      (fc1): Linear(in_features=768, out_features=128, bias=True)\n",
       "      (fc2): Linear(in_features=128, out_features=512, bias=True)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "  )\n",
       "  (soft_prompt_embeddings): ParameterDict(\n",
       "      (graphical_appearance_name): Parameter containing: [torch.FloatTensor of size 10x512]\n",
       "      (product_type_name): Parameter containing: [torch.FloatTensor of size 10x512]\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mo_model = MultiOutputClipModel(model, selected_class_names, 768, 128, 512).to(device)\n",
    "# mo_model.train()\n",
    "\n",
    "num_virtual_tokens = 16  # You can adjust this number\n",
    "mo_model = MultiOutputClipModel(model, selected_class_names, 768, 128, 512, num_virtual_tokens).to(device)\n",
    "mo_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clip_model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight\n",
      "clip_model.base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight\n",
      "output_layers.product_type_name.fc1.weight\n",
      "output_layers.product_type_name.fc1.bias\n",
      "output_layers.product_type_name.fc2.weight\n",
      "output_layers.product_type_name.fc2.bias\n",
      "output_layers.graphical_appearance_name.fc1.weight\n",
      "output_layers.graphical_appearance_name.fc1.bias\n",
      "output_layers.graphical_appearance_name.fc2.weight\n",
      "output_layers.graphical_appearance_name.fc2.bias\n",
      "soft_prompt_embeddings.graphical_appearance_name\n",
      "soft_prompt_embeddings.product_type_name\n"
     ]
    }
   ],
   "source": [
    "# show all trainable parameters in mo_model\n",
    "for name, param in mo_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate text input\n",
    "text_input_dict = {\n",
    "    class_name: processor(text=[f\"A photo of a {label}\" for label in label_names[class_name]], \n",
    "                          return_tensors=\"pt\", padding=True).to(device)\n",
    "    for class_name in selected_class_names\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f005ebe28b2448db338e85cd1090db6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011167449533331819, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tobiaspeihengli/Downloads/DD2430/DD2430_HM_fine-tuning_VLM/wandb/run-20241021_190953-1rfch24v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/peiheng-kth-royal-institute-of-technology/Multi_head_lora_prompt_tune_experiment256samples/runs/1rfch24v' target=\"_blank\">devoted-capybara-20</a></strong> to <a href='https://wandb.ai/peiheng-kth-royal-institute-of-technology/Multi_head_lora_prompt_tune_experiment256samples' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/peiheng-kth-royal-institute-of-technology/Multi_head_lora_prompt_tune_experiment256samples' target=\"_blank\">https://wandb.ai/peiheng-kth-royal-institute-of-technology/Multi_head_lora_prompt_tune_experiment256samples</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/peiheng-kth-royal-institute-of-technology/Multi_head_lora_prompt_tune_experiment256samples/runs/1rfch24v' target=\"_blank\">https://wandb.ai/peiheng-kth-royal-institute-of-technology/Multi_head_lora_prompt_tune_experiment256samples/runs/1rfch24v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/peiheng-kth-royal-institute-of-technology/Multi_head_lora_prompt_tune_experiment256samples/runs/1rfch24v?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x3730c0c40>"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 1  # Adjust as needed\n",
    "criteria = MultiOutputClipCriterion(class_names=selected_class_names)\n",
    "optimizer = torch.optim.AdamW(mo_model.parameters(), lr=1e-4)\n",
    "step = 0\n",
    "wandb.init(project=\"Multi_head_lora_prompt_tune_experiment256samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criteria, device, text_inputs, class_names):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = {class_name: 0 for class_name in class_names}\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, image_ids in tqdm(dataloader):\n",
    "            images = images.to(device)\n",
    "            logits_per_image_dict = model(pixel_values=images, text_input_dict=text_inputs)\n",
    "\n",
    "            # Get true labels from image_ids\n",
    "            true_labels_dict = {\n",
    "                class_name: [label_names_to_idx[class_name][articles.loc[article_id_to_idx[image_id.item()], class_name]] \n",
    "                           for image_id in image_ids]\n",
    "                for class_name in class_names\n",
    "            }\n",
    "            true_labels_dict = {class_name: torch.tensor(true_labels).to(device)\n",
    "                                for class_name, true_labels in true_labels_dict.items()}\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criteria(logits_per_image_dict, true_labels_dict)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "\n",
    "            # Predictions and accuracy\n",
    "            total_samples += images.size(0)\n",
    "            for class_name in class_names:\n",
    "                _, preds = torch.max(logits_per_image_dict[class_name], dim=1)\n",
    "                total_correct[class_name] += (preds == true_labels_dict[class_name]).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / total_samples / len(class_names)\n",
    "    accuracy = {class_name: total_correct[class_name] / total_samples for class_name in class_names}\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:22<00:00, 11.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: 4.1548, Accuracy: 0.0118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:10<00:00, 10.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 4.0514, Validation Accuracy: 0.0275\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acb114b4a1164d3d8ff588b2df944a05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>▁█</td></tr><tr><td>train_loss</td><td>█▁</td></tr><tr><td>val_accuracy</td><td>▁</td></tr><tr><td>val_accuracy_graphical_appearance_name</td><td>▁</td></tr><tr><td>val_accuracy_product_type_name</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>0.01587</td></tr><tr><td>train_loss</td><td>8.23861</td></tr><tr><td>val_accuracy</td><td>0.02745</td></tr><tr><td>val_accuracy_graphical_appearance_name</td><td>0.02745</td></tr><tr><td>val_accuracy_product_type_name</td><td>0.02745</td></tr><tr><td>val_loss</td><td>4.05139</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">devoted-capybara-20</strong> at: <a href='https://wandb.ai/peiheng-kth-royal-institute-of-technology/Multi_head_lora_prompt_tune_experiment256samples/runs/1rfch24v' target=\"_blank\">https://wandb.ai/peiheng-kth-royal-institute-of-technology/Multi_head_lora_prompt_tune_experiment256samples/runs/1rfch24v</a><br/> View project at: <a href='https://wandb.ai/peiheng-kth-royal-institute-of-technology/Multi_head_lora_prompt_tune_experiment256samples' target=\"_blank\">https://wandb.ai/peiheng-kth-royal-institute-of-technology/Multi_head_lora_prompt_tune_experiment256samples</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241021_190953-1rfch24v/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    mo_model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for images, image_ids in tqdm(train_dataloader):\n",
    "        images = images.to(device)\n",
    "        logits_per_image_dict = mo_model(pixel_values=images, text_input_dict=text_input_dict)\n",
    "\n",
    "        # Get true labels from image_ids\n",
    "        true_labels_dict = {\n",
    "            class_name: [label_names_to_idx[class_name][articles.loc[article_id_to_idx[image_id.item()], class_name]] \n",
    "                       for image_id in image_ids]\n",
    "            for class_name in selected_class_names\n",
    "        }\n",
    "        true_labels_dict = {class_name: torch.tensor(true_labels).to(device) \n",
    "                            for class_name, true_labels in true_labels_dict.items()}\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criteria(logits_per_image_dict, true_labels_dict)\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "\n",
    "        # Predictions and accuracy\n",
    "        correct = 0\n",
    "        total_samples += images.size(0)\n",
    "        for class_name in selected_class_names:\n",
    "            _, preds = torch.max(logits_per_image_dict[class_name], dim=1)\n",
    "            correct += (preds == true_labels_dict[class_name]).sum().item()\n",
    "        total_correct += correct\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # log the loss and accuracy to wandb\n",
    "        wandb.log({\"train_loss\": loss.item(), \"train_accuracy\": correct / images.size(0) / len(selected_class_names)},\n",
    "                  step=step)\n",
    "        step += 1\n",
    "\n",
    "    avg_loss = total_loss / total_samples / len(selected_class_names)\n",
    "    accuracy = total_correct / total_samples / len(selected_class_names)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Validate after each epoch\n",
    "    val_loss, val_accuracy_dict = validate(mo_model, val_dataloader, criteria, device, text_input_dict, selected_class_names)\n",
    "    val_accuracy = sum(val_accuracy_dict.values()) / len(val_accuracy_dict)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Log to wandb\n",
    "    log_dict = {\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_accuracy\": val_accuracy\n",
    "    }\n",
    "    for class_name, accuracy in val_accuracy_dict.items():\n",
    "        log_dict[f\"val_accuracy_{class_name}\"] = accuracy\n",
    "\n",
    "    wandb.log(log_dict, step=step)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(mo_model.state_dict(), \"model/multihead_lora_prompt_tune.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:10<00:00, 10.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 4.0457\n",
      "Test Accuracy per Class:\n",
      "product_type_name: 0.0118\n",
      "graphical_appearance_name: 0.0157\n",
      "Average Test Accuracy: 0.0137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy_dict = validate(\n",
    "    mo_model, test_dataloader, criteria, device, text_input_dict, selected_class_names\n",
    ")\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "print(\"Test Accuracy per Class:\")\n",
    "for class_name, accuracy in test_accuracy_dict.items():\n",
    "    print(f\"{class_name}: {accuracy:.4f}\")\n",
    "\n",
    "test_accuracy = sum(test_accuracy_dict.values()) / len(test_accuracy_dict)\n",
    "print(f\"Average Test Accuracy: {test_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
