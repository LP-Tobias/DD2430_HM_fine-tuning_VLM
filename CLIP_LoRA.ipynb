{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":31254,"databundleVersionId":3103714,"sourceType":"competition"}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom tqdm.notebook import tqdm\nimport os\nfrom pathlib import Path\n!pip install peft","metadata":{"execution":{"iopub.status.busy":"2024-09-22T15:49:05.685807Z","iopub.execute_input":"2024-09-22T15:49:05.686250Z","iopub.status.idle":"2024-09-22T15:49:26.368782Z","shell.execute_reply.started":"2024-09-22T15:49:05.686206Z","shell.execute_reply":"2024-09-22T15:49:26.367241Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting peft\n  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0+cpu)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.44.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.33.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.4)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.24.6)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.7.4)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.12.0\n","output_type":"stream"}]},{"cell_type":"code","source":"text_path = '/kaggle/input/h-and-m-personalized-fashion-recommendations/articles.csv'\narticles = pd.read_csv(text_path)\nprint(articles.shape) # 100k data points\narticles.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T13:27:19.200964Z","iopub.execute_input":"2024-09-22T13:27:19.201899Z","iopub.status.idle":"2024-09-22T13:27:20.696789Z","shell.execute_reply.started":"2024-09-22T13:27:19.201840Z","shell.execute_reply":"2024-09-22T13:27:20.695075Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"(105542, 25)\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   article_id  product_code          prod_name  product_type_no  \\\n0   108775015        108775          Strap top              253   \n1   108775044        108775          Strap top              253   \n2   108775051        108775      Strap top (1)              253   \n3   110065001        110065  OP T-shirt (Idro)              306   \n4   110065002        110065  OP T-shirt (Idro)              306   \n\n  product_type_name  product_group_name  graphical_appearance_no  \\\n0          Vest top  Garment Upper body                  1010016   \n1          Vest top  Garment Upper body                  1010016   \n2          Vest top  Garment Upper body                  1010017   \n3               Bra           Underwear                  1010016   \n4               Bra           Underwear                  1010016   \n\n  graphical_appearance_name  colour_group_code colour_group_name  ...  \\\n0                     Solid                  9             Black  ...   \n1                     Solid                 10             White  ...   \n2                    Stripe                 11         Off White  ...   \n3                     Solid                  9             Black  ...   \n4                     Solid                 10             White  ...   \n\n   department_name index_code        index_name index_group_no  \\\n0     Jersey Basic          A        Ladieswear              1   \n1     Jersey Basic          A        Ladieswear              1   \n2     Jersey Basic          A        Ladieswear              1   \n3   Clean Lingerie          B  Lingeries/Tights              1   \n4   Clean Lingerie          B  Lingeries/Tights              1   \n\n   index_group_name section_no            section_name garment_group_no  \\\n0        Ladieswear         16  Womens Everyday Basics             1002   \n1        Ladieswear         16  Womens Everyday Basics             1002   \n2        Ladieswear         16  Womens Everyday Basics             1002   \n3        Ladieswear         61         Womens Lingerie             1017   \n4        Ladieswear         61         Womens Lingerie             1017   \n\n   garment_group_name                                        detail_desc  \n0        Jersey Basic            Jersey top with narrow shoulder straps.  \n1        Jersey Basic            Jersey top with narrow shoulder straps.  \n2        Jersey Basic            Jersey top with narrow shoulder straps.  \n3   Under-, Nightwear  Microfibre T-shirt bra with underwired, moulde...  \n4   Under-, Nightwear  Microfibre T-shirt bra with underwired, moulde...  \n\n[5 rows x 25 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>article_id</th>\n      <th>product_code</th>\n      <th>prod_name</th>\n      <th>product_type_no</th>\n      <th>product_type_name</th>\n      <th>product_group_name</th>\n      <th>graphical_appearance_no</th>\n      <th>graphical_appearance_name</th>\n      <th>colour_group_code</th>\n      <th>colour_group_name</th>\n      <th>...</th>\n      <th>department_name</th>\n      <th>index_code</th>\n      <th>index_name</th>\n      <th>index_group_no</th>\n      <th>index_group_name</th>\n      <th>section_no</th>\n      <th>section_name</th>\n      <th>garment_group_no</th>\n      <th>garment_group_name</th>\n      <th>detail_desc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>108775015</td>\n      <td>108775</td>\n      <td>Strap top</td>\n      <td>253</td>\n      <td>Vest top</td>\n      <td>Garment Upper body</td>\n      <td>1010016</td>\n      <td>Solid</td>\n      <td>9</td>\n      <td>Black</td>\n      <td>...</td>\n      <td>Jersey Basic</td>\n      <td>A</td>\n      <td>Ladieswear</td>\n      <td>1</td>\n      <td>Ladieswear</td>\n      <td>16</td>\n      <td>Womens Everyday Basics</td>\n      <td>1002</td>\n      <td>Jersey Basic</td>\n      <td>Jersey top with narrow shoulder straps.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>108775044</td>\n      <td>108775</td>\n      <td>Strap top</td>\n      <td>253</td>\n      <td>Vest top</td>\n      <td>Garment Upper body</td>\n      <td>1010016</td>\n      <td>Solid</td>\n      <td>10</td>\n      <td>White</td>\n      <td>...</td>\n      <td>Jersey Basic</td>\n      <td>A</td>\n      <td>Ladieswear</td>\n      <td>1</td>\n      <td>Ladieswear</td>\n      <td>16</td>\n      <td>Womens Everyday Basics</td>\n      <td>1002</td>\n      <td>Jersey Basic</td>\n      <td>Jersey top with narrow shoulder straps.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>108775051</td>\n      <td>108775</td>\n      <td>Strap top (1)</td>\n      <td>253</td>\n      <td>Vest top</td>\n      <td>Garment Upper body</td>\n      <td>1010017</td>\n      <td>Stripe</td>\n      <td>11</td>\n      <td>Off White</td>\n      <td>...</td>\n      <td>Jersey Basic</td>\n      <td>A</td>\n      <td>Ladieswear</td>\n      <td>1</td>\n      <td>Ladieswear</td>\n      <td>16</td>\n      <td>Womens Everyday Basics</td>\n      <td>1002</td>\n      <td>Jersey Basic</td>\n      <td>Jersey top with narrow shoulder straps.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>110065001</td>\n      <td>110065</td>\n      <td>OP T-shirt (Idro)</td>\n      <td>306</td>\n      <td>Bra</td>\n      <td>Underwear</td>\n      <td>1010016</td>\n      <td>Solid</td>\n      <td>9</td>\n      <td>Black</td>\n      <td>...</td>\n      <td>Clean Lingerie</td>\n      <td>B</td>\n      <td>Lingeries/Tights</td>\n      <td>1</td>\n      <td>Ladieswear</td>\n      <td>61</td>\n      <td>Womens Lingerie</td>\n      <td>1017</td>\n      <td>Under-, Nightwear</td>\n      <td>Microfibre T-shirt bra with underwired, moulde...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>110065002</td>\n      <td>110065</td>\n      <td>OP T-shirt (Idro)</td>\n      <td>306</td>\n      <td>Bra</td>\n      <td>Underwear</td>\n      <td>1010016</td>\n      <td>Solid</td>\n      <td>10</td>\n      <td>White</td>\n      <td>...</td>\n      <td>Clean Lingerie</td>\n      <td>B</td>\n      <td>Lingeries/Tights</td>\n      <td>1</td>\n      <td>Ladieswear</td>\n      <td>61</td>\n      <td>Womens Lingerie</td>\n      <td>1017</td>\n      <td>Under-, Nightwear</td>\n      <td>Microfibre T-shirt bra with underwired, moulde...</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 25 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Images and their features (articles.csv)\n\n### This table contains all h&m articles with details such as a type of product, a color, a product group and other features.  \n**Article data description:**\n\n- `article_id` : A unique identifier of every article.\n- `product_code`, `prod_name` : A unique identifier of every product and its name (not the same).\n- `product_type`, `product_type_name` : The group of product_code and its name.\n- `product_group_name` : Product Group. Father to product type.\n- `graphical_appearance_no`, `graphical_appearance_name` : The group of graphics and its name.\n- `colour_group_code`, `colour_group_name` : The group of color and its name.\n- `perceived_colour_value_id`, `perceived_colour_value_name`, `perceived_colour_master_id`, `perceived_colour_master_name` : The added color info.\n- `department_no`, `department_name` : A unique identifier of every department and its name.\n- `index_code`, `index_name` : A unique identifier of every index and its name.\n- `index_group_no`, `index_group_name` : A group of indices and its name.\n- `section_no`, `section_name` : A unique identifier of every section and its name.\n- `garment_group_no`, `garment_group_name` : A unique identifier of every garment and its name.\n- `detail_desc` : Details.","metadata":{}},{"cell_type":"code","source":"for col in articles.columns:\n    if not 'no' in col and not 'code' in col and not 'id' in col:\n        un_n = articles[col].nunique()\n        print(f'n of unique {col}: {un_n}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(15, 7))\nax = sns.histplot(data=articles, y='index_name', color='skyblue', ax=ax)\nax.set_xlabel('count by index name')\nax.set_ylabel('index name')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(15, 7))\nax = sns.histplot(data=articles, y='garment_group_name', color='orange', hue='index_group_name', multiple=\"stack\")\nax.set_xlabel('count by garment group')\nax.set_ylabel('garment group')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"articles.groupby(['index_group_name', 'index_name']).count()['article_id']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.options.display.max_rows = None\narticles.groupby(['product_group_name', 'product_type_name']).count()['article_id']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LoRA finetune CLIP","metadata":{}},{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset\nfrom peft import LoraConfig, get_peft_model\nfrom transformers import (\n    CLIPProcessor, \n    CLIPModel, \n    TrainingArguments, \n    Trainer\n)\n\n# Load CLIP model and processor on CPU\nmodel_name = \"openai/clip-vit-base-patch32\"\nmodel = CLIPModel.from_pretrained(model_name)\nprocessor = CLIPProcessor.from_pretrained(model_name)\nmodel","metadata":{"execution":{"iopub.status.busy":"2024-09-22T15:49:26.371558Z","iopub.execute_input":"2024-09-22T15:49:26.372436Z","iopub.status.idle":"2024-09-22T15:50:00.947288Z","shell.execute_reply.started":"2024-09-22T15:49:26.372381Z","shell.execute_reply":"2024-09-22T15:50:00.945889Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e44cc7ed0c14ffaabf82d58ef8ea80d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3417faa1d59248d2ab796a672c76dcf4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b08bd8a883344fc9176ee25c1b8c2d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bd795c6786a486cbb00623d9574142c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f7d06f384474488a8f83ac9513f3312"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c62d01acce7e4cd5888880010d0cccbe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b277786973d48919edfe5c29818730a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da31d6e8852546d88218d8ce48c0aad0"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"CLIPModel(\n  (text_model): CLIPTextTransformer(\n    (embeddings): CLIPTextEmbeddings(\n      (token_embedding): Embedding(49408, 512)\n      (position_embedding): Embedding(77, 512)\n    )\n    (encoder): CLIPEncoder(\n      (layers): ModuleList(\n        (0-11): 12 x CLIPEncoderLayer(\n          (self_attn): CLIPSdpaAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): CLIPMLP(\n            (activation_fn): QuickGELUActivation()\n            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  )\n  (vision_model): CLIPVisionTransformer(\n    (embeddings): CLIPVisionEmbeddings(\n      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n      (position_embedding): Embedding(50, 768)\n    )\n    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (encoder): CLIPEncoder(\n      (layers): ModuleList(\n        (0-11): 12 x CLIPEncoderLayer(\n          (self_attn): CLIPSdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): CLIPMLP(\n            (activation_fn): QuickGELUActivation()\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# How process processing images\nfrom PIL import Image\n# Download and open an example image\njpg = '/kaggle/input/h-and-m-personalized-fashion-recommendations/images/017/0176754003.jpg'\nimage = Image.open(jpg)\n\n# Process the image\ninputs = processor(images=image, return_tensors=\"pt\")\n\n# Visualize the processing steps\nfig, axs = plt.subplots(1, 3, figsize=(15, 5))\n\n# Original image\naxs[0].imshow(image)\naxs[0].set_title(\"Original Image\")\naxs[0].axis('off')\n\n# Resized and cropped image\nresized_image = image.resize((224, 224))\naxs[1].imshow(resized_image)\naxs[1].set_title(\"Resized and Cropped\")\naxs[1].axis('off')\n\n# Normalized image\nnormalized_image = inputs['pixel_values'][0].permute(1, 2, 0)\naxs[2].imshow(normalized_image)\naxs[2].set_title(\"Normalized\")\naxs[2].axis('off')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Original image shape:\", image.size)\nprint(\"Processed image shape:\", inputs['pixel_values'].shape)\nprint(\"Pixel value range:\", inputs['pixel_values'].min().item(), \"to\", inputs['pixel_values'].max().item())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## product_group_name","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom datasets import Dataset\nimport itertools\n\n# 图片文本目录路径\nimages_path = '/kaggle/input/h-and-m-personalized-fashion-recommendations/images'\ntext_path = '/kaggle/input/h-and-m-personalized-fashion-recommendations/articles.csv'\n\n# 读取csv文件\n# df = pd.read_csv(text_path)\ndf = articles\ndf['image_path'] = ''\n\n# 为每个article_id添加对应的图片路径，并将表格保存到/kaggle/working/articles_with_image_path.csv\nfor i in range(df.shape[0]):\n    # 用article_id获取图片路径\n    article_id = str(df.iloc[i]['article_id'])\n    image_path = images_path + f'/0{article_id[:2]}' + f'/0{article_id}.jpg'\n    if not os.path.exists(image_path):\n        continue\n    df.loc[i, 'image_path'] = image_path\n    \n\ndf.to_csv('/kaggle/working/articles_with_image_path.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove all the columns that their image_path is NAN\ndf = pd.read_csv('/kaggle/working/articles_with_image_path.csv')\nfilter_product_group_name = ['Unknown','Underwear/nightwear','Cosmetic','Bags','Items',\n    'Furniture','Garment and Shoe care','Stationery','Interior textile','Fun']\nformat_df = df[df['image_path'].notna() & (df['image_path'] != '') & (~df['product_group_name'].isin(filter_product_group_name))]\nprint(f'清洗后的df行列数: {format_df.shape}')\n\n# 将df拆分为测试集df和训练集df，并保证product_group_name类别比例保持一致。\nfrom sklearn.model_selection import StratifiedShuffleSplit\nimport pandas as pd\nimport numpy as np\n\ndef stratified_split(df, text_column, test_size=0.2, random_state=42):\n    # 为每个唯一的 text 值分配一个类别标签\n    df['text_category'] = pd.Categorical(df[text_column]).codes\n    \n    # 初始化 StratifiedShuffleSplit\n    splitter = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n    \n    # 进行分层抽样\n    for train_index, test_index in splitter.split(df, df['text_category']):\n        train_df = df.iloc[train_index].copy()\n        test_df = df.iloc[test_index].copy()\n    \n    # 删除临时的 'text_category' 列\n    train_df.drop('text_category', axis=1, inplace=True)\n    test_df.drop('text_category', axis=1, inplace=True)\n    \n    return train_df, test_df\n\n# 进行分层抽样\ntrain_df, test_df = stratified_split(format_df, text_column='product_group_name', test_size=0.2)\n\nprint(f\"训练集大小: {len(train_df)}\")\nprint(f\"测试集大小: {len(test_df)}\")\n\n# 检查每个集合中各类别的比例\ndef check_proportions(df, column):\n    return df[column].value_counts(normalize=True)\n\nprint(\"\\n训练集中的类别比例:\")\nprint(check_proportions(train_df, 'product_group_name'))\n\nprint(\"\\n测试集中的类别比例:\")\nprint(check_proportions(test_df, 'product_group_name'))\n# Unknown之后的占比太小（只占了300个dp不到）感觉可以删除，但如何处理这部分的推理","metadata":{"execution":{"iopub.status.busy":"2024-09-22T15:50:00.949180Z","iopub.execute_input":"2024-09-22T15:50:00.949994Z","iopub.status.idle":"2024-09-22T15:50:02.501269Z","shell.execute_reply.started":"2024-09-22T15:50:00.949938Z","shell.execute_reply":"2024-09-22T15:50:02.500073Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"清洗后的df行列数: (104803, 27)\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/1618043607.py:15: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['text_category'] = pd.Categorical(df[text_column]).codes\n","output_type":"stream"},{"name":"stdout","text":"训练集大小: 83842\n测试集大小: 20961\n\n训练集中的类别比例:\nproduct_group_name\nGarment Upper body    0.407242\nGarment Lower body    0.188641\nGarment Full body     0.126679\nAccessories           0.105007\nUnderwear             0.052110\nShoes                 0.049200\nSwimwear              0.029818\nSocks & Tights        0.023198\nNightwear             0.018105\nName: proportion, dtype: float64\n\n测试集中的类别比例:\nproduct_group_name\nGarment Upper body    0.407232\nGarment Lower body    0.188636\nGarment Full body     0.126664\nAccessories           0.105005\nUnderwear             0.052144\nShoes                 0.049187\nSwimwear              0.029817\nSocks & Tights        0.023186\nNightwear             0.018129\nName: proportion, dtype: float64\n","output_type":"stream"}]},{"cell_type":"code","source":"from PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\n\nclass TextImageDataset(Dataset): # 接受处理后的df构建迭代器数据集\n    def __init__(self, dataframe, prompt):\n        self.dataframe = dataframe\n        self.prompt = prompt\n        \n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, index):\n        row = self.dataframe.iloc[index]\n        text = row['product_group_name']\n        text = self.prompt.format(text)\n        image = Image.open(row['image_path'])\n        print(f\"Sample at index {index}: {text}\") \n        return {\n            'text': text,\n            'image': image,\n        }\n\n\n# 创建数据集实例\n# df = pd.DataFrame({'text': ['example1', 'example2'], 'image_path': ['path/to/image1.jpg', None]})\nprompt = 'A photo of a {}'\ntrain_dataset = TextImageDataset(train_df, prompt)\ntest_dataset = TextImageDataset(test_df, prompt)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:15:40.705209Z","iopub.execute_input":"2024-09-22T16:15:40.705722Z","iopub.status.idle":"2024-09-22T16:15:40.717103Z","shell.execute_reply.started":"2024-09-22T16:15:40.705672Z","shell.execute_reply":"2024-09-22T16:15:40.715660Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollator\nclass CustomDataCollator(DataCollator):\n    def __init__(self):\n        pass\n\n    def __call__(self, batch):\n        texts = [item['text'] for item in batch]\n        images = [item['image'] for item in batch]\n        \n        # 处理文本\n        text_inputs = processor(\n            text=texts,\n            padding=True,\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n        \n        # 处理图像\n        image_inputs = processor(\n            images=images,\n            return_tensors=\"pt\",\n        )\n        \n        # 合并文本和图像的输入\n        inputs = {\n            'input_ids': text_inputs['input_ids'],\n            'attention_mask': text_inputs['attention_mask'],\n            'pixel_values': image_inputs['pixel_values'],\n        }\n        \n        return inputs\n    \ndata_collator = CustomDataCollator()","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:54:29.618705Z","iopub.execute_input":"2024-09-22T16:54:29.619221Z","iopub.status.idle":"2024-09-22T16:54:29.686827Z","shell.execute_reply.started":"2024-09-22T16:54:29.619178Z","shell.execute_reply":"2024-09-22T16:54:29.685002Z"},"trusted":true},"execution_count":28,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataCollator\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCustomDataCollator\u001b[39;00m(DataCollator):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n","\u001b[0;31mTypeError\u001b[0m: NewType.__init__() takes 3 positional arguments but 4 were given"],"ename":"TypeError","evalue":"NewType.__init__() takes 3 positional arguments but 4 were given","output_type":"error"}]},{"cell_type":"code","source":"# Configure LoRA\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\", 'k_proj'],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\npeft_model = get_peft_model(model, lora_config)\nprint(peft_model.print_trainable_parameters())\n\n# Define data collator\ndef collate_fn(batch):\n    texts = [item['text'] for item in batch]\n    images = [item['image'] for item in batch]\n    \n    # 处理文本\n    text_inputs = processor(\n        text=texts,\n        padding=True,\n        truncation=True,\n        return_tensors=\"pt\",\n    )\n    \n    # 处理图像\n    image_inputs = processor(\n        images=images,\n        return_tensors=\"pt\",\n    )\n    \n    # 合并文本和图像的输入\n    inputs = {\n        'input_ids': text_inputs['input_ids'],\n        'attention_mask': text_inputs['attention_mask'],\n        'pixel_values': image_inputs['pixel_values'],\n    }\n    \n    return inputs\n\n# Set up training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/clip_lora_output\",\n    learning_rate=5e-5,\n    per_device_train_batch_size=8,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    report_to='tensorboard'\n)\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=peft_model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    data_collator=collate_fn,\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:50:14.842869Z","iopub.execute_input":"2024-09-22T16:50:14.843339Z","iopub.status.idle":"2024-09-22T16:50:15.005402Z","shell.execute_reply.started":"2024-09-22T16:50:14.843290Z","shell.execute_reply":"2024-09-22T16:50:15.003943Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"trainable params: 1,474,560 || all params: 152,751,873 || trainable%: 0.9653\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"# Start training\ntrainer.train()\n\n# Save the fine-tuned model\npeft_model.save_pretrained(\"./clip_peft_finetuned\")","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:50:17.636787Z","iopub.execute_input":"2024-09-22T16:50:17.637228Z","iopub.status.idle":"2024-09-22T16:50:18.497249Z","shell.execute_reply.started":"2024-09-22T16:50:17.637186Z","shell.execute_reply":"2024-09-22T16:50:18.495275Z"},"trusted":true},"execution_count":25,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Save the fine-tuned model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m peft_model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./clip_peft_finetuned\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1948\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1946\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1948\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1949\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1950\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1953\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2246\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2243\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2245\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2246\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   2247\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39minclude_num_input_tokens_seen:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/data_loader.py:454\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 454\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","\u001b[0;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"],"ename":"TypeError","evalue":"'DataLoader' object is not subscriptable","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}