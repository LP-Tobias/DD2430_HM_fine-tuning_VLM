{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":31254,"databundleVersionId":3103714,"sourceType":"competition"}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom tqdm.notebook import tqdm\nimport os\nfrom pathlib import Path\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nimport torch.nn.functional as F\nimport torch\n!pip install peft","metadata":{"execution":{"iopub.status.busy":"2024-10-09T07:55:47.045233Z","iopub.execute_input":"2024-10-09T07:55:47.045604Z","iopub.status.idle":"2024-10-09T07:56:02.757495Z","shell.execute_reply.started":"2024-10-09T07:55:47.045541Z","shell.execute_reply":"2024-10-09T07:56:02.756285Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.44.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.33.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.4)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.24.6)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Features table of product items\n\n### This table contains all h&m articles with details such as a type of product, a color, a product group and other features.  \n**Article data description:**\n\n- `article_id` : A unique identifier of every article.\n- `product_code`, `prod_name` : A unique identifier of every product and its name (not the same).\n- `product_type`, `product_type_name` : The group of product_code and its name.\n- `product_group_name` : Product Group. Father to product type.\n- `graphical_appearance_no`, `graphical_appearance_name` : The group of graphics and its name.\n- `colour_group_code`, `colour_group_name` : The group of color and its name.\n- `perceived_colour_value_id`, `perceived_colour_value_name`, `perceived_colour_master_id`, `perceived_colour_master_name` : The added color info.\n- `department_no`, `department_name` : A unique identifier of every department and its name.\n- `index_code`, `index_name` : A unique identifier of every index and its name.\n- `index_group_no`, `index_group_name` : A group of indices and its name.\n- `section_no`, `section_name` : A unique identifier of every section and its name.\n- `garment_group_no`, `garment_group_name` : A unique identifier of every garment and its name.\n- `detail_desc` : Details.","metadata":{}},{"cell_type":"code","source":"text_path = '/kaggle/input/h-and-m-personalized-fashion-recommendations/articles.csv'\narticles = pd.read_csv(text_path)\nprint(articles.shape) # 100k data points\narticles.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LoRA finetune CLIP (single column)","metadata":{}},{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset\nfrom peft import LoraConfig, get_peft_model\nfrom transformers import (\n    CLIPProcessor, \n    CLIPModel, \n    TrainingArguments, \n    Trainer\n)\n\n# Load CLIP model and processor\nmodel_name = \"openai/clip-vit-base-patch32\"\nmodel = CLIPModel.from_pretrained(model_name)\nprocessor = CLIPProcessor.from_pretrained(model_name)\nmodel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset on product_group_name ","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom datasets import Dataset\nimport itertools\n\n# 图片文本目录路径\nimages_path = '/kaggle/input/h-and-m-personalized-fashion-recommendations/images'\ntext_path = '/kaggle/input/h-and-m-personalized-fashion-recommendations/articles.csv'\n\n# 读取csv文件\n# df = pd.read_csv(text_path)\ndf = articles\ndf['image_path'] = ''\n\n# 为每个article_id添加对应的图片路径，并将表格保存到/kaggle/working/articles_with_image_path.csv\nfor i in range(df.shape[0]):\n    # 用article_id获取图片路径\n    article_id = str(df.iloc[i]['article_id'])\n    image_path = images_path + f'/0{article_id[:2]}' + f'/0{article_id}.jpg'\n    if not os.path.exists(image_path):\n        continue\n    df.loc[i, 'image_path'] = image_path\n    \n\ndf.to_csv('/kaggle/working/articles_with_image_path.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove all the columns that their image_path is NAN\ndf = pd.read_csv('/kaggle/working/articles_with_image_path.csv')\nfilter_product_group_name = ['Unknown','Underwear/nightwear','Cosmetic','Bags','Items',\n    'Furniture','Garment and Shoe care','Stationery','Interior textile','Fun']\nformat_df = df[df['image_path'].notna() & (df['image_path'] != '') & (~df['product_group_name'].isin(filter_product_group_name))]\nprint(f'清洗后的df行列数: {format_df.shape}')\n\n# 将df拆分为测试集df和训练集df，并保证product_group_name类别比例保持一致。\nfrom sklearn.model_selection import StratifiedShuffleSplit\nimport pandas as pd\nimport numpy as np\n\ndef stratified_split(df, text_column, test_size=0.2, random_state=42):\n    # 为每个唯一的 text 值分配一个类别标签\n    df['text_category'] = pd.Categorical(df[text_column]).codes\n    \n    # 初始化 StratifiedShuffleSplit\n    splitter = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n    \n    # 进行分层抽样\n    for train_index, test_index in splitter.split(df, df['text_category']):\n        train_df = df.iloc[train_index].copy()\n        test_df = df.iloc[test_index].copy()\n    \n    # 删除临时的 'text_category' 列\n    train_df.drop('text_category', axis=1, inplace=True)\n    test_df.drop('text_category', axis=1, inplace=True)\n    \n    return train_df, test_df\n\n# 进行分层抽样\ntrain_df, val_df = stratified_split(format_df, text_column='product_group_name', test_size=0.2)\n\nprint(f\"训练集大小: {len(train_df)}\")\nprint(f\"验证集大小: {len(val_df)}\")\n\n# 检查每个集合中各类别的比例\ndef check_proportions(df, column):\n    return df[column].value_counts(normalize=True)\n\nprint(\"\\n训练集中的类别比例:\")\nprint(check_proportions(train_df, 'product_group_name'))\n\nprint(\"\\n验证集中的类别比例:\")\nprint(check_proportions(val_df, 'product_group_name'))\n# Unknown之后的占比太小（只占了300个dp不到）感觉可以删除，但如何处理这部分的推理","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 数据集构造\nclass TextImageDataset(Dataset): # 接受处理后的df构建迭代器数据集\n    def __init__(self, dataframe, prompt):\n        self.dataframe = dataframe\n        self.prompt = prompt\n        \n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, index):\n        row = self.dataframe.iloc[index]\n        text = row['product_group_name']\n        text = self.prompt.format(text)\n        image = Image.open(row['image_path'])\n\n        return {\n            'text': text,\n            'image': image,\n        }\n\n\n# 创建数据集实例\nprompt = 'A photo of a {}'\ntrain_dataset = TextImageDataset(train_df, prompt)\nval_dataset = TextImageDataset(val_df, prompt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Multi labels training","metadata":{}},{"cell_type":"markdown","source":"## Model loading and lora config","metadata":{}},{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset\nfrom peft import LoraConfig, get_peft_model\nfrom transformers import (\n    CLIPProcessor, \n    CLIPModel, \n    TrainingArguments, \n    Trainer\n)\n\n# Load CLIP model and processor\nmodel_name = \"openai/clip-vit-base-patch32\"\nmodel = CLIPModel.from_pretrained(model_name)\nprocessor = CLIPProcessor.from_pretrained(model_name)\n\n# Configure LoRA\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\", 'k_proj'],\n    lora_dropout=0.05,\n    bias=\"none\",\n#     task_type=\"classification\"\n)\n\npeft_model = get_peft_model(model, lora_config)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\npeft_model.to(device)\nprint(peft_model.print_trainable_parameters())","metadata":{"execution":{"iopub.status.busy":"2024-10-09T07:56:02.759765Z","iopub.execute_input":"2024-10-09T07:56:02.760516Z","iopub.status.idle":"2024-10-09T07:56:09.948482Z","shell.execute_reply.started":"2024-10-09T07:56:02.760468Z","shell.execute_reply":"2024-10-09T07:56:09.947470Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 1,474,560 || all params: 152,751,873 || trainable%: 0.9653\nNone\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Data processing and Dataset ","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/working/articles_with_image_path.csv')\ndf = df[df['image_path'].notna() & (df['image_path'] != '')]\ntarget_columns = [\"product_group_name\", \"product_type_name\", \"graphical_appearance_name\", \"colour_group_name\", \"perceived_colour_value_name\", \"perceived_colour_master_name\", \"department_name\", \"index_name\", \"index_group_name\", \"section_name\", \"garment_group_name\"]\n# 11 target col\nprint(df.shape)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-09T07:56:09.949605Z","iopub.execute_input":"2024-10-09T07:56:09.950224Z","iopub.status.idle":"2024-10-09T07:56:10.875383Z","shell.execute_reply.started":"2024-10-09T07:56:09.950188Z","shell.execute_reply":"2024-10-09T07:56:10.874419Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"(105100, 27)\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0  article_id  product_code          prod_name  product_type_no  \\\n0           0   108775015        108775          Strap top              253   \n1           1   108775044        108775          Strap top              253   \n2           2   108775051        108775      Strap top (1)              253   \n3           3   110065001        110065  OP T-shirt (Idro)              306   \n4           4   110065002        110065  OP T-shirt (Idro)              306   \n\n  product_type_name  product_group_name  graphical_appearance_no  \\\n0          Vest top  Garment Upper body                  1010016   \n1          Vest top  Garment Upper body                  1010016   \n2          Vest top  Garment Upper body                  1010017   \n3               Bra           Underwear                  1010016   \n4               Bra           Underwear                  1010016   \n\n  graphical_appearance_name  colour_group_code  ... index_code  \\\n0                     Solid                  9  ...          A   \n1                     Solid                 10  ...          A   \n2                    Stripe                 11  ...          A   \n3                     Solid                  9  ...          B   \n4                     Solid                 10  ...          B   \n\n         index_name index_group_no  index_group_name section_no  \\\n0        Ladieswear              1        Ladieswear         16   \n1        Ladieswear              1        Ladieswear         16   \n2        Ladieswear              1        Ladieswear         16   \n3  Lingeries/Tights              1        Ladieswear         61   \n4  Lingeries/Tights              1        Ladieswear         61   \n\n             section_name garment_group_no garment_group_name  \\\n0  Womens Everyday Basics             1002       Jersey Basic   \n1  Womens Everyday Basics             1002       Jersey Basic   \n2  Womens Everyday Basics             1002       Jersey Basic   \n3         Womens Lingerie             1017  Under-, Nightwear   \n4         Womens Lingerie             1017  Under-, Nightwear   \n\n                                         detail_desc  \\\n0            Jersey top with narrow shoulder straps.   \n1            Jersey top with narrow shoulder straps.   \n2            Jersey top with narrow shoulder straps.   \n3  Microfibre T-shirt bra with underwired, moulde...   \n4  Microfibre T-shirt bra with underwired, moulde...   \n\n                                          image_path  \n0  /kaggle/input/h-and-m-personalized-fashion-rec...  \n1  /kaggle/input/h-and-m-personalized-fashion-rec...  \n2  /kaggle/input/h-and-m-personalized-fashion-rec...  \n3  /kaggle/input/h-and-m-personalized-fashion-rec...  \n4  /kaggle/input/h-and-m-personalized-fashion-rec...  \n\n[5 rows x 27 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>article_id</th>\n      <th>product_code</th>\n      <th>prod_name</th>\n      <th>product_type_no</th>\n      <th>product_type_name</th>\n      <th>product_group_name</th>\n      <th>graphical_appearance_no</th>\n      <th>graphical_appearance_name</th>\n      <th>colour_group_code</th>\n      <th>...</th>\n      <th>index_code</th>\n      <th>index_name</th>\n      <th>index_group_no</th>\n      <th>index_group_name</th>\n      <th>section_no</th>\n      <th>section_name</th>\n      <th>garment_group_no</th>\n      <th>garment_group_name</th>\n      <th>detail_desc</th>\n      <th>image_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>108775015</td>\n      <td>108775</td>\n      <td>Strap top</td>\n      <td>253</td>\n      <td>Vest top</td>\n      <td>Garment Upper body</td>\n      <td>1010016</td>\n      <td>Solid</td>\n      <td>9</td>\n      <td>...</td>\n      <td>A</td>\n      <td>Ladieswear</td>\n      <td>1</td>\n      <td>Ladieswear</td>\n      <td>16</td>\n      <td>Womens Everyday Basics</td>\n      <td>1002</td>\n      <td>Jersey Basic</td>\n      <td>Jersey top with narrow shoulder straps.</td>\n      <td>/kaggle/input/h-and-m-personalized-fashion-rec...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>108775044</td>\n      <td>108775</td>\n      <td>Strap top</td>\n      <td>253</td>\n      <td>Vest top</td>\n      <td>Garment Upper body</td>\n      <td>1010016</td>\n      <td>Solid</td>\n      <td>10</td>\n      <td>...</td>\n      <td>A</td>\n      <td>Ladieswear</td>\n      <td>1</td>\n      <td>Ladieswear</td>\n      <td>16</td>\n      <td>Womens Everyday Basics</td>\n      <td>1002</td>\n      <td>Jersey Basic</td>\n      <td>Jersey top with narrow shoulder straps.</td>\n      <td>/kaggle/input/h-and-m-personalized-fashion-rec...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>108775051</td>\n      <td>108775</td>\n      <td>Strap top (1)</td>\n      <td>253</td>\n      <td>Vest top</td>\n      <td>Garment Upper body</td>\n      <td>1010017</td>\n      <td>Stripe</td>\n      <td>11</td>\n      <td>...</td>\n      <td>A</td>\n      <td>Ladieswear</td>\n      <td>1</td>\n      <td>Ladieswear</td>\n      <td>16</td>\n      <td>Womens Everyday Basics</td>\n      <td>1002</td>\n      <td>Jersey Basic</td>\n      <td>Jersey top with narrow shoulder straps.</td>\n      <td>/kaggle/input/h-and-m-personalized-fashion-rec...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>110065001</td>\n      <td>110065</td>\n      <td>OP T-shirt (Idro)</td>\n      <td>306</td>\n      <td>Bra</td>\n      <td>Underwear</td>\n      <td>1010016</td>\n      <td>Solid</td>\n      <td>9</td>\n      <td>...</td>\n      <td>B</td>\n      <td>Lingeries/Tights</td>\n      <td>1</td>\n      <td>Ladieswear</td>\n      <td>61</td>\n      <td>Womens Lingerie</td>\n      <td>1017</td>\n      <td>Under-, Nightwear</td>\n      <td>Microfibre T-shirt bra with underwired, moulde...</td>\n      <td>/kaggle/input/h-and-m-personalized-fashion-rec...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>110065002</td>\n      <td>110065</td>\n      <td>OP T-shirt (Idro)</td>\n      <td>306</td>\n      <td>Bra</td>\n      <td>Underwear</td>\n      <td>1010016</td>\n      <td>Solid</td>\n      <td>10</td>\n      <td>...</td>\n      <td>B</td>\n      <td>Lingeries/Tights</td>\n      <td>1</td>\n      <td>Ladieswear</td>\n      <td>61</td>\n      <td>Womens Lingerie</td>\n      <td>1017</td>\n      <td>Under-, Nightwear</td>\n      <td>Microfibre T-shirt bra with underwired, moulde...</td>\n      <td>/kaggle/input/h-and-m-personalized-fashion-rec...</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 27 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def split_dataset(df, val_ratio=0.2, seed=42):\n    \"\"\"\n    将DataFrame划分为训练集和验证集\n    \n    Args:\n        df: 输入的DataFrame\n        val_ratio: 验证集占比，默认0.2\n        seed: 随机种子，用于复现结果\n        \n    Returns:\n        train_df: 训练集DataFrame\n        val_df: 验证集DataFrame\n    \"\"\"\n    # 设置随机种子\n    np.random.seed(seed)\n    \n    # 获取总行数\n    n_total = len(df)\n    \n    # 计算验证集大小\n    n_val = int(n_total * val_ratio)\n    \n    # 生成随机索引\n    indices = np.random.permutation(n_total)\n    val_indices = indices[:n_val]\n    train_indices = indices[n_val:]\n    \n    # 划分数据集\n    val_df = df.iloc[val_indices].copy().reset_index(drop=True)\n    train_df = df.iloc[train_indices].copy().reset_index(drop=True)\n    \n    print(f\"总数据量: {n_total}\")\n    print(f\"训练集大小: {len(train_df)} ({1-val_ratio:.1%})\")\n    print(f\"验证集大小: {len(val_df)} ({val_ratio:.1%})\")\n    \n    return train_df, val_df\n\ntrain_df, val_df = split_dataset(df, val_ratio=0.2)\n\ncolumns_unique_labels = {}\nfor t_col in target_columns:\n    columns_unique_labels[t_col] = list(df[t_col].unique())\nfor k,v in columns_unique_labels.items():\n    print(f'\\nClass {k} has {len(v)} unique labels:')\n    print(v)","metadata":{"execution":{"iopub.status.busy":"2024-10-09T07:56:19.997395Z","iopub.execute_input":"2024-10-09T07:56:19.998378Z","iopub.status.idle":"2024-10-09T07:56:20.223243Z","shell.execute_reply.started":"2024-10-09T07:56:19.998333Z","shell.execute_reply":"2024-10-09T07:56:20.222212Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"总数据量: 105100\n训练集大小: 84080 (80.0%)\n验证集大小: 21020 (20.0%)\n\nClass product_group_name has 19 unique labels:\n['Garment Upper body', 'Underwear', 'Socks & Tights', 'Garment Lower body', 'Accessories', 'Items', 'Nightwear', 'Unknown', 'Underwear/nightwear', 'Shoes', 'Swimwear', 'Garment Full body', 'Cosmetic', 'Interior textile', 'Bags', 'Furniture', 'Garment and Shoe care', 'Fun', 'Stationery']\n\nClass product_type_name has 131 unique labels:\n['Vest top', 'Bra', 'Underwear Tights', 'Socks', 'Leggings/Tights', 'Sweater', 'Top', 'Trousers', 'Hair clip', 'Umbrella', 'Pyjama jumpsuit/playsuit', 'Bodysuit', 'Hair string', 'Unknown', 'Hoodie', 'Sleep Bag', 'Hair/alice band', 'Belt', 'Boots', 'Bikini top', 'Swimwear bottom', 'Underwear bottom', 'Swimsuit', 'Skirt', 'T-shirt', 'Dress', 'Hat/beanie', 'Kids Underwear top', 'Shorts', 'Shirt', 'Cap/peaked', 'Pyjama set', 'Sneakers', 'Sunglasses', 'Cardigan', 'Gloves', 'Earring', 'Bag', 'Blazer', 'Other shoe', 'Jumpsuit/Playsuit', 'Sandals', 'Jacket', 'Costumes', 'Robe', 'Scarf', 'Coat', 'Other accessories', 'Polo shirt', 'Slippers', 'Night gown', 'Alice band', 'Straw hat', 'Hat/brim', 'Tailored Waistcoat', 'Ballerinas', 'Tie', 'Necklace', 'Pyjama bottom', 'Felt hat', 'Bracelet', 'Blouse', 'Outdoor overall', 'Watch', 'Underwear body', 'Beanie', 'Giftbox', 'Sleeping sack', 'Dungarees', 'Outdoor trousers', 'Wallet', 'Swimwear set', 'Swimwear top', 'Flat shoe', 'Garment Set', 'Ring', 'Waterbottle', 'Wedge', 'Long John', 'Outdoor Waistcoat', 'Pumps', 'Flip flop', 'Braces', 'Bootie', 'Fine cosmetics', 'Heeled sandals', 'Nipple covers', 'Chem. cosmetics', 'Hair ties', 'Underwear corset', 'Bra extender', 'Underdress', 'Underwear set', 'Sarong', 'Soft Toys', 'Leg warmers', 'Blanket', 'Hairband', 'Tote bag', 'Weekend/Gym bag', 'Cushion', 'Backpack', 'Earrings', 'Bucket hat', 'Flat shoes', 'Heels', 'Cap', 'Shoulder bag', 'Side table', 'Accessories set', 'Headband', 'Baby Bib', 'Keychain', 'Dog Wear', 'Washing bag', 'Sewing kit', 'Cross-body bag', 'Moccasins', 'Towel', 'Wood balls', 'Zipper head', 'Mobile case', 'Pre-walkers', 'Toy', 'Marker pen', 'Bumbag', 'Dog wear', 'Eyeglasses', 'Wireless earphone case', 'Stain remover spray', 'Clothing mist']\n\nClass graphical_appearance_name has 30 unique labels:\n['Solid', 'Stripe', 'All over pattern', 'Melange', 'Transparent', 'Metallic', 'Application/3D', 'Denim', 'Colour blocking', 'Dot', 'Other structure', 'Contrast', 'Treatment', 'Check', 'Chambray', 'Front print', 'Glittering/Metallic', 'Mixed solid/pattern', 'Placement print', 'Other pattern', 'Neps', 'Embroidery', 'Lace', 'Jacquard', 'Unknown', 'Argyle', 'Slub', 'Mesh', 'Sequin', 'Hologram']\n\nClass colour_group_name has 50 unique labels:\n['Black', 'White', 'Off White', 'Light Beige', 'Beige', 'Grey', 'Light Blue', 'Light Grey', 'Dark Blue', 'Dark Grey', 'Pink', 'Dark Red', 'Greyish Beige', 'Light Orange', 'Silver', 'Gold', 'Light Pink', 'Dark Pink', 'Yellowish Brown', 'Blue', 'Light Turquoise', 'Yellow', 'Greenish Khaki', 'Dark Yellow', 'Other Pink', 'Dark Purple', 'Red', 'Transparent', 'Dark Green', 'Other Red', 'Turquoise', 'Dark Orange', 'Other', 'Orange', 'Dark Beige', 'Other Yellow', 'Light Green', 'Other Orange', 'Purple', 'Light Red', 'Light Yellow', 'Green', 'Light Purple', 'Dark Turquoise', 'Bronze/Copper', 'Other Purple', 'Other Turquoise', 'Other Green', 'Other Blue', 'Unknown']\n\nClass perceived_colour_value_name has 8 unique labels:\n['Dark', 'Light', 'Dusty Light', 'Medium Dusty', 'Bright', 'Medium', 'Undefined', 'Unknown']\n\nClass perceived_colour_master_name has 20 unique labels:\n['Black', 'White', 'Beige', 'Grey', 'Blue', 'Pink', 'Lilac Purple', 'Red', 'Mole', 'Orange', 'Metal', 'Brown', 'Turquoise', 'Yellow', 'Khaki green', 'Green', 'undefined', 'Unknown', 'Yellowish Green', 'Bluish Green']\n\nClass department_name has 250 unique labels:\n['Jersey Basic', 'Clean Lingerie', 'Tights basic', 'Baby basics', 'Casual Lingerie', 'Jersey', 'EQ & Special Collections', 'Hair Accessories', 'Other items', 'Baby Nightwear', 'Men Sport Woven', 'Men Sport Bottoms', 'Kids Boy Denim', 'Shopbasket Socks', 'Socks', 'UW', 'Young Girl Jersey Basic', 'Jacket Street', 'Belts', 'Divided Shoes', 'Swimwear', 'Underwear Jersey', 'Basic 1', 'Tops Knitwear DS', 'Men Sport Acc', 'Kids Boy Jersey Basic', 'Young Girl UW/NW', 'Shirt', 'Nightwear', 'Trouser', 'Small Accessories', 'Sunglasses', 'Gloves/Hats', 'Knit & Woven', 'Basics', 'Accessories', 'Jewellery', 'Jersey Fancy DS', 'Trousers DS', 'Bags', 'Blazer S&T', 'Knitwear', 'Woven bottoms', 'Shorts', 'Dresses DS', 'Expressive Lingerie', 'Kids Girl UW/NW', 'Young Boy Jersey Basic', 'Kids Girl S&T', 'Young Girl S&T', 'Shoes / Boots inactive from s5', 'Nursing', 'Jersey Fancy', 'Shoes', 'Functional Lingerie', 'Men Sport Tops', 'Other Accessories', 'Young Boy Trouser', 'Outdoor/Blazers DS', 'Mama Lingerie', 'Socks Bin', 'Denim Other Garments', 'Everyday Waredrobe Denim', 'Trousers', 'Denim Trousers', 'Outdoor/Blazers', 'Young Boy Denim', 'Scarves', 'Dresses', 'Skirts', 'Kids Girl Jersey Basic', 'Baby Socks', 'Trousers & Skirt', 'Young Boy Shirt', 'Kids Girl Big Acc', 'Young Girl Denim', 'Flats', 'Baby Toys/Acc', 'Projects Dresses', 'Denim shorts', 'Jersey inactive from s1', 'Promotion/Other/Offer', 'Bags & Items', 'Small Bags', 'Trouser S&T', 'Shirt S&T', 'Woven Tops', 'Denim trousers', 'Young Girl Knitwear', 'Outwear', 'Knitwear inactive from s1', 'Projects Woven Tops', 'Projects Jersey & Knitwear', 'Denim wardrobe H&M man inactive from S.6', 'Baby Boy Outdoor', 'Premium Quality', 'Woven inactive from s1', 'Tops woven DS', 'Newborn', 'Skirt', 'Jersey License', 'Jewellery Extended', 'Light Basic Jersey', 'Suit Extended inactive from s1', 'Young Boy UW/NW', 'Tops Knitwear', 'Kids Boy Socks', 'Young Boy Socks', 'Outdoor inactive from s1', 'Kids Girl Swimwear', 'Young Girl Swimwear', 'Ladies Sport Bras', 'Ladies Sport Bottoms', 'Jersey fancy', 'Blazer', 'Baby Boy Woven', 'Kids Boy Swimwear', 'Young boy Swimwear', 'Kids Dress-up/Football', 'Sneakers small girl inactive from s2', 'Tops Woven', 'Blouse', 'Shorts DS', 'Boots', 'Kids Boy Big Acc', 'Boys Small Acc & Bags', 'Woven top', 'Young Girl Dresses', 'Knitwear Basic', 'Sneakers', 'Dress', 'Baby Girl Woven', 'Baby Girl Jersey Fancy', 'Jacket Casual', 'Kids Boy UW/NW', 'Kids Girl Jersey Fancy', 'Ladies Sport Acc', 'Baby Exclusive', 'Young Boy Shorts', 'EQ Divided Basics', 'Shorts & Skirts', 'Promotion/ Other /Offer', 'Kids Boy Trouser', 'Tops Fancy Jersey', 'Kids Girl Knitwear', 'Kids Boy Jersey Fancy', 'Heels', 'Woven Premium', 'Blouse & Dress', 'Kids Girl Trouser', 'Kids Girl Dresses', 'Projects Woven Bottoms', 'Conscious Exclusive', 'Shoes Other', 'Kids Girl Outdoor', 'Young Girl Jersey Fancy', 'Young Boy Outdoor', 'Young Girl Outdoor', 'Woven Occasion', 'Kids Boy Outdoor', 'Young Boy Jersey Fancy', 'Kids Boy Shoes', 'Ladies Sport Woven', 'Bottoms Girls', 'Tops Girls', 'Young Girl Big Acc', 'Young Boy Shoes', 'Young Boy Knitwear', 'Suit', 'Kids Girl Shoes', 'Jacket Smart', 'Projects', 'Kids Boy Knitwear', 'Woven bottoms inactive from S.7', 'Jersey/Knitwear Premium', 'Sneakers big girl inactive from s2', 'Suit jacket', 'Kids Boy Shirt', 'Jacket', 'Kids Girl Denim', 'Young Girl Trouser', 'Girls Small Acc/Bags', 'Shirt Extended inactive from s1', 'Tops Boys', 'Baby Girl Knitwear', 'Baby Shoes', 'Jackets', 'Jersey Occasion', 'Young Boy Big Acc', 'Kids Girl License', 'Baby Boy Jersey Fancy', 'Bottoms Boys', 'Local relevance', 'Dress-up Boys', 'Baby Boy Knitwear', 'Equatorial Assortment', 'Loungewear', 'Studio Collection', 'Small Acc. Jewellery & Other', 'Woven', 'Outwear & Blazers', 'Underwear Jersey Fancy inactive from s1', 'Special Collection', 'Take Care External', 'Equatorial', 'Divided+', 'OL Extended Sizes', 'Campaigns', 'Underwear Woven', 'Heavy Basic Jersey', 'EQ H&M Man', 'Young Girl Shoes', 'AK Other', 'Kids Boy Shorts', 'Skirts DS', 'Bottoms', 'Shopbasket Lingerie', 'Kids Boy License', 'Baby Girl Outdoor', 'Promotion / Other / Offer', 'Divided+ inactive from s.1', 'Tops & Bottoms Other', 'Take care', 'Girls Local Relevance', 'AK Dresses & Outdoor', 'Limited Edition', 'AK Tops Jersey & Woven', 'AK Tops Knitwear', 'Divided Swimwear', 'AK Bottoms', 'Girls Projects', 'On Demand', 'Test Ladies', 'Socks Wall', 'Jersey inactive from S.6', 'Boys Local Relevance', 'License', 'Price Items', 'Asia Assortment', 'Accessories Boys', 'Read & React', 'EQ Ladies Denim', 'Small Accessories Extended', 'Baby Boy Local Relevance', 'Kids Girl Exclusive', 'Baby Girl Local Relevance', 'EQ Divided Blue', 'Kids Boy Exclusive', 'Accessories Other', 'Blanks']\n\nClass index_name has 10 unique labels:\n['Ladieswear', 'Lingeries/Tights', 'Baby Sizes 50-98', 'Menswear', 'Ladies Accessories', 'Sport', 'Children Sizes 92-140', 'Divided', 'Children Sizes 134-170', 'Children Accessories, Swimwear']\n\nClass index_group_name has 5 unique labels:\n['Ladieswear', 'Baby/Children', 'Menswear', 'Sport', 'Divided']\n\nClass section_name has 56 unique labels:\n['Womens Everyday Basics', 'Womens Lingerie', 'Womens Nightwear, Socks & Tigh', 'Baby Essentials & Complements', 'Men Underwear', 'Mama', 'Womens Small accessories', 'Men H&M Sport', 'Kids Boy', 'Divided Basics', 'Girls Underwear & Basics', 'Mens Outerwear', 'Womens Big accessories', 'Divided Accessories', 'Womens Swimwear, beachwear', 'Divided Selected', 'Boys Underwear & Basics', 'Contemporary Street', 'Contemporary Casual', 'Men Accessories', 'Men Suits & Tailoring', 'Womens Everyday Collection', 'Men Shoes', 'Young Boy', 'H&M+', 'Divided Collection', 'Ladies Denim', 'Contemporary Smart', 'Womens Trend', 'Kids Outerwear', 'Young Girl', 'Womens Shoes', 'Womens Tailoring', 'Divided Projects', 'Denim Men', 'Men Other', 'Womens Jackets', 'Men Other 2', 'Baby Boy', 'Womens Casual', 'Kids Accessories, Swimwear & D', 'Ladies H&M Sport', 'Kids & Baby Shoes', 'Baby Girl', 'Kids Girl', 'Divided Complements Other', 'Womens Premium', 'Special Collections', 'Kids Sports', 'Men Project', 'Men Edition', 'Collaborations', 'Divided Asia keys', 'EQ Divided', 'Kids Local Relevance', 'Ladies Other']\n\nClass garment_group_name has 21 unique labels:\n['Jersey Basic', 'Under-, Nightwear', 'Socks and Tights', 'Jersey Fancy', 'Accessories', 'Trousers Denim', 'Outdoor', 'Shoes', 'Swimwear', 'Knitwear', 'Shirts', 'Trousers', 'Dressed', 'Shorts', 'Dresses Ladies', 'Skirts', 'Special Offers', 'Blouses', 'Unknown', 'Woven/Jersey/Knitted mix Baby', 'Dresses/Skirts girls']\n","output_type":"stream"}]},{"cell_type":"code","source":"class TrainDataset(Dataset):\n    def __init__(self, dataframe, target_columns):\n        self.dataframe = dataframe\n        self.target_columns = target_columns\n        self.prompt = 'The {column} of photo is {label}' # temporary, need to be changed\n            \n    def __len__(self):\n        return len(self.dataframe)\n    \n    def __getitem__(self, index):\n        row = self.dataframe.iloc[index]\n        image = Image.open(row['image_path'])\n        labels = []\n        for col in self.target_columns:\n            labels.append(row[col].strip())\n        \n        text_list = [self.prompt.format(column=' '.join(c.split('_')), label=l) for c, l in zip(self.target_columns, labels)]\n        text = '. '.join(text_list)\n        # select the labels from k columns to create the prompt\n        return {\n            'image': image,\n            'text': text,\n        }\n    \n# Define data collator for training dataset\ndef collate_fn(batch):\n#     print(batch)\n    texts = [item['text'] for item in batch]\n    images = [item['image'] for item in batch]\n    \n    # 处理文本\n    text_inputs = processor(\n        text=texts,\n        padding=True,\n        truncation=True,\n        return_tensors=\"pt\",\n    )\n#     print(text_inputs['input_ids'].shape)\n    # 处理图像\n    image_inputs = processor(\n        images=images,\n        return_tensors=\"pt\",\n    )\n    \n    # 合并文本和图像的输入\n    inputs = {\n        'input_ids': text_inputs['input_ids'],\n        'attention_mask': text_inputs['attention_mask'],\n        'pixel_values': image_inputs['pixel_values'],\n    }\n    \n    return inputs\n\n# 重复构造train_ds再连接起来\nfrom torch.utils.data import Dataset, ConcatDataset\n\nt_columns_list = [\n    [\"product_group_name\", \"product_type_name\", \"graphical_appearance_name\"],\n    [\"colour_group_name\", \"perceived_colour_value_name\", \"perceived_colour_master_name\"],\n    [\"department_name\", \"index_name\", \"index_group_name\"],\n    [\"section_name\", \"garment_group_name\"],\n] \ntrain_ds_list = []\nfor t_cols in t_columns_list:\n    train_ds = TrainDataset(train_df, t_cols)\n    train_ds_list.append(train_ds)\n\ntrain_ds = ConcatDataset(train_ds_list)\nprint(len(train_ds))\nfor i in range(0, 2):\n    print(train_ds[i])","metadata":{"execution":{"iopub.status.busy":"2024-10-09T07:56:23.496847Z","iopub.execute_input":"2024-10-09T07:56:23.497740Z","iopub.status.idle":"2024-10-09T07:56:23.522965Z","shell.execute_reply.started":"2024-10-09T07:56:23.497696Z","shell.execute_reply":"2024-10-09T07:56:23.521930Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"336320\n{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1166x1750 at 0x795F0F6CB0D0>, 'text': 'The product group name of photo is Garment Full body. The product type name of photo is Dress. The graphical appearance name of photo is Check'}\n{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1470x1750 at 0x795F0F6CBCA0>, 'text': 'The product group name of photo is Garment Upper body. The product type name of photo is Hoodie. The graphical appearance name of photo is Solid'}\n","output_type":"stream"}]},{"cell_type":"code","source":"class ValidateDataset(Dataset):\n    def __init__(self, dataframe, target_columns):\n        self.dataframe = dataframe\n        self.target_columns = target_columns\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, index):\n        row = self.dataframe.iloc[index]\n        image = Image.open(row['image_path'])\n        dp = {'image': image}\n        for col in self.target_columns:\n            dp[col] = row[col]\n        return dp\n\ndef collate_fn_val(batch):\n    batch_list = {}\n    for item in batch:\n        for k,v in item.items():\n            if k not in batch_list:\n                batch_list[k] = [v]\n            else:\n                batch_list[k].append(v)\n    return batch_list\n    \nval_ds = ValidateDataset(val_df, target_columns)\nfor i in range(2):\n    print(val_ds[i])","metadata":{"execution":{"iopub.status.busy":"2024-10-09T07:56:25.270435Z","iopub.execute_input":"2024-10-09T07:56:25.270838Z","iopub.status.idle":"2024-10-09T07:56:25.283285Z","shell.execute_reply.started":"2024-10-09T07:56:25.270799Z","shell.execute_reply":"2024-10-09T07:56:25.282361Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=722x1750 at 0x795F161AEEC0>, 'product_group_name': 'Garment Lower body', 'product_type_name': 'Trousers', 'graphical_appearance_name': 'Denim', 'colour_group_name': 'Blue', 'perceived_colour_value_name': 'Medium Dusty', 'perceived_colour_master_name': 'Blue', 'department_name': 'Young Boy Denim', 'index_name': 'Children Sizes 134-170', 'index_group_name': 'Baby/Children', 'section_name': 'Young Boy', 'garment_group_name': 'Trousers Denim'}\n{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1414x1750 at 0x795F0F6CB970>, 'product_group_name': 'Garment Upper body', 'product_type_name': 'Blazer', 'graphical_appearance_name': 'Solid', 'colour_group_name': 'Black', 'perceived_colour_value_name': 'Dark', 'perceived_colour_master_name': 'Black', 'department_name': 'Suit', 'index_name': 'Ladieswear', 'index_group_name': 'Ladieswear', 'section_name': 'Womens Tailoring', 'garment_group_name': 'Dressed'}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Inference","metadata":{"execution":{"iopub.status.busy":"2024-10-08T14:47:58.491238Z","iopub.execute_input":"2024-10-08T14:47:58.492260Z","iopub.status.idle":"2024-10-08T14:47:58.497849Z","shell.execute_reply.started":"2024-10-08T14:47:58.492211Z","shell.execute_reply":"2024-10-08T14:47:58.496508Z"}}},{"cell_type":"code","source":"# 需要推理的目标column name和每个column的unique label\ndef single_column_infer_fn(model, images, column, unique_labels):\n    '''\n    input: \n        model: CLIP model\n        processor: CLIP processor (global).\n        images: list of images\n        column: the specific column name\n        unique_labels: all possible unique labels in this column\n    output: \n        probs: classification probabilities\n    '''\n    infer_prompt = 'The {c} of photo is {label}' # temporary, need to be changed\n    col = ' '.join(column.split('_')) # replaced _ by space\n    unique_prompts = [infer_prompt.format(c=col, label=l) for l in unique_labels]\n#     print(unique_prompts)\n    device = model.device\n    inputs = processor(\n        text=unique_prompts,\n        images=images,\n        padding=True,\n        truncation=True,\n        return_tensors=\"pt\",\n    )\n    \n    input_ids = inputs['input_ids'].to(device)\n    attention_mask = inputs['attention_mask'].to(device)\n    pixel_values = inputs['pixel_values'].to(device)\n\n    with torch.no_grad():\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask if attention_mask is not None else None,\n            pixel_values=pixel_values\n        )\n        logits_per_image = outputs.logits_per_image\n    return logits_per_image","metadata":{"execution":{"iopub.status.busy":"2024-10-09T07:56:28.675151Z","iopub.execute_input":"2024-10-09T07:56:28.675550Z","iopub.status.idle":"2024-10-09T07:56:28.684124Z","shell.execute_reply.started":"2024-10-09T07:56:28.675512Z","shell.execute_reply":"2024-10-09T07:56:28.683042Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def compute_loss_and_eval(model, val_dl, columns_unique_labels):\n    '''\n    columns_unique_labels: {'column_name1': ['unique_label_name'], 'column_name2': ['unique_label_name']}.\n    val_dl: 'image', 'column1', 'column2' ...\n    '''\n    device = model.device\n    print(f'{len(columns_unique_labels)} column need to be evaluated')\n    acc_loss_dict = {}\n    for column,unique_labels in columns_unique_labels.items():\n        print(f'\\nEvaluation on column: {column}')\n        all_preds = []\n        all_labels = []\n        single_column_loss = 0\n        for batch in tqdm(val_dl):\n            logits_per_image = single_column_infer_fn(model, batch['image'], column, unique_labels)\n            \n            probs = F.softmax(logits_per_image, dim=-1)\n            preds = [unique_labels[idx] for idx in probs.argmax(dim=-1).cpu().numpy()]\n            labels = batch[column]\n#             is necessary to compute loss?\n            # 将字符串标签转换为索引\n            label_indices = torch.tensor([unique_labels.index(label) for label in labels], \n                                       device=device)\n            # 计算交叉熵损失\n            loss_fct = torch.nn.CrossEntropyLoss()\n            loss = loss_fct(logits_per_image, label_indices)\n#             print(len(batch[column]))\n            single_column_loss += loss.item() * len(batch[column])\n            all_preds.extend(preds)\n            all_labels.extend(labels)\n        accuracy = sum(p == l for p, l in zip(all_preds, all_labels)) / len(all_labels)\n        single_column_loss = single_column_loss / len(all_labels)\n        acc_loss_dict[column] = (accuracy, single_column_loss)\n        print(f'Model accuracy on {column}: {accuracy}')\n        print(f'Loss on {column}: {single_column_loss}')\n    return acc_loss_dict\n    ","metadata":{"execution":{"iopub.status.busy":"2024-10-09T07:56:35.193792Z","iopub.execute_input":"2024-10-09T07:56:35.194507Z","iopub.status.idle":"2024-10-09T07:56:35.204886Z","shell.execute_reply.started":"2024-10-09T07:56:35.194466Z","shell.execute_reply":"2024-10-09T07:56:35.203844Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# original model evaluation on validate dataset\nval_dl = DataLoader(val_ds, batch_size=256, collate_fn=collate_fn_val)\ntmp = compute_loss_and_eval(model, val_dl, columns_unique_labels)","metadata":{"execution":{"iopub.status.busy":"2024-10-09T08:01:51.749101Z","iopub.execute_input":"2024-10-09T08:01:51.749768Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"11 column need to be evaluated\n\nEvaluation on column: product_group_name\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/83 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bb487c4bbf940038706b28e7c6af744"}},"metadata":{}},{"name":"stdout","text":"Model accuracy on product_group_name: 0.5303044719314938\nLoss on product_group_name: 1.444383701452406\n\nEvaluation on column: product_type_name\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/83 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b8f131f8d174d6ba599fdcd7cc25071"}},"metadata":{}},{"name":"stdout","text":"Model accuracy on product_type_name: 0.3889153187440533\nLoss on product_type_name: 2.223591276081713\n\nEvaluation on column: graphical_appearance_name\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/83 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9449528e55474f578b21d529e1f233bc"}},"metadata":{}},{"name":"stdout","text":"Model accuracy on graphical_appearance_name: 0.07117031398667935\nLoss on graphical_appearance_name: 4.683136934558059\n\nEvaluation on column: colour_group_name\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/83 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e7e231351db40e9b24222b081ad5399"}},"metadata":{}},{"name":"stdout","text":"Model accuracy on colour_group_name: 0.3423882017126546\nLoss on colour_group_name: 2.419921142821307\n\nEvaluation on column: perceived_colour_value_name\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/83 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0807625b0e24472f8ebd5905239268e9"}},"metadata":{}},{"name":"stdout","text":"Model accuracy on perceived_colour_value_name: 0.19757373929590866\nLoss on perceived_colour_value_name: 2.039835844076213\n\nEvaluation on column: perceived_colour_master_name\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/83 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8d9a8adaa9b45789b6680b6ac14c8d8"}},"metadata":{}},{"name":"stdout","text":"Model accuracy on perceived_colour_master_name: 0.49700285442435777\nLoss on perceived_colour_master_name: 1.8545446902655285\n\nEvaluation on column: department_name\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/83 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78a33de8bd6944e087de8c9a6725dc9b"}},"metadata":{}},{"name":"stdout","text":"Model accuracy on department_name: 0.06745956232159847\nLoss on department_name: 5.418500460997635\n\nEvaluation on column: index_name\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/83 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e67bfee1d0c4e8a8699b8c6f5d72753"}},"metadata":{}},{"name":"stdout","text":"Model accuracy on index_name: 0.32331113225499525\nLoss on index_name: 2.577958757326106\n\nEvaluation on column: index_group_name\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/83 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51e79a69066842f994ea4dd6d4e267a6"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Training loop func","metadata":{}},{"cell_type":"code","source":"import wandb\n\ndef train(\n    model=None, \n    train_ds=None, \n    val_ds=None, \n    n_epochs=3, \n    batch_size=128, \n    lr=1e-5, \n    infer_fn=None,\n    project_name=\"H&M\",  # wandb project name\n    run_name='10.09',  # optional wandb run name\n):\n    # Initialize wandb\n    wandb.init(\n        project=project_name,\n        name=run_name,\n        config={\n            \"learning_rate\": lr,\n            \"epochs\": n_epochs,\n            \"batch_size\": batch_size\n        }\n    )\n\n    train_dl = DataLoader(train_ds, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n    val_dl = DataLoader(val_ds, batch_size=batch_size, collate_fn=collate_fn_val)\n    optimizer = AdamW(model.parameters(), lr=lr)\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    \n    global_step = 0\n    print('Training Begin...')\n    \n    for epoch in range(n_epochs):\n        model.train()\n        running_loss = 0.0    \n        for batch in tqdm(train_dl):\n            input_ids = batch['input_ids'].to(device)\n            pixel_values = batch['pixel_values'].to(device)\n            \n            outputs = model(input_ids=input_ids, pixel_values=pixel_values)\n            logits_per_image = outputs.logits_per_image\n            logits_per_text = outputs.logits_per_text\n            \n            labels = torch.arange(logits_per_image.size(0)).to(device)\n            loss_img = F.cross_entropy(logits_per_image, labels)\n            loss_txt = F.cross_entropy(logits_per_text, labels)\n            loss = (loss_img + loss_txt) / 2\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n            global_step += 1\n            \n            # Log training loss every 100 steps\n            if global_step % 100 == 0:\n                wandb.log({\n                    \"train/loss\": running_loss / 100,\n                    \"train/step\": global_step\n                })\n                running_loss = 0.0\n        \n        model.eval()\n        acc_loss_dict = compute_loss_and_eval(model, val_dl, columns_unique_labels)\n        \n        # Log validation metrics\n        log_dict = {\n            f\"val/epoch\": epoch,\n            f\"val/loss\": loss.item()\n        }\n        \n        # Log individual column metrics\n        for column, (accuracy, single_column_loss) in acc_loss_dict.items():\n            log_dict.update({\n                f\"val/loss_{column}\": single_column_loss,\n                f\"val/accuracy_{column}\": accuracy\n            })\n        \n        wandb.log(log_dict)\n        \n        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n        print(f\"Evaluation results on val_dl:\\n {acc_loss_dict}\")\n    \n    # Close wandb run\n    wandb.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ready to Train","metadata":{}},{"cell_type":"code","source":"train_arg = {\n    'model': peft_model,\n    'train_ds': train_ds,\n    'val_ds': val_ds,\n    'n_epochs': 3,\n    'batch_size': 256,\n    'lr': 1e-5,\n    'infer_fn': None,\n}\ntorch.cuda.empty_cache()\ntrain(**train_arg)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}