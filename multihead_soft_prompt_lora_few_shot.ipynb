{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "# from src import util\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from peft import LoraConfig, get_peft_model, PromptTuningConfig, PeftType\n",
    "import wandb\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_paths_and_labels_from_df(df, data_dir):\n",
    "    article_ids = df[\"article_id\"].values\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for article_id in article_ids:\n",
    "        image_path = f\"{data_dir}/images/0{str(article_id)[:2]}/0{article_id}.jpg\"\n",
    "        # Check if the image file exists\n",
    "        if os.path.exists(image_path):\n",
    "            image_paths.append(image_path)\n",
    "            # Add corresponding label only if the image exists\n",
    "            labels.append(df[df[\"article_id\"] == article_id])\n",
    "\n",
    "    return image_paths, labels\n",
    "\n",
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_paths, processor=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.processor = processor\n",
    "        self.image_ids = []\n",
    "\n",
    "        for image_path in self.image_paths:\n",
    "            if not os.path.exists(image_path):\n",
    "                raise FileNotFoundError(f\"Image {image_path} not found.\")\n",
    "            else:\n",
    "                image_id = int(image_path.split(\"/\")[-1].split(\".\")[0])\n",
    "                self.image_ids.append(image_id)\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx])\n",
    "        if self.processor is not None:\n",
    "            inputs = self.processor(images=image, return_tensors=\"pt\", padding=True)\n",
    "            image = inputs[\"pixel_values\"][0]\n",
    "        return image, self.image_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", \n",
    "                                  cache_dir=\"model\", local_files_only=False)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\", \n",
    "                                          cache_dir=\"model\", local_files_only=False)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = \"data\"\n",
    "# data_dir = \"/kaggle/input/h-and-m-personalized-fashion-recommendations\"\n",
    "data_dir = \"/Users/tobiaspeihengli/Downloads/DD2430/h-and-m-personalized-fashion-recommendations\"\n",
    "# articles_filtered: remove articles with no images\n",
    "articles = pd.read_csv(f\"{data_dir}/articles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map from article_id to df index\n",
    "article_id_to_idx = {article_id: idx for idx, article_id in enumerate(articles[\"article_id\"])}\n",
    "\n",
    "# get all classes of the dataframe\n",
    "class_names = articles.columns.tolist()\n",
    "label_names = dict()\n",
    "label_names_to_idx = dict()\n",
    "for class_name in class_names:\n",
    "    label_names[class_name] = articles[class_name].unique()\n",
    "    label_names_to_idx[class_name] = {label_name: idx for idx, label_name in enumerate(label_names[class_name])}\n",
    "\n",
    "article_ids = label_names[\"article_id\"]\n",
    "selected_class_names = [\"product_group_name\", \"product_type_name\", \"graphical_appearance_name\", \"colour_group_name\", \"perceived_colour_value_name\", \"perceived_colour_master_name\", \"department_name\", \"index_name\", \"index_group_name\", \"section_name\", \"garment_group_name\"]\n",
    "# selected_class_names = [\"product_type_name\", \"graphical_appearance_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped by product_code\n",
    "grouped = articles.groupby(\"product_code\")\n",
    "groups = [group for _, group in grouped]\n",
    "\n",
    "# split 0.8/0.1/0.1\n",
    "train_groups, test_groups = train_test_split(groups, test_size=0.2, random_state=42) \n",
    "val_groups, test_groups = train_test_split(test_groups, test_size=0.5, random_state=42) \n",
    "\n",
    "train_df = pd.concat(train_groups)\n",
    "val_df = pd.concat(val_groups)\n",
    "test_df = pd.concat(test_groups)\n",
    "\n",
    "# 256 for local test\n",
    "# train_df = train_df.sample(256)\n",
    "# val_df = val_df.sample(256)\n",
    "# test_df = test_df.sample(256)\n",
    "\n",
    "print(f\"{len(train_df)=} {len(val_df)=} {len(test_df)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_paths, train_labels = get_image_paths_and_labels_from_df(train_df, data_dir)\n",
    "# val_paths, val_labels = get_image_paths_and_labels_from_df(val_df, data_dir)\n",
    "# test_paths, test_labels = get_image_paths_and_labels_from_df(test_df, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# train_dataset = ImageDataset(train_paths, processor)\n",
    "# train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True) # Don't forget to change the batch size to 1\n",
    "\n",
    "# val_dataset = ImageDataset(val_paths, processor)\n",
    "# val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "# test_dataset = ImageDataset(test_paths, processor)\n",
    "# test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiOutputLayer(torch.nn.Module):\n",
    "    def __init__(self, input_size, inter_size, output_size):\n",
    "        super(MultiOutputLayer, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, inter_size)\n",
    "        self.fc2 = torch.nn.Linear(inter_size, output_size)\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.act = torch.nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiOutputClipModel(torch.nn.Module):\n",
    "#     def __init__(self, clip_model, class_names, vision_hidden_size, inter_size, output_size):\n",
    "#         super(MultiOutputClipModel, self).__init__()\n",
    "#         self.clip_model = clip_model\n",
    "#         self.class_names = class_names\n",
    "#         self.output_layers = torch.nn.ModuleDict({\n",
    "#             class_name: MultiOutputLayer(vision_hidden_size, inter_size, output_size)\n",
    "#             for class_name in self.class_names\n",
    "#         })\n",
    "    \n",
    "#     def forward(\n",
    "#         self,\n",
    "#         text_input_dict,\n",
    "#         pixel_values,\n",
    "#         # position_ids = None,\n",
    "#         output_attentions = None,\n",
    "#         output_hidden_states = None,\n",
    "#         return_dict = None,\n",
    "#     ):\n",
    "\n",
    "#         output_attentions = output_attentions if output_attentions is not None else self.clip_model.config.output_attentions\n",
    "#         output_hidden_states = (\n",
    "#             output_hidden_states if output_hidden_states is not None else self.clip_model.config.output_hidden_states\n",
    "#         )\n",
    "#         return_dict = return_dict if return_dict is not None else self.clip_model.config.use_return_dict\n",
    "\n",
    "#         vision_outputs = self.clip_model.vision_model(\n",
    "#             pixel_values=pixel_values,\n",
    "#             output_attentions=output_attentions,\n",
    "#             output_hidden_states=output_hidden_states,\n",
    "#             return_dict=return_dict,\n",
    "#         )\n",
    "\n",
    "#         vision_embeds = vision_outputs[1]\n",
    "#         vision_embeds_dict = {\n",
    "#             class_name: output_layer(vision_embeds) \n",
    "#                 for class_name, output_layer in self.output_layers.items()\n",
    "#         }\n",
    "\n",
    "#         text_outputs_dict = {\n",
    "#             class_name: self.clip_model.text_model(\n",
    "#                 input_ids=text_input_dict[class_name][\"input_ids\"],\n",
    "#                 attention_mask=text_input_dict[class_name][\"attention_mask\"],\n",
    "#                 # position_ids=position_ids,\n",
    "#                 output_attentions=output_attentions,\n",
    "#                 output_hidden_states=output_hidden_states,\n",
    "#                 return_dict=return_dict,\n",
    "#             ) for class_name in self.class_names\n",
    "#         }\n",
    "\n",
    "#         text_embeds_dict = {\n",
    "#             class_name: self.clip_model.text_projection(text_outputs[1])\n",
    "#                 for class_name, text_outputs in text_outputs_dict.items()\n",
    "#         }\n",
    "\n",
    "#         logits_per_image_dict = {\n",
    "#             class_name: vision_embeds_dict[class_name] @ text_embeds_dict[class_name].T\n",
    "#                 for class_name in self.output_layers.keys()\n",
    "#         }\n",
    "\n",
    "#         return logits_per_image_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiOutputClipModel(torch.nn.Module):\n",
    "    def __init__(self, clip_model, class_names, vision_hidden_size, inter_size, output_size, num_virtual_tokens):\n",
    "        super(MultiOutputClipModel, self).__init__()\n",
    "        self.clip_model = clip_model\n",
    "        self.class_names = class_names\n",
    "        self.output_layers = torch.nn.ModuleDict({\n",
    "            class_name: MultiOutputLayer(vision_hidden_size, inter_size, output_size)\n",
    "            for class_name in self.class_names\n",
    "        })\n",
    "\n",
    "        # Soft prompt embeddings per class\n",
    "        self.num_virtual_tokens = num_virtual_tokens\n",
    "        embedding_dim = self.clip_model.text_model.embeddings.token_embedding.embedding_dim\n",
    "        self.soft_prompt_embeddings = torch.nn.ParameterDict({\n",
    "            class_name: torch.nn.Parameter(torch.randn(num_virtual_tokens, embedding_dim))\n",
    "            for class_name in self.class_names\n",
    "        })\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        text_input_dict,\n",
    "        pixel_values,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.clip_model.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.clip_model.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.clip_model.config.use_return_dict\n",
    "\n",
    "        # Vision processing remains the same\n",
    "        vision_outputs = self.clip_model.vision_model(\n",
    "            pixel_values=pixel_values,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        vision_embeds = vision_outputs[1]\n",
    "        vision_embeds_dict = {\n",
    "            class_name: output_layer(vision_embeds) \n",
    "                for class_name, output_layer in self.output_layers.items()\n",
    "        }\n",
    "\n",
    "        # Text processing with soft prompts\n",
    "        text_embeds_dict = {}\n",
    "        for class_name in self.class_names:\n",
    "            text_inputs = text_input_dict[class_name]\n",
    "            input_ids = text_inputs[\"input_ids\"]\n",
    "            attention_mask = text_inputs[\"attention_mask\"]\n",
    "\n",
    "            input_embeds = self.clip_model.text_model.embeddings.token_embedding(input_ids)\n",
    "            batch_size = input_embeds.shape[0]\n",
    "\n",
    "            # Expand and Concatenate\n",
    "            soft_prompt = self.soft_prompt_embeddings[class_name].unsqueeze(0).expand(batch_size, -1, -1)\n",
    "            input_embeds = torch.cat([soft_prompt, input_embeds], dim=1)\n",
    "\n",
    "            # Adjust attention mask\n",
    "            soft_prompt_mask = torch.ones(batch_size, self.num_virtual_tokens).to(attention_mask.device)\n",
    "            attention_mask = torch.cat([soft_prompt_mask, attention_mask], dim=1)\n",
    "            valid_token_indices = (attention_mask.sum(dim=-1) - 1).long()\n",
    "            attention_mask = attention_mask[:, None, None, :]\n",
    "            attention_mask = attention_mask.expand(-1, 1, attention_mask.size(-1), attention_mask.size(-1))\n",
    "\n",
    "            encoder_outputs = self.clip_model.text_model.encoder(\n",
    "                inputs_embeds=input_embeds,\n",
    "                attention_mask=attention_mask,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "            last_hidden_state = encoder_outputs[0]\n",
    "            # print(last_hidden_state.shape)\n",
    "\n",
    "            pooled_output = last_hidden_state[torch.arange(last_hidden_state.shape[0]), valid_token_indices]\n",
    "\n",
    "            pooled_output = self.clip_model.text_model.final_layer_norm(pooled_output)\n",
    "\n",
    "            text_embeds = self.clip_model.text_projection(pooled_output)\n",
    "\n",
    "            text_embeds_dict[class_name] = text_embeds\n",
    "\n",
    "\n",
    "        # Compute logits\n",
    "        logits_per_image_dict = {\n",
    "            class_name: vision_embeds_dict[class_name] @ text_embeds_dict[class_name].T\n",
    "                for class_name in self.output_layers.keys()\n",
    "        }\n",
    "\n",
    "        return logits_per_image_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom criterion: cross entropy loss across all classes\n",
    "class MultiOutputClipCriterion(torch.nn.Module):\n",
    "    def __init__(self, class_names):\n",
    "        super(MultiOutputClipCriterion, self).__init__()\n",
    "        self.class_names = class_names\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, logits_dict, labels_dict):\n",
    "        loss = 0\n",
    "        for class_name in self.class_names:\n",
    "            logits = logits_dict[class_name]\n",
    "            labels = labels_dict[class_name]\n",
    "            loss += self.criterion(logits, labels)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all parameters in model\n",
    "\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                  # Low-rank dimension (adjustable)\n",
    "    lora_alpha=32,          # Scaling factor (adjustable)\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\"],  # Specify which layers to apply LoRA to\n",
    "    lora_dropout=0.05,       # Dropout rate (optional)\n",
    "    bias=\"none\",            # Whether to include biases (\"none\", \"all\", \"lora_only\")\n",
    "    task_type=\"classification\"  # Task type (\"classification\" or \"regression\")\n",
    ")\n",
    "\n",
    "# Apply LoRA to the CLIP model\n",
    "model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def get_few_shot_train_df(articles, class_name, n_few_shot):\n",
    "    labels = articles[class_name].unique()\n",
    "    few_shot_samples = []\n",
    "    for label in labels:\n",
    "        label_articles = articles[articles[class_name] == label]\n",
    "        samples = label_articles.sample(n=min(n_few_shot, len(label_articles)), random_state=42)\n",
    "        few_shot_samples.append(samples)\n",
    "    train_df_class = pd.concat(few_shot_samples)\n",
    "    return train_df_class\n",
    "\n",
    "def get_class_df(df, class_name):\n",
    "    return df[df[class_name].notna()]\n",
    "\n",
    "num_epochs = 10  # Adjust as needed\n",
    "num_virtual_tokens = 16  # You can adjust this number\n",
    "\n",
    "n_few_shot_list = [2**i for i in range(0, 5)]  # [1, 2, 4, 8, ..., 1024]\n",
    "\n",
    "for class_name in selected_class_names:\n",
    "    print(f\"Starting experiments for class: {class_name}\")\n",
    "    # Prepare label names and indices for this class\n",
    "    labels = label_names[class_name]\n",
    "    label_names_class = {class_name: labels}\n",
    "    label_names_to_idx_class = {class_name: {label: idx for idx, label in enumerate(labels)}}\n",
    "\n",
    "    # Prepare validation and test datasets for this class\n",
    "    val_df_class = get_class_df(val_df, class_name)\n",
    "    test_df_class = get_class_df(test_df, class_name)\n",
    "    val_paths, val_labels = get_image_paths_and_labels_from_df(val_df_class, data_dir)\n",
    "    test_paths, test_labels = get_image_paths_and_labels_from_df(test_df_class, data_dir)\n",
    "    val_dataset = ImageDataset(val_paths, processor)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "    test_dataset = ImageDataset(test_paths, processor)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "    # Text inputs for the model\n",
    "    text_input_dict = {\n",
    "        class_name: processor(text=[f\"{label}\" for label in labels], \n",
    "                              return_tensors=\"pt\", padding=True).to(device)\n",
    "    }\n",
    "\n",
    "    for n_few_shot in n_few_shot_list:\n",
    "        print(f\"\\nTraining with n_few_shot = {n_few_shot}\")\n",
    "        # Prepare training dataset for this class and n_few_shot\n",
    "        train_df_class = get_few_shot_train_df(train_df, class_name, n_few_shot)\n",
    "        train_paths, train_labels = get_image_paths_and_labels_from_df(train_df_class, data_dir)\n",
    "        train_dataset = ImageDataset(train_paths, processor)\n",
    "        train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=n_few_shot, shuffle=True)\n",
    "\n",
    "        # Initialize model for each n_few_shot\n",
    "        mo_model = MultiOutputClipModel(model, [class_name], 768, 128, 512, num_virtual_tokens).to(device)\n",
    "        mo_model.train()\n",
    "\n",
    "        criteria = MultiOutputClipCriterion(class_names=[class_name])\n",
    "        optimizer = torch.optim.AdamW(mo_model.parameters(), lr=1e-4)\n",
    "        step = 0\n",
    "\n",
    "        # Initialize wandb with n_few_shot and class_name in config\n",
    "        wandb.init(project=f\"Few_shot_experiment_{class_name}\",\n",
    "                   name=f\"n_few_shot_{n_few_shot}\",\n",
    "                   config={'n_few_shot': n_few_shot, 'class_name': class_name},\n",
    "                   reinit=True)\n",
    "\n",
    "        def validate(model, dataloader, criteria, device, text_inputs, class_names):\n",
    "            model.eval()\n",
    "            total_loss = 0.0\n",
    "            total_correct = {class_name: 0 for class_name in class_names}\n",
    "            total_samples = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for images, image_ids in tqdm(dataloader):\n",
    "                    images = images.to(device)\n",
    "                    logits_per_image_dict = model(pixel_values=images, text_input_dict=text_inputs)\n",
    "\n",
    "                    # Get true labels from image_ids\n",
    "                    true_labels_dict = {\n",
    "                        class_name: [label_names_to_idx_class[class_name][articles.loc[article_id_to_idx[image_id.item()], class_name]] \n",
    "                                   for image_id in image_ids]\n",
    "                        for class_name in class_names\n",
    "                    }\n",
    "                    true_labels_dict = {class_name: torch.tensor(true_labels).to(device)\n",
    "                                        for class_name, true_labels in true_labels_dict.items()}\n",
    "                    \n",
    "                    # Compute loss\n",
    "                    loss = criteria(logits_per_image_dict, true_labels_dict)\n",
    "                    total_loss += loss.item() * images.size(0)\n",
    "\n",
    "                    # Predictions and accuracy\n",
    "                    total_samples += images.size(0)\n",
    "                    for class_name in class_names:\n",
    "                        _, preds = torch.max(logits_per_image_dict[class_name], dim=1)\n",
    "                        total_correct[class_name] += (preds == true_labels_dict[class_name]).sum().item()\n",
    "\n",
    "            avg_loss = total_loss / total_samples\n",
    "            accuracy = {class_name: total_correct[class_name] / total_samples for class_name in class_names}\n",
    "            return avg_loss, accuracy\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            mo_model.train()\n",
    "            total_loss = 0.0\n",
    "            total_correct = 0\n",
    "            total_samples = 0\n",
    "\n",
    "            for images, image_ids in tqdm(train_dataloader):\n",
    "                images = images.to(device)\n",
    "                logits_per_image_dict = mo_model(pixel_values=images, text_input_dict=text_input_dict)\n",
    "\n",
    "                # Get true labels from image_ids\n",
    "                true_labels_dict = {\n",
    "                    class_name: [label_names_to_idx_class[class_name][articles.loc[article_id_to_idx[image_id.item()], class_name]] \n",
    "                               for image_id in image_ids]\n",
    "                }\n",
    "                true_labels_dict = {class_name: torch.tensor(true_labels).to(device) \n",
    "                                    for class_name, true_labels in true_labels_dict.items()}\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = criteria(logits_per_image_dict, true_labels_dict)\n",
    "                total_loss += loss.item() * images.size(0)\n",
    "\n",
    "                # Predictions and accuracy\n",
    "                correct = 0\n",
    "                total_samples += images.size(0)\n",
    "                for class_name in [class_name]:\n",
    "                    _, preds = torch.max(logits_per_image_dict[class_name], dim=1)\n",
    "                    correct += (preds == true_labels_dict[class_name]).sum().item()\n",
    "                total_correct += correct\n",
    "\n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # log the loss and accuracy to wandb\n",
    "                wandb.log({\"train_loss\": loss.item(), \"train_accuracy\": correct / images.size(0)},\n",
    "                          step=step)\n",
    "                step += 1\n",
    "\n",
    "            avg_loss = total_loss / total_samples\n",
    "            accuracy = total_correct / total_samples\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        # Validate after each epoch\n",
    "        val_loss, val_accuracy_dict = validate(mo_model, val_dataloader, criteria, device, text_input_dict, [class_name])\n",
    "        val_accuracy = val_accuracy_dict[class_name]\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "        # Log to wandb\n",
    "        log_dict = {\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_accuracy,\n",
    "            \"n_few_shot\": n_few_shot\n",
    "        }\n",
    "        wandb.log(log_dict, step=step)\n",
    "\n",
    "        wandb.finish()\n",
    "\n",
    "        print(f\"Finished training for n_few_shot = {n_few_shot} for class: {class_name}\")\n",
    "\n",
    "    print(f\"Completed all experiments for class: {class_name}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# n_few_shot = 4  # Adjust the number of few-shot examples per label\n",
    "\n",
    "# def get_few_shot_train_df(articles, class_name, n_few_shot):\n",
    "#     labels = articles[class_name].unique()\n",
    "#     few_shot_samples = []\n",
    "#     for label in labels:\n",
    "#         label_articles = articles[articles[class_name] == label]\n",
    "#         samples = label_articles.sample(n=min(n_few_shot, len(label_articles)), random_state=42)\n",
    "#         few_shot_samples.append(samples)\n",
    "#     train_df_class = pd.concat(few_shot_samples)\n",
    "#     return train_df_class\n",
    "\n",
    "# def get_class_df(df, class_name):\n",
    "#     return df[df[class_name].notna()]\n",
    "\n",
    "# num_epochs = 1  # Adjust as needed\n",
    "# num_virtual_tokens = 16  # You can adjust this number\n",
    "\n",
    "# for class_name in selected_class_names:\n",
    "#     print(f\"Training for class: {class_name}\")\n",
    "#     # Prepare label names and indices for this class\n",
    "#     labels = label_names[class_name]\n",
    "#     label_names_class = {class_name: labels}\n",
    "#     label_names_to_idx_class = {class_name: {label: idx for idx, label in enumerate(labels)}}\n",
    "\n",
    "#     # Prepare training, validation, and test datasets for this class\n",
    "#     train_df_class = get_few_shot_train_df(train_df, class_name, n_few_shot)\n",
    "#     val_df_class = get_class_df(val_df, class_name)\n",
    "#     test_df_class = get_class_df(test_df, class_name)\n",
    "\n",
    "#     train_paths, train_labels = get_image_paths_and_labels_from_df(train_df_class, data_dir)\n",
    "#     val_paths, val_labels = get_image_paths_and_labels_from_df(val_df_class, data_dir)\n",
    "#     test_paths, test_labels = get_image_paths_and_labels_from_df(test_df_class, data_dir)\n",
    "\n",
    "#     train_dataset = ImageDataset(train_paths, processor)\n",
    "#     train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True) # Don't forget to change the batch size to 1\n",
    "\n",
    "#     val_dataset = ImageDataset(val_paths, processor)\n",
    "#     val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "#     test_dataset = ImageDataset(test_paths, processor)\n",
    "#     test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "#     mo_model = MultiOutputClipModel(model, [class_name], 768, 128, 512, num_virtual_tokens).to(device)\n",
    "#     mo_model.train()\n",
    "\n",
    "#     text_input_dict = {\n",
    "#         class_name: processor(text=[f\"{label}\" for label in labels], \n",
    "#                               return_tensors=\"pt\", padding=True).to(device)\n",
    "#     }\n",
    "\n",
    "#     criteria = MultiOutputClipCriterion(class_names=[class_name])\n",
    "#     optimizer = torch.optim.AdamW(mo_model.parameters(), lr=1e-4)\n",
    "#     step = 0\n",
    "#     wandb.init(project=f\"Few_shot_experiment_{class_name}\")\n",
    "\n",
    "#     def validate(model, dataloader, criteria, device, text_inputs, class_names):\n",
    "#         model.eval()\n",
    "#         total_loss = 0.0\n",
    "#         total_correct = {class_name: 0 for class_name in class_names}\n",
    "#         total_samples = 0\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             for images, image_ids in tqdm(dataloader):\n",
    "#                 images = images.to(device)\n",
    "#                 logits_per_image_dict = model(pixel_values=images, text_input_dict=text_inputs)\n",
    "\n",
    "#                 # Get true labels from image_ids\n",
    "#                 true_labels_dict = {\n",
    "#                     class_name: [label_names_to_idx_class[class_name][articles.loc[article_id_to_idx[image_id.item()], class_name]] \n",
    "#                                for image_id in image_ids]\n",
    "#                     for class_name in class_names\n",
    "#                 }\n",
    "#                 true_labels_dict = {class_name: torch.tensor(true_labels).to(device)\n",
    "#                                     for class_name, true_labels in true_labels_dict.items()}\n",
    "                \n",
    "#                 # Compute loss\n",
    "#                 loss = criteria(logits_per_image_dict, true_labels_dict)\n",
    "#                 total_loss += loss.item() * images.size(0)\n",
    "\n",
    "#                 # Predictions and accuracy\n",
    "#                 total_samples += images.size(0)\n",
    "#                 for class_name in class_names:\n",
    "#                     _, preds = torch.max(logits_per_image_dict[class_name], dim=1)\n",
    "#                     total_correct[class_name] += (preds == true_labels_dict[class_name]).sum().item()\n",
    "\n",
    "#         avg_loss = total_loss / total_samples\n",
    "#         accuracy = {class_name: total_correct[class_name] / total_samples for class_name in class_names}\n",
    "#         return avg_loss, accuracy\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         mo_model.train()\n",
    "#         total_loss = 0.0\n",
    "#         total_correct = 0\n",
    "#         total_samples = 0\n",
    "\n",
    "#         for images, image_ids in tqdm(train_dataloader):\n",
    "#             images = images.to(device)\n",
    "#             logits_per_image_dict = mo_model(pixel_values=images, text_input_dict=text_input_dict)\n",
    "\n",
    "#             # Get true labels from image_ids\n",
    "#             true_labels_dict = {\n",
    "#                 class_name: [label_names_to_idx_class[class_name][articles.loc[article_id_to_idx[image_id.item()], class_name]] \n",
    "#                            for image_id in image_ids]\n",
    "#             }\n",
    "#             true_labels_dict = {class_name: torch.tensor(true_labels).to(device) \n",
    "#                                 for class_name, true_labels in true_labels_dict.items()}\n",
    "            \n",
    "#             # Compute loss\n",
    "#             loss = criteria(logits_per_image_dict, true_labels_dict)\n",
    "#             total_loss += loss.item() * images.size(0)\n",
    "\n",
    "#             # Predictions and accuracy\n",
    "#             correct = 0\n",
    "#             total_samples += images.size(0)\n",
    "#             for class_name in [class_name]:\n",
    "#                 _, preds = torch.max(logits_per_image_dict[class_name], dim=1)\n",
    "#                 correct += (preds == true_labels_dict[class_name]).sum().item()\n",
    "#             total_correct += correct\n",
    "\n",
    "#             # Backward pass\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             # log the loss and accuracy to wandb\n",
    "#             wandb.log({\"train_loss\": loss.item(), \"train_accuracy\": correct / images.size(0)},\n",
    "#                       step=step)\n",
    "#             step += 1\n",
    "\n",
    "#         avg_loss = total_loss / total_samples\n",
    "#         accuracy = total_correct / total_samples\n",
    "#         print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "#         # Validate after each epoch\n",
    "#         val_loss, val_accuracy_dict = validate(mo_model, val_dataloader, criteria, device, text_input_dict, [class_name])\n",
    "#         val_accuracy = val_accuracy_dict[class_name]\n",
    "#         print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "#         # Log to wandb\n",
    "#         log_dict = {\n",
    "#             \"val_loss\": val_loss,\n",
    "#             \"val_accuracy\": val_accuracy\n",
    "#         }\n",
    "#         wandb.log(log_dict, step=step)\n",
    "\n",
    "#     wandb.finish()\n",
    "\n",
    "#     test_loss, test_accuracy_dict = validate(\n",
    "#         mo_model, test_dataloader, criteria, device, text_input_dict, [class_name]\n",
    "#     )\n",
    "\n",
    "#     print(f\"Test Loss: {test_loss:.4f}\")\n",
    "#     print(f\"Test Accuracy: {test_accuracy_dict[class_name]:.4f}\")\n",
    "\n",
    "#     print(f\"Finished training for class: {class_name}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mo_model = MultiOutputClipModel(model, selected_class_names, 768, 128, 512).to(device)\n",
    "# mo_model.train()\n",
    "\n",
    "# num_virtual_tokens = 16  # You can adjust this number\n",
    "# mo_model = MultiOutputClipModel(model, selected_class_names, 768, 128, 512, num_virtual_tokens).to(device)\n",
    "# mo_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all trainable parameters in mo_model\n",
    "# for name, param in mo_model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate text input\n",
    "# text_input_dict = {\n",
    "#     class_name: processor(text=[f\"{label}\" for label in label_names[class_name]], \n",
    "#                           return_tensors=\"pt\", padding=True).to(device)\n",
    "#     for class_name in selected_class_names\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 1  # Adjust as needed\n",
    "# criteria = MultiOutputClipCriterion(class_names=selected_class_names)\n",
    "# optimizer = torch.optim.AdamW(mo_model.parameters(), lr=1e-4)\n",
    "# step = 0\n",
    "# wandb.init(project=\"Multi_head_lora_prompt_tune_experiment256samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def validate(model, dataloader, criteria, device, text_inputs, class_names):\n",
    "#     model.eval()\n",
    "#     total_loss = 0.0\n",
    "#     total_correct = {class_name: 0 for class_name in class_names}\n",
    "#     total_samples = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for images, image_ids in tqdm(dataloader):\n",
    "#             images = images.to(device)\n",
    "#             logits_per_image_dict = model(pixel_values=images, text_input_dict=text_inputs)\n",
    "\n",
    "#             # Get true labels from image_ids\n",
    "#             true_labels_dict = {\n",
    "#                 class_name: [label_names_to_idx[class_name][articles.loc[article_id_to_idx[image_id.item()], class_name]] \n",
    "#                            for image_id in image_ids]\n",
    "#                 for class_name in class_names\n",
    "#             }\n",
    "#             true_labels_dict = {class_name: torch.tensor(true_labels).to(device)\n",
    "#                                 for class_name, true_labels in true_labels_dict.items()}\n",
    "            \n",
    "#             # Compute loss\n",
    "#             loss = criteria(logits_per_image_dict, true_labels_dict)\n",
    "#             total_loss += loss.item() * images.size(0)\n",
    "\n",
    "#             # Predictions and accuracy\n",
    "#             total_samples += images.size(0)\n",
    "#             for class_name in class_names:\n",
    "#                 _, preds = torch.max(logits_per_image_dict[class_name], dim=1)\n",
    "#                 total_correct[class_name] += (preds == true_labels_dict[class_name]).sum().item()\n",
    "\n",
    "#     avg_loss = total_loss / total_samples / len(class_names)\n",
    "#     accuracy = {class_name: total_correct[class_name] / total_samples for class_name in class_names}\n",
    "#     return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(num_epochs):\n",
    "#     mo_model.train()\n",
    "#     total_loss = 0.0\n",
    "#     total_correct = 0\n",
    "#     total_samples = 0\n",
    "\n",
    "#     for images, image_ids in tqdm(train_dataloader):\n",
    "#         images = images.to(device)\n",
    "#         logits_per_image_dict = mo_model(pixel_values=images, text_input_dict=text_input_dict)\n",
    "\n",
    "#         # Get true labels from image_ids\n",
    "#         true_labels_dict = {\n",
    "#             class_name: [label_names_to_idx[class_name][articles.loc[article_id_to_idx[image_id.item()], class_name]] \n",
    "#                        for image_id in image_ids]\n",
    "#             for class_name in selected_class_names\n",
    "#         }\n",
    "#         true_labels_dict = {class_name: torch.tensor(true_labels).to(device) \n",
    "#                             for class_name, true_labels in true_labels_dict.items()}\n",
    "        \n",
    "#         # Compute loss\n",
    "#         loss = criteria(logits_per_image_dict, true_labels_dict)\n",
    "#         total_loss += loss.item() * images.size(0)\n",
    "\n",
    "#         # Predictions and accuracy\n",
    "#         correct = 0\n",
    "#         total_samples += images.size(0)\n",
    "#         for class_name in selected_class_names:\n",
    "#             _, preds = torch.max(logits_per_image_dict[class_name], dim=1)\n",
    "#             correct += (preds == true_labels_dict[class_name]).sum().item()\n",
    "#         total_correct += correct\n",
    "\n",
    "#         # Backward pass\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # log the loss and accuracy to wandb\n",
    "#         wandb.log({\"train_loss\": loss.item(), \"train_accuracy\": correct / images.size(0) / len(selected_class_names)},\n",
    "#                   step=step)\n",
    "#         step += 1\n",
    "\n",
    "#     avg_loss = total_loss / total_samples / len(selected_class_names)\n",
    "#     accuracy = total_correct / total_samples / len(selected_class_names)\n",
    "#     print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "#     # Validate after each epoch\n",
    "#     val_loss, val_accuracy_dict = validate(mo_model, val_dataloader, criteria, device, text_input_dict, selected_class_names)\n",
    "#     val_accuracy = sum(val_accuracy_dict.values()) / len(val_accuracy_dict)\n",
    "#     print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "#     # Log to wandb\n",
    "#     log_dict = {\n",
    "#         \"val_loss\": val_loss,\n",
    "#         \"val_accuracy\": val_accuracy\n",
    "#     }\n",
    "#     for class_name, accuracy in val_accuracy_dict.items():\n",
    "#         log_dict[f\"val_accuracy_{class_name}\"] = accuracy\n",
    "\n",
    "#     wandb.log(log_dict, step=step)\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "# torch.save(mo_model.state_dict(), \"model/multihead_lora_prompt_tune.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loss, test_accuracy_dict = validate(\n",
    "#     mo_model, test_dataloader, criteria, device, text_input_dict, selected_class_names\n",
    "# )\n",
    "\n",
    "# print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# print(\"Test Accuracy per Class:\")\n",
    "# for class_name, accuracy in test_accuracy_dict.items():\n",
    "#     print(f\"{class_name}: {accuracy:.4f}\")\n",
    "\n",
    "# test_accuracy = sum(test_accuracy_dict.values()) / len(test_accuracy_dict)\n",
    "# print(f\"Average Test Accuracy: {test_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
